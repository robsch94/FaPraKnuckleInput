{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import *\n",
    "from keras import optimizers\n",
    "from keras import utils\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Importing matplotlib to plot images.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Importing SK-learn to calculate precision and recall\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, LeaveOneGroupOut\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "# Used for graph export\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.framework import graph_io\n",
    "from keras import backend as K\n",
    "\n",
    "import pickle as pkl\n",
    "import h5py\n",
    "\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import telegram\n",
    "from keras.callbacks import Callback\n",
    "from callbacks import TelegramCallback\n",
    "from callbacks.TelegramData import TelegramData\n",
    "\n",
    "\n",
    "# create callback\n",
    "config = {\n",
    "    'token': TelegramData.Token,   # paste your bot token\n",
    "    'telegram_id': TelegramData.ID,                                   # paste your telegram_id\n",
    "    'model_name': 'Jan_LSTM_inter'\n",
    "}\n",
    "\n",
    "tg_callback = TelegramCallback(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggingTensorBoard(TensorBoard):    \n",
    "\n",
    "    def __init__(self, log_dir, settings_str_to_log, **kwargs):\n",
    "        super(LoggingTensorBoard, self).__init__(log_dir, **kwargs)\n",
    "\n",
    "        self.settings_str = settings_str_to_log\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        TensorBoard.on_train_begin(self, logs=logs)\n",
    "\n",
    "        tensor =  tf.convert_to_tensor(self.settings_str)\n",
    "        summary = tf.summary.text (\"Run_Settings\", tensor)\n",
    "\n",
    "        with  tf.Session() as sess:\n",
    "            s = sess.run(summary)\n",
    "            self.writer.add_summary(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  9  6  4 14 17 16 12  3 10 18  5] [13  8 11 15  7]\n"
     ]
    }
   ],
   "source": [
    "dfAll = pd.read_pickle(\"PklData/df_lstm_norm50_100.pkl\")\n",
    "\n",
    "lst = dfAll.userID.unique()\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(lst)\n",
    "test_ids = lst[-5:]\n",
    "train_ids = lst[:-5]\n",
    "print(train_ids, test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAll.userID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAll.TaskID = dfAll.TaskID % 17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = dfAll[dfAll.userID.isin(train_ids)]\n",
    "df_test = dfAll[dfAll.userID.isin(test_ids)]\n",
    "\n",
    "df_train2 = df_train[['Blobs', 'TaskID']].copy()\n",
    "df_test2 = df_test[['Blobs', 'TaskID']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate(df_train2.Blobs.values).reshape(-1,50,27,15,1)\n",
    "x_test = np.concatenate(df_test2.Blobs.values).reshape(-1,50,27,15,1)\n",
    "\n",
    "y_train = df_train2.TaskID.values\n",
    "y_test = df_test2.TaskID.values\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices (one-hot notation)\n",
    "num_classes = len(dfAll.TaskID.unique())\n",
    "y_train_one_hot = utils.to_categorical(df_train2.TaskID, num_classes)\n",
    "y_test_one_hot = utils.to_categorical(df_test2.TaskID, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If GPU is not available: \n",
    "# GPU_USE = '/cpu:0'\n",
    "#config = tf.ConfigProto(device_count = {\"GPU\": 1})\n",
    "\n",
    "\n",
    "# If GPU is available: \n",
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = True\n",
    "config.allow_soft_placement = True\n",
    "config.gpu_options.allow_growth=True\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "\n",
    "# Limit the maximum memory used\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "\n",
    "# set session config\n",
    "tf.keras.backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_1 (TimeDist (None, 50, 27, 15, 64)    640       \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 50, 27, 15, 32)    18464     \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 50, 14, 8, 32)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 50, 14, 8, 32)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 50, 14, 8, 32)     9248      \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 50, 14, 8, 16)     4624      \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 50, 7, 4, 16)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 50, 7, 4, 16)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 50, 448)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 50, 128)           57472     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 300)           514800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 17)                1717      \n",
      "=================================================================\n",
      "Total params: 767,365\n",
      "Trainable params: 767,365\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "KnuckleFinger_LSTM_Jan_20190301_110039\n",
      "Train on 6624 samples, validate on 2569 samples\n",
      "Epoch 1/5000\n",
      "6624/6624 [==============================] - 76s 11ms/step - loss: 82.8096 - acc: 0.1067 - val_loss: 68.0277 - val_acc: 0.1911\n",
      "Epoch 2/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 55.8976 - acc: 0.2156 - val_loss: 44.6242 - val_acc: 0.2822\n",
      "Epoch 3/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 35.6091 - acc: 0.2713 - val_loss: 27.3864 - val_acc: 0.3157\n",
      "Epoch 4/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 21.0613 - acc: 0.3139 - val_loss: 15.5366 - val_acc: 0.3040\n",
      "Epoch 5/5000\n",
      "6624/6624 [==============================] - 70s 11ms/step - loss: 11.5785 - acc: 0.3336 - val_loss: 8.3068 - val_acc: 0.3643\n",
      "Epoch 6/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 6.3810 - acc: 0.3404 - val_loss: 4.8475 - val_acc: 0.3433\n",
      "Epoch 7/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 3.8150 - acc: 0.3554 - val_loss: 3.1043 - val_acc: 0.3752\n",
      "Epoch 8/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 2.7518 - acc: 0.3687 - val_loss: 2.5571 - val_acc: 0.3749\n",
      "Epoch 9/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 2.4005 - acc: 0.3884 - val_loss: 2.3263 - val_acc: 0.3865\n",
      "Epoch 10/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 2.2252 - acc: 0.4069 - val_loss: 2.2131 - val_acc: 0.4048\n",
      "Epoch 11/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 2.1329 - acc: 0.4103 - val_loss: 2.1090 - val_acc: 0.3970\n",
      "Epoch 12/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 2.0592 - acc: 0.4254 - val_loss: 2.0893 - val_acc: 0.3978\n",
      "Epoch 13/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 2.0101 - acc: 0.4316 - val_loss: 1.9980 - val_acc: 0.4344\n",
      "Epoch 14/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.9762 - acc: 0.4419 - val_loss: 1.9934 - val_acc: 0.4169\n",
      "Epoch 15/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.9266 - acc: 0.4470 - val_loss: 1.9516 - val_acc: 0.4301\n",
      "Epoch 16/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.8834 - acc: 0.4476 - val_loss: 1.8747 - val_acc: 0.4527\n",
      "Epoch 17/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.8569 - acc: 0.4633 - val_loss: 1.9592 - val_acc: 0.4255\n",
      "Epoch 18/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.8274 - acc: 0.4653 - val_loss: 1.9781 - val_acc: 0.4095\n",
      "Epoch 19/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.8063 - acc: 0.4707 - val_loss: 1.8026 - val_acc: 0.4729\n",
      "Epoch 20/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.7736 - acc: 0.4828 - val_loss: 1.8546 - val_acc: 0.4449\n",
      "Epoch 21/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.7487 - acc: 0.4909 - val_loss: 1.7582 - val_acc: 0.4866\n",
      "Epoch 22/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.7302 - acc: 0.4875 - val_loss: 1.7630 - val_acc: 0.4749\n",
      "Epoch 23/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.7059 - acc: 0.4950 - val_loss: 1.7837 - val_acc: 0.4659\n",
      "Epoch 24/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.6886 - acc: 0.5112 - val_loss: 1.7356 - val_acc: 0.4963\n",
      "Epoch 25/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.6907 - acc: 0.5083 - val_loss: 1.6611 - val_acc: 0.5173\n",
      "Epoch 26/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.6518 - acc: 0.5162 - val_loss: 1.7173 - val_acc: 0.5053\n",
      "Epoch 27/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.6706 - acc: 0.5115 - val_loss: 1.7484 - val_acc: 0.4811\n",
      "Epoch 28/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.6212 - acc: 0.5285 - val_loss: 1.6632 - val_acc: 0.5162\n",
      "Epoch 29/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.6048 - acc: 0.5282 - val_loss: 1.6383 - val_acc: 0.5415\n",
      "Epoch 30/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.5978 - acc: 0.5346 - val_loss: 1.7123 - val_acc: 0.5119\n",
      "Epoch 31/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.5673 - acc: 0.5418 - val_loss: 1.6323 - val_acc: 0.5302\n",
      "Epoch 32/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.5445 - acc: 0.5556 - val_loss: 1.6232 - val_acc: 0.5313\n",
      "Epoch 33/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.5276 - acc: 0.5613 - val_loss: 1.5859 - val_acc: 0.5442\n",
      "Epoch 34/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.5107 - acc: 0.5633 - val_loss: 1.5561 - val_acc: 0.5516\n",
      "Epoch 35/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.4960 - acc: 0.5657 - val_loss: 1.5459 - val_acc: 0.5636\n",
      "Epoch 36/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.4806 - acc: 0.5787 - val_loss: 1.5846 - val_acc: 0.5446\n",
      "Epoch 37/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.4817 - acc: 0.5785 - val_loss: 1.5496 - val_acc: 0.5707\n",
      "Epoch 38/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.4456 - acc: 0.5916 - val_loss: 1.5433 - val_acc: 0.5551\n",
      "Epoch 39/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.4370 - acc: 0.5903 - val_loss: 1.5128 - val_acc: 0.5800\n",
      "Epoch 40/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.4269 - acc: 0.5907 - val_loss: 1.4754 - val_acc: 0.5671\n",
      "Epoch 41/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.3999 - acc: 0.6037 - val_loss: 1.4966 - val_acc: 0.5827\n",
      "Epoch 42/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.3775 - acc: 0.6132 - val_loss: 1.4934 - val_acc: 0.5734\n",
      "Epoch 43/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.3730 - acc: 0.6200 - val_loss: 1.4483 - val_acc: 0.5889\n",
      "Epoch 44/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.3568 - acc: 0.6214 - val_loss: 1.4223 - val_acc: 0.5967\n",
      "Epoch 45/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.3245 - acc: 0.6360 - val_loss: 1.4395 - val_acc: 0.5913\n",
      "Epoch 46/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.3367 - acc: 0.6307 - val_loss: 1.4217 - val_acc: 0.5987\n",
      "Epoch 47/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.3109 - acc: 0.6360 - val_loss: 1.4235 - val_acc: 0.5878\n",
      "Epoch 48/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.3007 - acc: 0.6422 - val_loss: 1.4346 - val_acc: 0.6018\n",
      "Epoch 49/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.2991 - acc: 0.6415 - val_loss: 1.3911 - val_acc: 0.6088\n",
      "Epoch 50/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.2634 - acc: 0.6505 - val_loss: 1.3618 - val_acc: 0.6096\n",
      "Epoch 51/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.2561 - acc: 0.6546 - val_loss: 1.4016 - val_acc: 0.5979\n",
      "Epoch 52/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.2397 - acc: 0.6590 - val_loss: 1.3616 - val_acc: 0.6053\n",
      "Epoch 53/5000\n",
      "6624/6624 [==============================] - 70s 11ms/step - loss: 1.2582 - acc: 0.6541 - val_loss: 1.3337 - val_acc: 0.6135\n",
      "Epoch 54/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.2305 - acc: 0.6623 - val_loss: 1.3187 - val_acc: 0.6345\n",
      "Epoch 55/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.2086 - acc: 0.6753 - val_loss: 1.3455 - val_acc: 0.6232\n",
      "Epoch 56/5000\n",
      "6624/6624 [==============================] - 70s 11ms/step - loss: 1.1963 - acc: 0.6721 - val_loss: 1.3278 - val_acc: 0.6228\n",
      "Epoch 57/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.2042 - acc: 0.6766 - val_loss: 1.3556 - val_acc: 0.6174\n",
      "Epoch 58/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.1854 - acc: 0.6789 - val_loss: 1.3341 - val_acc: 0.6166\n",
      "Epoch 59/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.1853 - acc: 0.6787 - val_loss: 1.3920 - val_acc: 0.6100\n",
      "Epoch 60/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.1752 - acc: 0.6843 - val_loss: 1.2619 - val_acc: 0.6489\n",
      "Epoch 61/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.1575 - acc: 0.6866 - val_loss: 1.3079 - val_acc: 0.6333\n",
      "Epoch 62/5000\n",
      "6624/6624 [==============================] - 70s 11ms/step - loss: 1.1568 - acc: 0.6846 - val_loss: 1.3193 - val_acc: 0.6248\n",
      "Epoch 63/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.1429 - acc: 0.6940 - val_loss: 1.2806 - val_acc: 0.6357\n",
      "Epoch 64/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.1290 - acc: 0.6970 - val_loss: 1.2667 - val_acc: 0.6501\n",
      "Epoch 65/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.1374 - acc: 0.6928 - val_loss: 1.3366 - val_acc: 0.6283\n",
      "Epoch 66/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.1398 - acc: 0.6984 - val_loss: 1.2618 - val_acc: 0.6489\n",
      "Epoch 67/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.1299 - acc: 0.6949 - val_loss: 1.2833 - val_acc: 0.6329\n",
      "Epoch 68/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.0957 - acc: 0.7082 - val_loss: 1.2799 - val_acc: 0.6364\n",
      "Epoch 69/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.1027 - acc: 0.7029 - val_loss: 1.2038 - val_acc: 0.6578\n",
      "Epoch 70/5000\n",
      "6624/6624 [==============================] - 70s 11ms/step - loss: 1.1034 - acc: 0.7076 - val_loss: 1.2201 - val_acc: 0.6602\n",
      "Epoch 71/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.0835 - acc: 0.7138 - val_loss: 1.3024 - val_acc: 0.6489\n",
      "Epoch 72/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.0855 - acc: 0.7111 - val_loss: 1.2855 - val_acc: 0.6442\n",
      "Epoch 73/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.0597 - acc: 0.7136 - val_loss: 1.2216 - val_acc: 0.6551\n",
      "Epoch 74/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.0742 - acc: 0.7151 - val_loss: 1.2251 - val_acc: 0.6602\n",
      "Epoch 75/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.0445 - acc: 0.7251 - val_loss: 1.2447 - val_acc: 0.6594\n",
      "Epoch 76/5000\n",
      "6624/6624 [==============================] - 70s 11ms/step - loss: 1.0422 - acc: 0.7255 - val_loss: 1.1986 - val_acc: 0.6672\n",
      "Epoch 77/5000\n",
      "6624/6624 [==============================] - 70s 11ms/step - loss: 1.0544 - acc: 0.7248 - val_loss: 1.1820 - val_acc: 0.6715\n",
      "Epoch 78/5000\n",
      "6624/6624 [==============================] - 70s 11ms/step - loss: 1.0218 - acc: 0.7340 - val_loss: 1.1801 - val_acc: 0.6769\n",
      "Epoch 79/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.0254 - acc: 0.7286 - val_loss: 1.2425 - val_acc: 0.6637\n",
      "Epoch 80/5000\n",
      "6624/6624 [==============================] - 70s 11ms/step - loss: 1.0220 - acc: 0.7348 - val_loss: 1.2121 - val_acc: 0.6594\n",
      "Epoch 81/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.0324 - acc: 0.7289 - val_loss: 1.1896 - val_acc: 0.6703\n",
      "Epoch 82/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.0027 - acc: 0.7349 - val_loss: 1.2497 - val_acc: 0.6543\n",
      "Epoch 83/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 1.0123 - acc: 0.7334 - val_loss: 1.2386 - val_acc: 0.6660\n",
      "Epoch 84/5000\n",
      "6624/6624 [==============================] - 70s 11ms/step - loss: 1.0140 - acc: 0.7394 - val_loss: 1.2059 - val_acc: 0.6621\n",
      "Epoch 85/5000\n",
      "6624/6624 [==============================] - 70s 11ms/step - loss: 0.9869 - acc: 0.7455 - val_loss: 1.2044 - val_acc: 0.6773\n",
      "Epoch 86/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.9813 - acc: 0.7420 - val_loss: 1.1903 - val_acc: 0.6641\n",
      "Epoch 87/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.9956 - acc: 0.7417 - val_loss: 1.2027 - val_acc: 0.6730\n",
      "Epoch 88/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.9854 - acc: 0.7517 - val_loss: 1.2254 - val_acc: 0.6660\n",
      "Epoch 89/5000\n",
      "6624/6624 [==============================] - 70s 11ms/step - loss: 0.9759 - acc: 0.7502 - val_loss: 1.2065 - val_acc: 0.6680\n",
      "Epoch 90/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.9678 - acc: 0.7530 - val_loss: 1.2345 - val_acc: 0.6660\n",
      "Epoch 91/5000\n",
      "6624/6624 [==============================] - 70s 11ms/step - loss: 0.9828 - acc: 0.7547 - val_loss: 1.2460 - val_acc: 0.6672\n",
      "Epoch 92/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.9659 - acc: 0.7533 - val_loss: 1.1570 - val_acc: 0.6769\n",
      "Epoch 93/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.9481 - acc: 0.7574 - val_loss: 1.2074 - val_acc: 0.6765\n",
      "Epoch 94/5000\n",
      "6624/6624 [==============================] - 70s 11ms/step - loss: 0.9413 - acc: 0.7577 - val_loss: 1.1776 - val_acc: 0.6707\n",
      "Epoch 95/5000\n",
      "6624/6624 [==============================] - 70s 11ms/step - loss: 0.9438 - acc: 0.7610 - val_loss: 1.1928 - val_acc: 0.6750\n",
      "Epoch 96/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.9446 - acc: 0.7580 - val_loss: 1.1735 - val_acc: 0.6882\n",
      "Epoch 97/5000\n",
      "6624/6624 [==============================] - 70s 11ms/step - loss: 0.9358 - acc: 0.7646 - val_loss: 1.1524 - val_acc: 0.6937\n",
      "Epoch 98/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.9248 - acc: 0.7720 - val_loss: 1.1999 - val_acc: 0.6750\n",
      "Epoch 99/5000\n",
      "6624/6624 [==============================] - 70s 11ms/step - loss: 0.9201 - acc: 0.7717 - val_loss: 1.1880 - val_acc: 0.6781\n",
      "Epoch 100/5000\n",
      "6624/6624 [==============================] - 70s 11ms/step - loss: 0.9217 - acc: 0.7710 - val_loss: 1.2240 - val_acc: 0.6676\n",
      "Epoch 101/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6624/6624 [==============================] - 70s 11ms/step - loss: 0.9178 - acc: 0.7722 - val_loss: 1.1467 - val_acc: 0.6874\n",
      "Epoch 102/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.9028 - acc: 0.7822 - val_loss: 1.1328 - val_acc: 0.6909\n",
      "Epoch 103/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.8998 - acc: 0.7784 - val_loss: 1.1329 - val_acc: 0.6940\n",
      "Epoch 104/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.8836 - acc: 0.7840 - val_loss: 1.1439 - val_acc: 0.6870\n",
      "Epoch 105/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.8863 - acc: 0.7865 - val_loss: 1.2263 - val_acc: 0.6765\n",
      "Epoch 106/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.8868 - acc: 0.7834 - val_loss: 1.1621 - val_acc: 0.6944\n",
      "Epoch 107/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.8675 - acc: 0.7903 - val_loss: 1.1920 - val_acc: 0.6761\n",
      "Epoch 108/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.8890 - acc: 0.7853 - val_loss: 1.1449 - val_acc: 0.6882\n",
      "Epoch 109/5000\n",
      "6624/6624 [==============================] - 70s 11ms/step - loss: 0.8654 - acc: 0.7951 - val_loss: 1.1431 - val_acc: 0.6890\n",
      "Epoch 110/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.8681 - acc: 0.7947 - val_loss: 1.1173 - val_acc: 0.7084\n",
      "Epoch 111/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.8623 - acc: 0.7897 - val_loss: 1.1162 - val_acc: 0.7174\n",
      "Epoch 112/5000\n",
      "6624/6624 [==============================] - 70s 11ms/step - loss: 0.8523 - acc: 0.7959 - val_loss: 1.1728 - val_acc: 0.6870\n",
      "Epoch 113/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.8475 - acc: 0.7968 - val_loss: 1.1543 - val_acc: 0.6944\n",
      "Epoch 114/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.8659 - acc: 0.7896 - val_loss: 1.1490 - val_acc: 0.7061\n",
      "Epoch 115/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.8530 - acc: 0.7995 - val_loss: 1.1487 - val_acc: 0.6979\n",
      "Epoch 116/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.8491 - acc: 0.8016 - val_loss: 1.1543 - val_acc: 0.7018\n",
      "Epoch 117/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.8532 - acc: 0.7954 - val_loss: 1.1537 - val_acc: 0.7077\n",
      "Epoch 118/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.8244 - acc: 0.8096 - val_loss: 1.1159 - val_acc: 0.7155\n",
      "Epoch 119/5000\n",
      "6624/6624 [==============================] - 70s 11ms/step - loss: 0.8272 - acc: 0.8077 - val_loss: 1.1911 - val_acc: 0.6870\n",
      "Epoch 120/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.8185 - acc: 0.8077 - val_loss: 1.1351 - val_acc: 0.7061\n",
      "Epoch 121/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.8112 - acc: 0.8139 - val_loss: 1.1515 - val_acc: 0.7049\n",
      "Epoch 122/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.8160 - acc: 0.8117 - val_loss: 1.1287 - val_acc: 0.7131\n",
      "Epoch 123/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.8150 - acc: 0.8089 - val_loss: 1.1776 - val_acc: 0.6948\n",
      "Epoch 124/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.7978 - acc: 0.8210 - val_loss: 1.1266 - val_acc: 0.7030\n",
      "Epoch 125/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.7946 - acc: 0.8178 - val_loss: 1.1249 - val_acc: 0.7092\n",
      "Epoch 126/5000\n",
      "6624/6624 [==============================] - 70s 11ms/step - loss: 0.7750 - acc: 0.8252 - val_loss: 1.1351 - val_acc: 0.7104\n",
      "Epoch 127/5000\n",
      "6624/6624 [==============================] - 72s 11ms/step - loss: 0.7813 - acc: 0.8252 - val_loss: 1.0653 - val_acc: 0.7232\n",
      "Epoch 128/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.7938 - acc: 0.8223 - val_loss: 1.2316 - val_acc: 0.6847\n",
      "Epoch 129/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.7960 - acc: 0.8228 - val_loss: 1.1282 - val_acc: 0.7081\n",
      "Epoch 130/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.7715 - acc: 0.8284 - val_loss: 1.1820 - val_acc: 0.7026\n",
      "Epoch 131/5000\n",
      "6624/6624 [==============================] - 70s 11ms/step - loss: 0.7661 - acc: 0.8293 - val_loss: 1.1379 - val_acc: 0.7147\n",
      "Epoch 132/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.7704 - acc: 0.8265 - val_loss: 1.1202 - val_acc: 0.7120\n",
      "Epoch 133/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.7643 - acc: 0.8270 - val_loss: 1.1010 - val_acc: 0.7186\n",
      "Epoch 134/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.7641 - acc: 0.8338 - val_loss: 1.1154 - val_acc: 0.7139\n",
      "Epoch 135/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.7560 - acc: 0.8317 - val_loss: 1.1627 - val_acc: 0.7007\n",
      "Epoch 136/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.7508 - acc: 0.8365 - val_loss: 1.0971 - val_acc: 0.7174\n",
      "Epoch 137/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.7475 - acc: 0.8389 - val_loss: 1.1349 - val_acc: 0.7166\n",
      "Epoch 138/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.7498 - acc: 0.8412 - val_loss: 1.1502 - val_acc: 0.6975\n",
      "Epoch 139/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.7483 - acc: 0.8418 - val_loss: 1.1536 - val_acc: 0.7034\n",
      "Epoch 140/5000\n",
      "6624/6624 [==============================] - 70s 11ms/step - loss: 0.7348 - acc: 0.8419 - val_loss: 1.1376 - val_acc: 0.7123\n",
      "Epoch 141/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.7488 - acc: 0.8376 - val_loss: 1.1319 - val_acc: 0.7135\n",
      "Epoch 142/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.7204 - acc: 0.8437 - val_loss: 1.1463 - val_acc: 0.7120\n",
      "Epoch 143/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.7203 - acc: 0.8430 - val_loss: 1.1568 - val_acc: 0.7108\n",
      "Epoch 144/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.7233 - acc: 0.8448 - val_loss: 1.1872 - val_acc: 0.7081\n",
      "Epoch 145/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.7100 - acc: 0.8472 - val_loss: 1.1368 - val_acc: 0.7092\n",
      "Epoch 146/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.7091 - acc: 0.8501 - val_loss: 1.1288 - val_acc: 0.7151\n",
      "Epoch 147/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.7082 - acc: 0.8472 - val_loss: 1.1370 - val_acc: 0.7151\n",
      "Epoch 148/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.7108 - acc: 0.8499 - val_loss: 1.1407 - val_acc: 0.7248\n",
      "Epoch 149/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.7027 - acc: 0.8496 - val_loss: 1.1147 - val_acc: 0.7186\n",
      "Epoch 150/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.6943 - acc: 0.8567 - val_loss: 1.1386 - val_acc: 0.7205\n",
      "Epoch 151/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.7077 - acc: 0.8514 - val_loss: 1.1628 - val_acc: 0.7127\n",
      "Epoch 152/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.7500 - acc: 0.8436 - val_loss: 1.1639 - val_acc: 0.7123\n",
      "Epoch 153/5000\n",
      "6624/6624 [==============================] - 70s 11ms/step - loss: 0.6860 - acc: 0.8588 - val_loss: 1.1495 - val_acc: 0.7123\n",
      "Epoch 154/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.7019 - acc: 0.8564 - val_loss: 1.1822 - val_acc: 0.7046\n",
      "Epoch 155/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.6899 - acc: 0.8570 - val_loss: 1.1381 - val_acc: 0.7077\n",
      "Epoch 156/5000\n",
      "6624/6624 [==============================] - 70s 11ms/step - loss: 0.6878 - acc: 0.8567 - val_loss: 1.1411 - val_acc: 0.7240\n",
      "Epoch 157/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.6951 - acc: 0.8558 - val_loss: 1.1391 - val_acc: 0.7143\n",
      "Epoch 158/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.6816 - acc: 0.8590 - val_loss: 1.1872 - val_acc: 0.7143\n",
      "Epoch 159/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.6789 - acc: 0.8626 - val_loss: 1.1713 - val_acc: 0.7143\n",
      "Epoch 160/5000\n",
      "6624/6624 [==============================] - 71s 11ms/step - loss: 0.6646 - acc: 0.8659 - val_loss: 1.2132 - val_acc: 0.7116\n",
      "Epoch 161/5000\n",
      " 750/6624 [==>...........................] - ETA: 54s - loss: 0.6674 - acc: 0.8680"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "epochs = 5000\n",
    "timesteps = 50\n",
    "data_dim = (27,15)\n",
    "l1v = 0.007\n",
    "l2v = 0.014\n",
    "\n",
    "\n",
    "tf.get_default_graph()\n",
    "model = Sequential()\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(64, kernel_size=(3,3), activation='relu', \n",
    "            padding='same', kernel_regularizer=regularizers.l1_l2(l1v,l2v)), input_shape=(timesteps ,27, 15, 1)))\n",
    "model.add(TimeDistributed(Conv2D(32, kernel_size=(3,3), activation='relu', padding='same', \n",
    "                                 kernel_regularizer=regularizers.l1_l2(l1v,l2v))))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2,2), strides=None, padding='same', data_format='channels_last')))\n",
    "model.add(TimeDistributed(Dropout(0.50)))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, kernel_size=(3, 3), activation='relu', \n",
    "            padding='same', kernel_regularizer=regularizers.l1_l2(l1v,l2v)), input_shape=(timesteps ,27, 15, 1)))\n",
    "model.add(TimeDistributed(Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same', \n",
    "                                 kernel_regularizer=regularizers.l1_l2(l1v,l2v))))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2,2), strides=None, padding='same', data_format='channels_last')))\n",
    "model.add(TimeDistributed(Dropout(0.50)))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "#model.add(TimeDistributed(Dense(128)))\n",
    "model.add(TimeDistributed(Dense(128)))\n",
    "#model.add(TimeDistributed(Dense(32)))\n",
    "model.add(LSTM(300, return_sequences=True, input_shape=(timesteps, data_dim),kernel_regularizer=regularizers.l1_l2(l1v,l2v)))\n",
    "model.add(Dropout(0.25))\n",
    "#model.add(LSTM(256, return_sequences=True, input_shape=(timesteps, data_dim),kernel_regularizer=regularizers.l1_l2(0.001,0.01)))\n",
    "#model.add(Dense(512))\n",
    "model.add(LSTM(100, return_sequences=False, input_shape=(timesteps, data_dim),kernel_regularizer=regularizers.l1_l2(l1v,l2v)))\n",
    "model.add(Dropout(0.25))\n",
    "#model.add(Dense(256))\n",
    "#model.add(LSTM(64, kernel_regularizer=regularizers.l2(0.01)))\n",
    "#model.add(Dropout(0.20))\n",
    "#model.add(Dense(64))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#optimizer = optimizers.Adagrad()\n",
    "optimizer = optimizers.Adam(lr = 0.0001, decay=1e-6)\n",
    "#optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.1)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Broadcast progress to the tensorboard.\n",
    "readable_timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%d_%H%M%S')\n",
    "tensorflowfolder = \"/srv/share/tensorboardfiles/KnuckleFinger_LSTM_Jan_\" + readable_timestamp\n",
    "\n",
    "config = \"\"\n",
    "for layer in model.layers:\n",
    "    config += str(layer.output).split('\\\"')[1].split(\"/\")[0] + str(layer.output_shape) + \"\\n\\n\"\n",
    "config += \"batchsize: \" + str(batch_size) + \"\\n\\n\" + \"epochs: \" + str(epochs) + \"\\n\\n\" \n",
    "config += \"l1: \" + str(l1v) + \"\\n\\n\" + \"l2: \" + str(l2v) + \"\\n\\n\"\n",
    "\n",
    "model.summary()\n",
    "\n",
    "readable_timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%d_%H%M%S')\n",
    "tensorflowfolder = \"/srv/share/tensorboardfiles/KnuckleFinger_LSTM_Jan_\" + readable_timestamp\n",
    "print(\"KnuckleFinger_LSTM_Jan_\" + readable_timestamp)\n",
    "logger = LoggingTensorBoard(settings_str_to_log = config, log_dir=tensorflowfolder, histogram_freq=0,\n",
    "                            write_graph=True, write_images=True, update_freq = 'epoch')\n",
    "storer = ModelCheckpoint(\"./ModelSnapshots/KnuckleFinger_LSTM_Jan_\" + readable_timestamp + \".h5\", monitor='val_loss', verbose=0,\n",
    "                         save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=100, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.95, \n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "history = model.fit(x_train, y_train_one_hot,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test_one_hot),\n",
    "                        callbacks=[storer,logger,tg_callback, learning_rate_reduction])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('lstm_01_03_19_inter_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['val_acc'], label=\"Test Accuracy\")\n",
    "plt.plot(history.history['acc'], label=\"Training Accuracy\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test_pred = model.predict(x_test, batch_size=30)\n",
    "y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "#model.predict(x_test)\n",
    "\n",
    "print ('\\n Summary of the precision, recall, F1 score for each class:')\n",
    "print (sklearn.metrics.classification_report(y_test, y_test_pred))\n",
    "\n",
    "print ('\\n Confusion matrix: ')\n",
    "print (sklearn.metrics.confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
