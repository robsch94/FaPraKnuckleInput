{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import *\n",
    "from keras import optimizers\n",
    "from keras import utils\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Importing matplotlib to plot images.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Importing SK-learn to calculate precision and recall\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, LeaveOneGroupOut\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "# Used for graph export\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.framework import graph_io\n",
    "from keras import backend as K\n",
    "\n",
    "import pickle as pkl\n",
    "import h5py\n",
    "\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import telegram\n",
    "from keras.callbacks import Callback\n",
    "from callbacks import TelegramCallback\n",
    "from callbacks.TelegramData import TelegramData\n",
    "\n",
    "\n",
    "# create callback\n",
    "config = {\n",
    "    'token': TelegramData.Token,   # paste your bot token\n",
    "    'telegram_id': TelegramData.ID,                                   # paste your telegram_id\n",
    "}\n",
    "\n",
    "tg_callback = TelegramCallback(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggingTensorBoard(TensorBoard):    \n",
    "\n",
    "    def __init__(self, log_dir, settings_str_to_log, **kwargs):\n",
    "        super(LoggingTensorBoard, self).__init__(log_dir, **kwargs)\n",
    "\n",
    "        self.settings_str = settings_str_to_log\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        TensorBoard.on_train_begin(self, logs=logs)\n",
    "\n",
    "        tensor =  tf.convert_to_tensor(self.settings_str)\n",
    "        summary = tf.summary.text (\"Run_Settings\", tensor)\n",
    "\n",
    "        with  tf.Session() as sess:\n",
    "            s = sess.run(summary)\n",
    "            self.writer.add_summary(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [1,2,3, 7, 8, 9, 10]\n",
    "test_ids = [4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAll = pd.read_pickle(\"PklData/df_blobs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = dfAll[dfAll.userID.isin(train_ids)]\n",
    "df_test = dfAll[dfAll.userID.isin(test_ids)]\n",
    "\n",
    "df_train2 = df_train[['Blobs', 'InputMethod']].copy()\n",
    "df_test2 = df_test[['Blobs', 'InputMethod']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.vstack(df_train2.Blobs)\n",
    "x_test = np.vstack(df_test2.Blobs)\n",
    "y_train = df_train2.InputMethod.values\n",
    "y_test = df_test2.InputMethod.values\n",
    "\n",
    "x_train = x_train.reshape(-1, 27, 15, 1)\n",
    "x_test = x_test.reshape(-1, 27, 15, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If GPU is not available: \n",
    "# GPU_USE = '/cpu:0'\n",
    "# config = tf.ConfigProto(device_count = {\"GPU\": 0})\n",
    "\n",
    "\n",
    "# If GPU is available: \n",
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = True\n",
    "config.allow_soft_placement = True\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "\n",
    "# Limit the maximum memory used\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.1\n",
    "\n",
    "# set session config\n",
    "tf.keras.backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Config for ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UNCOMMENT below line for segmenting the signal in overlapping windows of 90 samples with 50% overlap\n",
    "# segments, labels, subjects = segment_signal(df)\n",
    "\n",
    "## COMMENT below segments + labels files if you want to segment afresh . open a file, where you stored the pickled data\n",
    "segments = pickle.load(open('./data/segments_90_logo.p', 'rb'), encoding='latin1')\n",
    "labels = pickle.load(open('./data/labels_90_logo.p','rb'), encoding='latin1')\n",
    "subjects = pickle.load(open('./data/subjects_90_logo.p','rb'),encoding='latin1')\n",
    "\n",
    "## dump information to that file (UNCOMMENT to save fresh segmentation!)\n",
    "# pickle.dump(segments, open( './data/segments_90_logo.p','wb'))\n",
    "# pickle.dump(labels, open( './data/labels_90_logo.p','wb'))\n",
    "# pickle.dump(subjects, open( './data/subjects_90_logo.p','wb'))\n",
    "\n",
    "# segments, labels, subjects = segment_signal(df)\n",
    "groups = np.array(subjects)\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "logo.get_n_splits(segments, labels, groups)\n",
    "\n",
    "## defining parameters for the input and network layers\n",
    "## we are treating each segmeent or chunk as a 2D image (90 X 3)\n",
    "numOfRows = segments.shape[1]\n",
    "numOfColumns = segments.shape[2]\n",
    "\n",
    "## reshaping the data for network input\n",
    "reshapedSegments = segments.reshape(segments.shape[0], numOfRows, numOfColumns,1)\n",
    "## (observations, timesteps, features (x,y,z), channels)\n",
    "\n",
    "# categorically defining the classes of the activities\n",
    "labels = np.asarray(pd.get_dummies(labels),dtype = np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('segments shape:' + str(segments.shape))\n",
    "print('labels shape:' + str(labels.shape))\n",
    "print('\\n')\n",
    "print('Rows / Timesteps: ' + str(numOfRows))\n",
    "print('Columns / features: ' + str(numOfColumns))\n",
    "\n",
    "## key:\n",
    "## Conv2D: (observations, timesteps, features (acc + gyro), channels)\n",
    "## LSTM: (batch size, observations, timesteps, features (acc + gyro), channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv2D_LSTM_Model():\n",
    "    model = Sequential()\n",
    "    print(model.name)\n",
    "\n",
    "    # adding the first convLSTM layer with 32 filters and 5 by 5 kernal size, using the rectifier as the activation function\n",
    "    model.add(ConvLSTM2D(numFilters, (kernalSize1,kernalSize1),input_shape=(None, numOfRows, numOfColumns, 1),activation='relu', padding='same',return_sequences=True))\n",
    "    print(model.input_shape)\n",
    "    print(model.output_shape)\n",
    "    print(model.name)\n",
    "    \n",
    "    ## adding a maxpooling layer\n",
    "    model.add(TimeDistributed(MaxPooling2D(pool_size=(poolingWindowSz,poolingWindowSz),padding='valid')))\n",
    "    print(model.output_shape)\n",
    "\n",
    "    ## adding a dropout layer for the regularization and avoiding over fitting\n",
    "    model.add(Dropout(dropOutRatio))\n",
    "    print(model.output_shape)\n",
    "    \n",
    "    ## flattening the output in order to apple dense layer\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    print(model.output_shape)\n",
    "    \n",
    "    ## adding first fully connected layer with 256 outputs\n",
    "    model.add(Dense(numNueronsFCL1, activation='relu'))\n",
    "    print(model.output_shape)\n",
    "\n",
    "    ## adding second fully connected layer 128 outputs\n",
    "    model.add(Dense(numNueronsFCL2, activation='relu'))\n",
    "    print(model.output_shape)\n",
    "\n",
    "    ## flattening the output in order to apply the fully connected layer\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    print(model.output_shape)\n",
    "\n",
    "    ## adding softmax layer for the classification\n",
    "    model.add(Dense(numClasses, activation='softmax'))\n",
    "    print(model.output_shape)\n",
    "    print(model.name)\n",
    "\n",
    "    ## Compiling the model to generate a model\n",
    "    adam = optimizers.Adam(lr = 0.001, decay=1e-6)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave One Group Out for train-test split, and train the network!\n",
    "\n",
    "## NOTE: training with Epochs=2 and BatchSize=32 for illustrative purposes.  \n",
    "\n",
    "## reset and initialize graph\n",
    "tf.get_default_graph()\n",
    "\n",
    "if not os.path.exists('./train_history'):\n",
    "    os.makedirs('./train_history')\n",
    "    \n",
    "cvscores = []\n",
    "\n",
    "for index, (train_index, test_index) in enumerate(logo.split(reshapedSegments, labels, groups)):\n",
    "\n",
    "    print('Training on fold ' + str(index+1) + '/14...') ## 14 due to number of subjects in our dataset\n",
    "\n",
    "    # print('TRAIN:', train_index, 'TEST:', test_index)\n",
    "    trainX, testX = reshapedSegments[train_index], reshapedSegments[test_index]\n",
    "    trainY, testY = labels[train_index], labels[test_index]\n",
    "    # print(np.nan_to_num(trainX), np.nan_to_num(testX), trainY, testY)\n",
    "\n",
    "    ## clear model, and create it\n",
    "    model = None\n",
    "    model = Conv2D_LSTM_Model()\n",
    "\n",
    "    for layer in model.layers:\n",
    "        print(layer.name)\n",
    "    print(trainX.shape)\n",
    "\n",
    "    ## fit the model\n",
    "    history = model.fit(np.expand_dims(trainX,1),np.expand_dims(trainY,1), validation_data=(np.expand_dims(testX,1),np.expand_dims(testY,1)), epochs=Epochs,batch_size=batchSize,verbose=2)\n",
    "    \n",
    "    with open('train_history/train_history_dict_' + str(index), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "\n",
    "    ## evaluate the model\n",
    "    score = model.evaluate(np.expand_dims(testX,1),np.expand_dims(testY,1),verbose=2)\n",
    "    print('%s: %.2f%%' % (model.metrics_names[1], score[1]*100))\n",
    "    print('Baseline ConvLSTM Error: %.2f%%' %(100-score[1]*100))\n",
    "    cvscores.append(score[1] * 100)\n",
    "\n",
    "print('%.2f%% (+/- %.2f%%)' % (np.mean(cvscores), np.std(cvscores)))\n",
    "\n",
    "## Save your model!\n",
    "model.save('model_had_lstm_logo.h5')\n",
    "model.save_weights('model_weights_had_lstm_logo.h5')\n",
    "np.save('groundTruth_had_lstm_logo.npy',np.expand_dims(testY,1))\n",
    "np.save('testData_had_lstm_logo.npy',np.expand_dims(testX,1))\n",
    "\n",
    "## write to JSON, in case you wanrt to work with that data format later when inspecting your model\n",
    "with open('./data/model_had_logo.json', 'w') as json_file:\n",
    "    json_file.write(model.to_json())\n",
    "\n",
    "## write cvscores to file\n",
    "with open('train_history/cvscores_convlstm_logo.txt', 'w') as cvs_file:\n",
    "    cvs_file.write('%.2f%% (+/- %.2f%%)' % (np.mean(cvscores), np.std(cvscores)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
