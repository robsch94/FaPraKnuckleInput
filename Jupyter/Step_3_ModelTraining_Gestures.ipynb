{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import *\n",
    "from keras import optimizers\n",
    "from keras import utils\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Importing matplotlib to plot images.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Importing SK-learn to calculate precision and recall\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, LeaveOneGroupOut\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "# Used for graph export\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.framework import graph_io\n",
    "from keras import backend as K\n",
    "\n",
    "import pickle as pkl\n",
    "import h5py\n",
    "\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "import sys\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggingTensorBoard(TensorBoard):    \n",
    "\n",
    "    def __init__(self, log_dir, settings_str_to_log, **kwargs):\n",
    "        super(LoggingTensorBoard, self).__init__(log_dir, **kwargs)\n",
    "\n",
    "        self.settings_str = settings_str_to_log\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        TensorBoard.on_train_begin(self, logs=logs)\n",
    "\n",
    "        tensor =  tf.convert_to_tensor(self.settings_str)\n",
    "        summary = tf.summary.text (\"Run_Settings\", tensor)\n",
    "\n",
    "        with  tf.Session() as sess:\n",
    "            s = sess.run(summary)\n",
    "            self.writer.add_summary(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import telegram\\nfrom keras.callbacks import Callback\\nfrom callbacks import TelegramCallback\\nfrom TelegramData import TelegramData\\n\\n# create callback\\nconfig = {\\n    'token': TelegramData.TelegramToken,       # paste your bot token\\n    'telegram_id': TelegramData.TelegramId, # paste your telegram_id\\n}\\n\\ntg_callback = TelegramCallback(config)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import telegram\n",
    "from keras.callbacks import Callback\n",
    "from callbacks import TelegramCallback\n",
    "from TelegramData import TelegramData\n",
    "\n",
    "# create callback\n",
    "config = {\n",
    "    'token': TelegramData.TelegramToken,       # paste your bot token\n",
    "    'telegram_id': TelegramData.TelegramId, # paste your telegram_id\n",
    "}\n",
    "\n",
    "tg_callback = TelegramCallback(config)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [3, 4, 5, 6, 7, 8]\n",
    "test_ids = [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, split between train and test sets\n",
    "dfAll = pd.read_pickle(\"PklData/df_gestures.pkl\")\n",
    "#df_train = dfAll[(dfAll.userID != 1) | (dfAll.userID != 2)]\n",
    "#df_test = dfAll[(dfAll.userID == 1) | (dfAll.userID == 2)]\n",
    "df_train = dfAll[dfAll.userID.isin(train_ids)]\n",
    "df_test = dfAll[dfAll.userID.isin(test_ids)]\n",
    "\n",
    "df_test = df_test.reset_index()\n",
    "df_train = df_train.reset_index()\n",
    "\n",
    "df_train2 = df_train[['Gesture', 'TaskID']].copy()\n",
    "df_test2 = df_test[['Gesture', 'TaskID']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 7, 5, 6, 4, 8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.userID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.userID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "less: 0\n",
      "more: 0\n"
     ]
    }
   ],
   "source": [
    "less_errors = 0\n",
    "more_errors = 0\n",
    "for UserId in dfAll.userID.unique():\n",
    "    for TaskId in dfAll[dfAll.userID == UserId].TaskID.unique():\n",
    "        for VersionId in dfAll[(dfAll.userID == UserId) & (dfAll.TaskID == TaskId)].VersionID.unique():\n",
    "            temp_len = len(dfAll[(dfAll.userID == UserId) & (dfAll.TaskID == TaskId) & (dfAll.VersionID == VersionId)].RepetitionID.unique())\n",
    "            if temp_len < 1:\n",
    "                less_errors += 1\n",
    "            elif temp_len > 1:\n",
    "                more_errors += 1\n",
    "print(\"less: %s\" % less_errors)\n",
    "print(\"more: %s\" % more_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.vstack(df_train2.Gesture)\n",
    "x_test = np.vstack(df_test2.Gesture)\n",
    "y_train = df_train2.TaskID.values\n",
    "y_test = df_test2.TaskID.values\n",
    "\n",
    "x_train = x_train.reshape(-1, 27, 15, 1)\n",
    "x_test = x_test.reshape(-1, 27, 15, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices (one-hot notation)\n",
    "num_classes = 34\n",
    "y_train_one_hot = utils.to_categorical(df_train2.TaskID, num_classes)\n",
    "y_test_one_hot = utils.to_categorical(df_test2.TaskID, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>TaskID</th>\n",
       "      <th>VersionID</th>\n",
       "      <th>RepetitionID</th>\n",
       "      <th>Gesture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  TaskID  VersionID  RepetitionID  \\\n",
       "0       3      10          2             0   \n",
       "1       3      10          3             0   \n",
       "2       3      10          4             0   \n",
       "3       3      10          5             0   \n",
       "4       3      10          6             0   \n",
       "\n",
       "                                             Gesture  \n",
       "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "1  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "3  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "4  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAll.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJ4AAAD8CAYAAACGuR0qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAADGtJREFUeJzt3W+MXGUVBvDn2f/ttlRq2xXaQmtTTBYiJZaCERMQNQU1BWMa+GAaQ1j8UyOJMWn8Ah/7QSRqAAO6UhEKRG22MQ1QGw0hUcKCBVpbU4Rid213W9jSLZTt7s7xw9zVtd05d3bu7JzZmeeXbObPe2fuSXm4s/vOve+hmUGk0hqiC5D6pOBJCAVPQih4EkLBkxAKnoRQ8CSEgichFDwJ0VTJnbWw1drQXsldSoUNY+iEmS1O2y5T8EiuB/ATAI0AfmFmW73t29COa3hjll1Klfuj/fbtYrYr+aOWZCOABwDcBKATwO0kO0t9P6kvWX7HWwfgDTN708zOAngSwIbylCW1LkvwlgI4MulxX/Lc/yHZRbKXZO8oRjLsTmrJjP9Va2YPm9laM1vbjNaZ3p3MElmC1w9g+aTHy5LnRFJlCd5LAFaTXEmyBcBtAHaWpyypdSVPp5jZGMnNAJ5Ffjql28z2l60yqWmZ5vHMbBeAXWWqReqIvjKTEAqehFDwJISCJyEUPAmh4EkIBU9CKHgSQsGTEAqehFDwJISCJyEUPAmh4EkIBU9CKHgSQsGTEAqehFDwJISCJyEUPAmh4EkIBU9CKHgSQsGTEAqehFDwJISCJyEUPAmh4EkIBU9CZO1zcRjAMIBxAGNmtrYcRUntK0dnnxvM7EQZ3kfqiD5qJUTW4BmA50i+TLKrHAVJfcj6UXudmfWTXAJgN8mDZvb85A2SQHYBQBvmZtyd1IpMRzwz609uBwHsQL7N1LnbqMGKnCdLE712kvMn7gP4IoB95SpMaluWj9oOADtITrzPE2b2TFmqmin5Wgszy/b2relHdDb5/+S599/PVMNskaXBypsArixjLVJHNJ0iIRQ8CaHgSQgFT0IoeBJCwZMQCp6EKMdpUbNGw7x57jjnzvHfYOECd9ham1Nr4Acj7njjYMoZZkw5VljOHR4/+Z7/+grREU9CKHgSQsGTEAqehFDwJISCJyEUPAlRV/N4bG3xx1v88be+ttgdbzibXsMlOwb8DS5a4g6PfcS/bqVp/1vpRVQBHfEkhIInIRQ8CaHgSQgFT0IoeBJCwZMQdTWPZyMpE20X+ufbdX/jZ+74H06tSa3hb49f4o4f2XipOz5yzWl3fNU3G1NrqAY64kkIBU9CKHgSQsGTEAqehFDwJISCJyFS5/FIdgP4MoBBM7sieW4hgKcArABwGMBGMxuauTIrg+P+Nanfuu+77vjcwfHUfSw4fdAdn3M8ZXHIF1OuDW7zF4dsaG/3X9+ctnDkGXccRZyTCBR3xHsUwPpzntsCYI+ZrQawJ3ksUrTU4CWruL97ztMbAGxL7m8DcEuZ65IaV+rveB1mdjS5fwz59ZBFipb5jwszM+QbrUyJZBfJXpK9o/DXDZH6UWrwBkheBADJ7WChDdXnQqZSavB2AtiU3N8EoKc85Ui9SA0eye0A/gLgEyT7SN4BYCuAL5A8BODzyWORoqXO45nZ7QWGbixpj16Tk5S13xrmtJW0y//t2m+wkhs47o5f9NSwv4OUeUAAyJ350B3/aM9+/w0a/fPtcmf9ibSGhRf679/kv39D2vp8585/FHqf4jYTKS8FT0IoeBJCwZMQCp6EUPAkhIInISp/Xa0zD5Q2T9ewZJE7bo0p/x+9c9Idzp065b++Ak2Mx0dSvs9OmYtsXHCBO37q6qXu+DuX+/N4K3+dMlepeTypZgqehFDwJISCJyEUPAmh4EkIBU9CVH4ez+mnmnZN58DnLnbHR+f7c1zLnvzAHa8Jrf7lBX1fGXPH7/zUn93xF7Z3TreiKemIJyEUPAmh4EkIBU9CKHgSQsGTEAqehKiq8/GQ89eGG7rcH88tGHXH7Tf+HNZs0DDX71ebtj5ec5v/b7DjX1e644sb068dLoaOeBJCwZMQCp6EUPAkhIInIRQ8CaHgSYiqOh/Pxv0+ER/v8a85tZRrTtk0+9vz5j7wzynkkH9dbPNry93xkRG/DwYGD/jjRSpmRdBukoMk90167l6S/ST3Jj83l6UaqRulNlgBgPvNbE3ys6u8ZUmtK7XBikgmWf642EzyteSjuODCuupzIVMpNXgPAVgFYA2AowDuK7Sh+lzIVEoKnpkNmNm4meUAPAJgXXnLklpXUvAmuvokbgWwr9C2IlMppl/tdgDXA1hEsg/APQCuJ7kG+R5mhwHcVZZqUubxmk/4c1gjHX4vV5uXci5bc4v/+tGUZqwN/hxafh/Z5hLTzsfz5kkBYMVjb/sv/9Dvw5E7XZ41AkttsPLLsuxd6pa+MpMQCp6EUPAkhIInIRQ8CaHgSYjKnqBGgk3NhYdb/Hm0oSv9XqvLv33IHT/5g2XueNOwP0dlKXNYdta/rrcYjYtTennMSfna8cSQOzzW1z/dkmaEjngSQsGTEAqehFDwJISCJyEUPAmh4EmIys7jmfnntM1LuaYzxScv8OeoHrv5Mnf8kuf8Xq4th4/7BYwWMY+Xcm3v6av8Gv79Wf+cv8t+mnJdy5A/z1cpOuJJCAVPQih4EkLBkxAKnoRQ8CSEgichKr9gnHPtqZ31r1td+Ndj7vivXv20O373V/1FrX5+5kvu+KIFfr/c8VZ/fT4AuODVQXf8vRX+f5LVVx92x21OW2oN1UBHPAmh4EkIBU9CKHgSQsGTEAqehFDwJETl5/FyhdfAy53x12bjCX8N8GVPLXHHH/ynP093YPOD7vjKni53fOO1L7rjAPDqnVe44xc/489VjvUudsebhvpSa6gGxfS5WE7yTyT/TnI/ye8lzy8kuZvkoeTWv9paZJJiPmrHAHzfzDoBXAvgOyQ7AWwBsMfMVgPYkzwWKUoxfS6Omtkryf1hAAcALAWwAcC2ZLNtAG6ZqSKl9kzrdzySKwBcBeBFAB1mdjQZOgago8BrugB0AUAbUtbvlbpR9F+1JOcB+B2Au83s1OQxMzPkF+I+j/pcyFSKCh7JZuRD97iZ/T55emCi7UBy6592ITJJMX/VEvlV3g+Y2Y8nDe0EsCm5vwlAT/nLk1pVzO94nwHwdQCvk9ybPPdDAFsBPE3yDgBvA9g4MyVKLSqmz8ULAAqd4XhjOYtJaz6SNsE8r9dvHtJ0xl+YceXH/AnihXv9i6l7jvsnogLAqoEj7vjYEX8CmP7ak/Bb1FQPfWUmIRQ8CaHgSQgFT0IoeBJCwZMQCp6EqPyJoA4bSVlUMMXYgP+tXcv7fqPlzoPz/R00pTRCHvebFANAbuhk6jb1QEc8CaHgSQgFT0IoeBJCwZMQCp6EUPAkRFXN42VmU1728V+54eFM41I+OuJJCAVPQih4EkLBkxAKnoRQ8CSEgichFDwJoeBJCAVPQih4EkLBkxAKnoRQ8CSEgichsvS5uJdkP8m9yc/NM1+u1IpiTgSd6HPxCsn5AF4muTsZu9/MfjRz5UmtKmZF0KMAjib3h0lO9LkQKdm0fsc7p88FAGwm+RrJbrWUkunI0ufiIQCrAKxB/oh4X4HXdZHsJdk7imxro0jtKLnPhZkNmNm4meUAPAJg3VSvVYMVmUrJfS4mmqskbgWwr/zlSa3K0ufidpJrkG8ldRjAXTNSodSkLH0udpW/HKkX+uZCQih4EkLBkxAKnoRQ8CSEgichFDwJQUtZU66sOyOPI99UecIiACcqVsD0VXt9QPXVeKmZLU7bqKLBO2/nZK+ZrQ0rIEW11wfMjhqnoo9aCaHgSYjo4D0cvP801V4fMDtqPE/o73hSv6KPeFKnQoJHcj3Jf5B8g+SWiBrSkDxM8vXk0s3eKqinm+QgyX2TnltIcjfJQ8ntrLnupeLBI9kI4AEANwHoRP6E0s5K11GkG8xsTZVMVzwKYP05z20BsMfMVgPYkzyeFSKOeOsAvGFmb5rZWQBPAtgQUMesYmbPA3j3nKc3ANiW3N8G4JaKFpVBRPCWAjgy6XEfqvM6XQPwHMmXSXZFF1NAR3LdMwAcA9ARWcx01FZLqfK6zsz6SS4BsJvkweSoU5XMzEjOmimKiCNeP4Dlkx4vS56rKmbWn9wOAtiBApdvBhuYuNovuR0MrqdoEcF7CcBqkitJtgC4DcDOgDoKItmerBMDku0AvojqvHxzJ4BNyf1NAHoCa5mWin/UmtkYyc0AngXQCKDbzPZXuo4UHQB25C8pRhOAJ8zsmciCSG4HcD2ARST7ANwDYCuAp0negfxZPxvjKpwefXMhIfTNhYRQ8CSEgichFDwJoeBJCAVPQih4EkLBkxD/ATAuHmT6QhI/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "UserId = 6\n",
    "TaskId = 8\n",
    "\n",
    "data_point = dfAll[(dfAll.userID == UserId) & (dfAll.TaskID == TaskId)].iloc[0]\n",
    "\n",
    "plt.imshow(data_point.Gesture)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Label for image 350 is: 21')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 350\n",
    "plt.imshow(x_train[i].reshape(27, 15)) #np.sqrt(784) = 28\n",
    "plt.title(\"Label for image %i is: %s\" % (i, y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If GPU is not available: \n",
    "# GPU_USE = '/cpu:0'\n",
    "# config = tf.ConfigProto(device_count = {\"GPU\": 0})\n",
    "\n",
    "\n",
    "# If GPU is available: \n",
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = True\n",
    "config.allow_soft_placement = True\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "\n",
    "# Limit the maximum memory used\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.1\n",
    "\n",
    "# set session config\n",
    "tf.keras.backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 27, 15, 84)        840       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 27, 15, 84)        63588     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 7, 84)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 13, 7, 84)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 7, 42)         31794     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 7, 42)         15918     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 3, 42)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 3, 42)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 756)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 400)               302800    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               40100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 34)                3434      \n",
      "=================================================================\n",
      "Total params: 458,474\n",
      "Trainable params: 458,474\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3302 samples, validate on 1095 samples\n",
      "Epoch 1/500\n",
      "3302/3302 [==============================] - 4s 1ms/step - loss: 4.0488 - acc: 0.0878 - val_loss: 3.4622 - val_acc: 0.1297\n",
      "Epoch 2/500\n",
      "3302/3302 [==============================] - 2s 611us/step - loss: 3.1145 - acc: 0.1493 - val_loss: 2.7086 - val_acc: 0.2128\n",
      "Epoch 3/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 2.4913 - acc: 0.2835 - val_loss: 2.1874 - val_acc: 0.3187\n",
      "Epoch 4/500\n",
      "3302/3302 [==============================] - 2s 610us/step - loss: 2.0523 - acc: 0.3731 - val_loss: 1.7794 - val_acc: 0.4046\n",
      "Epoch 5/500\n",
      "3302/3302 [==============================] - 2s 606us/step - loss: 1.8868 - acc: 0.3970 - val_loss: 1.7174 - val_acc: 0.3936\n",
      "Epoch 6/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 1.5626 - acc: 0.4821 - val_loss: 1.7931 - val_acc: 0.4237\n",
      "Epoch 7/500\n",
      "3302/3302 [==============================] - 2s 615us/step - loss: 1.4713 - acc: 0.5082 - val_loss: 1.5521 - val_acc: 0.4594\n",
      "Epoch 8/500\n",
      "3302/3302 [==============================] - 2s 606us/step - loss: 1.2448 - acc: 0.5709 - val_loss: 1.5685 - val_acc: 0.4521\n",
      "Epoch 9/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 1.1892 - acc: 0.5966 - val_loss: 1.6470 - val_acc: 0.4639\n",
      "Epoch 10/500\n",
      "3302/3302 [==============================] - 2s 610us/step - loss: 1.0809 - acc: 0.6314 - val_loss: 1.6313 - val_acc: 0.4813\n",
      "Epoch 11/500\n",
      "3302/3302 [==============================] - 2s 601us/step - loss: 0.8977 - acc: 0.6850 - val_loss: 1.6502 - val_acc: 0.5160\n",
      "Epoch 12/500\n",
      "3302/3302 [==============================] - 2s 595us/step - loss: 0.8530 - acc: 0.7184 - val_loss: 1.4332 - val_acc: 0.5589\n",
      "Epoch 13/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 0.7281 - acc: 0.7544 - val_loss: 1.5247 - val_acc: 0.5744\n",
      "Epoch 14/500\n",
      "3302/3302 [==============================] - 2s 608us/step - loss: 0.6611 - acc: 0.7723 - val_loss: 1.6890 - val_acc: 0.5388\n",
      "Epoch 15/500\n",
      "3302/3302 [==============================] - 2s 599us/step - loss: 0.6211 - acc: 0.7941 - val_loss: 1.7154 - val_acc: 0.5388\n",
      "Epoch 16/500\n",
      "3302/3302 [==============================] - 2s 606us/step - loss: 0.5353 - acc: 0.8189 - val_loss: 1.4714 - val_acc: 0.5963\n",
      "Epoch 17/500\n",
      "3302/3302 [==============================] - 2s 595us/step - loss: 0.5232 - acc: 0.8216 - val_loss: 1.7329 - val_acc: 0.5717\n",
      "Epoch 18/500\n",
      "3302/3302 [==============================] - 2s 598us/step - loss: 0.4957 - acc: 0.8346 - val_loss: 1.5792 - val_acc: 0.6000\n",
      "Epoch 19/500\n",
      "3302/3302 [==============================] - 2s 606us/step - loss: 0.4383 - acc: 0.8465 - val_loss: 1.5736 - val_acc: 0.6055\n",
      "Epoch 20/500\n",
      "3302/3302 [==============================] - 2s 610us/step - loss: 0.3662 - acc: 0.8755 - val_loss: 1.4373 - val_acc: 0.6347\n",
      "Epoch 21/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 0.3918 - acc: 0.8740 - val_loss: 1.5570 - val_acc: 0.5863\n",
      "Epoch 22/500\n",
      "3302/3302 [==============================] - 2s 614us/step - loss: 0.4291 - acc: 0.8616 - val_loss: 1.5500 - val_acc: 0.6128\n",
      "Epoch 23/500\n",
      "3302/3302 [==============================] - 2s 590us/step - loss: 0.3414 - acc: 0.8940 - val_loss: 1.4824 - val_acc: 0.6521\n",
      "Epoch 24/500\n",
      "3302/3302 [==============================] - 2s 592us/step - loss: 0.2773 - acc: 0.9043 - val_loss: 1.6059 - val_acc: 0.6548\n",
      "Epoch 25/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.2950 - acc: 0.9049 - val_loss: 1.4983 - val_acc: 0.6603\n",
      "Epoch 26/500\n",
      "3302/3302 [==============================] - 2s 621us/step - loss: 0.2548 - acc: 0.9240 - val_loss: 1.6003 - val_acc: 0.6530\n",
      "Epoch 27/500\n",
      "3302/3302 [==============================] - 2s 611us/step - loss: 0.2291 - acc: 0.9303 - val_loss: 1.7494 - val_acc: 0.6484\n",
      "Epoch 28/500\n",
      "3302/3302 [==============================] - 2s 611us/step - loss: 0.2972 - acc: 0.9049 - val_loss: 1.6047 - val_acc: 0.6457\n",
      "Epoch 29/500\n",
      "3302/3302 [==============================] - 2s 597us/step - loss: 0.2147 - acc: 0.9264 - val_loss: 1.6934 - val_acc: 0.6457\n",
      "Epoch 30/500\n",
      "3302/3302 [==============================] - 2s 603us/step - loss: 0.2581 - acc: 0.9234 - val_loss: 1.6029 - val_acc: 0.6511\n",
      "Epoch 31/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 0.1903 - acc: 0.9364 - val_loss: 1.6774 - val_acc: 0.6740\n",
      "Epoch 32/500\n",
      "3302/3302 [==============================] - 2s 625us/step - loss: 0.1729 - acc: 0.9452 - val_loss: 1.6608 - val_acc: 0.6594\n",
      "Epoch 33/500\n",
      "3302/3302 [==============================] - 2s 594us/step - loss: 0.2190 - acc: 0.9331 - val_loss: 1.5959 - val_acc: 0.6648\n",
      "Epoch 34/500\n",
      "3302/3302 [==============================] - 2s 609us/step - loss: 0.2180 - acc: 0.9316 - val_loss: 1.6313 - val_acc: 0.6521\n",
      "Epoch 35/500\n",
      "3302/3302 [==============================] - 3s 777us/step - loss: 0.1986 - acc: 0.9437 - val_loss: 1.5226 - val_acc: 0.6731\n",
      "Epoch 36/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.1439 - acc: 0.9603 - val_loss: 2.0805 - val_acc: 0.6311\n",
      "Epoch 37/500\n",
      "3302/3302 [==============================] - 2s 612us/step - loss: 0.1709 - acc: 0.9479 - val_loss: 1.7835 - val_acc: 0.6639\n",
      "Epoch 38/500\n",
      "3302/3302 [==============================] - 2s 619us/step - loss: 0.1965 - acc: 0.9434 - val_loss: 1.5729 - val_acc: 0.6648\n",
      "Epoch 39/500\n",
      "3302/3302 [==============================] - 2s 604us/step - loss: 0.1811 - acc: 0.9494 - val_loss: 1.7205 - val_acc: 0.6511\n",
      "Epoch 40/500\n",
      "3302/3302 [==============================] - 2s 597us/step - loss: 0.1543 - acc: 0.9525 - val_loss: 2.1203 - val_acc: 0.6073\n",
      "Epoch 41/500\n",
      "3302/3302 [==============================] - 2s 596us/step - loss: 0.1683 - acc: 0.9497 - val_loss: 1.8988 - val_acc: 0.6311\n",
      "Epoch 42/500\n",
      "3302/3302 [==============================] - 2s 594us/step - loss: 0.2359 - acc: 0.9310 - val_loss: 1.5703 - val_acc: 0.6667\n",
      "Epoch 43/500\n",
      "3302/3302 [==============================] - 2s 604us/step - loss: 0.1501 - acc: 0.9488 - val_loss: 1.9249 - val_acc: 0.6356\n",
      "Epoch 44/500\n",
      "3302/3302 [==============================] - 2s 604us/step - loss: 0.1906 - acc: 0.9425 - val_loss: 1.7685 - val_acc: 0.6694\n",
      "Epoch 45/500\n",
      "3302/3302 [==============================] - 2s 598us/step - loss: 0.2491 - acc: 0.9267 - val_loss: 1.5614 - val_acc: 0.6849\n",
      "Epoch 46/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 0.1374 - acc: 0.9603 - val_loss: 1.6614 - val_acc: 0.6776\n",
      "Epoch 47/500\n",
      "3302/3302 [==============================] - 2s 611us/step - loss: 0.1727 - acc: 0.9476 - val_loss: 1.6551 - val_acc: 0.6566\n",
      "Epoch 48/500\n",
      "3302/3302 [==============================] - 2s 616us/step - loss: 0.0996 - acc: 0.9700 - val_loss: 1.8698 - val_acc: 0.6731\n",
      "Epoch 49/500\n",
      "3302/3302 [==============================] - 2s 614us/step - loss: 0.1001 - acc: 0.9706 - val_loss: 1.8923 - val_acc: 0.6776\n",
      "Epoch 50/500\n",
      "3302/3302 [==============================] - 2s 606us/step - loss: 0.0893 - acc: 0.9721 - val_loss: 1.9552 - val_acc: 0.6557\n",
      "Epoch 51/500\n",
      "3302/3302 [==============================] - 2s 598us/step - loss: 0.1129 - acc: 0.9658 - val_loss: 1.9060 - val_acc: 0.6758\n",
      "Epoch 52/500\n",
      "3302/3302 [==============================] - 2s 615us/step - loss: 0.1097 - acc: 0.9691 - val_loss: 1.8452 - val_acc: 0.6877\n",
      "Epoch 53/500\n",
      "3302/3302 [==============================] - 2s 601us/step - loss: 0.1017 - acc: 0.9697 - val_loss: 1.7939 - val_acc: 0.6950\n",
      "Epoch 54/500\n",
      "3302/3302 [==============================] - 2s 591us/step - loss: 0.1141 - acc: 0.9688 - val_loss: 1.9618 - val_acc: 0.6767\n",
      "Epoch 55/500\n",
      "3302/3302 [==============================] - 2s 597us/step - loss: 0.1381 - acc: 0.9612 - val_loss: 1.9405 - val_acc: 0.6822\n",
      "Epoch 56/500\n",
      "3302/3302 [==============================] - 2s 614us/step - loss: 0.1160 - acc: 0.9682 - val_loss: 2.0857 - val_acc: 0.6795\n",
      "Epoch 57/500\n",
      "3302/3302 [==============================] - 2s 611us/step - loss: 0.1267 - acc: 0.9609 - val_loss: 2.2655 - val_acc: 0.6183\n",
      "Epoch 58/500\n",
      "3302/3302 [==============================] - 2s 613us/step - loss: 0.1415 - acc: 0.9631 - val_loss: 2.6556 - val_acc: 0.6100\n",
      "Epoch 59/500\n",
      "3302/3302 [==============================] - 2s 609us/step - loss: 0.2149 - acc: 0.9443 - val_loss: 1.9040 - val_acc: 0.6694\n",
      "Epoch 60/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.1640 - acc: 0.9576 - val_loss: 1.7919 - val_acc: 0.6849\n",
      "Epoch 61/500\n",
      "3302/3302 [==============================] - 2s 619us/step - loss: 0.1681 - acc: 0.9534 - val_loss: 1.8507 - val_acc: 0.6767\n",
      "Epoch 62/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 0.0942 - acc: 0.9737 - val_loss: 2.0890 - val_acc: 0.6438\n",
      "Epoch 63/500\n",
      "3302/3302 [==============================] - 2s 617us/step - loss: 0.0984 - acc: 0.9718 - val_loss: 1.8717 - val_acc: 0.6813\n",
      "Epoch 64/500\n",
      "3302/3302 [==============================] - 2s 606us/step - loss: 0.1083 - acc: 0.9667 - val_loss: 1.9944 - val_acc: 0.6886\n",
      "Epoch 65/500\n",
      "3302/3302 [==============================] - 2s 595us/step - loss: 0.0885 - acc: 0.9767 - val_loss: 2.1812 - val_acc: 0.6639\n",
      "Epoch 66/500\n",
      "3302/3302 [==============================] - 2s 601us/step - loss: 0.4476 - acc: 0.8913 - val_loss: 1.5613 - val_acc: 0.6521\n",
      "Epoch 67/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 0.3131 - acc: 0.9140 - val_loss: 1.5031 - val_acc: 0.6932\n",
      "Epoch 68/500\n",
      "3302/3302 [==============================] - 2s 604us/step - loss: 0.1711 - acc: 0.9552 - val_loss: 1.6214 - val_acc: 0.6685\n",
      "Epoch 69/500\n",
      "3302/3302 [==============================] - 2s 594us/step - loss: 0.0738 - acc: 0.9764 - val_loss: 1.7093 - val_acc: 0.6712\n",
      "Epoch 70/500\n",
      "3302/3302 [==============================] - 2s 598us/step - loss: 0.0905 - acc: 0.9779 - val_loss: 1.8859 - val_acc: 0.6557\n",
      "Epoch 71/500\n",
      "3302/3302 [==============================] - 2s 599us/step - loss: 0.1241 - acc: 0.9652 - val_loss: 1.8015 - val_acc: 0.6804\n",
      "Epoch 72/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 0.0687 - acc: 0.9797 - val_loss: 1.8297 - val_acc: 0.6822\n",
      "Epoch 73/500\n",
      "3302/3302 [==============================] - 2s 590us/step - loss: 0.0688 - acc: 0.9852 - val_loss: 1.9570 - val_acc: 0.6721\n",
      "Epoch 74/500\n",
      "3302/3302 [==============================] - 2s 608us/step - loss: 0.0607 - acc: 0.9815 - val_loss: 1.8914 - val_acc: 0.6639\n",
      "Epoch 75/500\n",
      "3302/3302 [==============================] - 2s 599us/step - loss: 0.2251 - acc: 0.9422 - val_loss: 1.7558 - val_acc: 0.6758\n",
      "Epoch 76/500\n",
      "3302/3302 [==============================] - 2s 621us/step - loss: 0.1087 - acc: 0.9649 - val_loss: 1.7074 - val_acc: 0.6767\n",
      "Epoch 77/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 0.0978 - acc: 0.9712 - val_loss: 1.8227 - val_acc: 0.6804\n",
      "Epoch 78/500\n",
      "3302/3302 [==============================] - 2s 594us/step - loss: 0.0596 - acc: 0.9843 - val_loss: 2.0442 - val_acc: 0.6813\n",
      "Epoch 79/500\n",
      "3302/3302 [==============================] - 2s 597us/step - loss: 0.0763 - acc: 0.9788 - val_loss: 1.8735 - val_acc: 0.6895\n",
      "Epoch 80/500\n",
      "3302/3302 [==============================] - 2s 588us/step - loss: 0.0528 - acc: 0.9839 - val_loss: 1.8625 - val_acc: 0.6950\n",
      "Epoch 81/500\n",
      "3302/3302 [==============================] - 2s 597us/step - loss: 0.0696 - acc: 0.9785 - val_loss: 1.7794 - val_acc: 0.6795\n",
      "Epoch 82/500\n",
      "3302/3302 [==============================] - 2s 593us/step - loss: 0.0538 - acc: 0.9873 - val_loss: 2.0298 - val_acc: 0.6740\n",
      "Epoch 83/500\n",
      "3302/3302 [==============================] - 2s 596us/step - loss: 0.1156 - acc: 0.9688 - val_loss: 2.2031 - val_acc: 0.6694\n",
      "Epoch 84/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 0.2295 - acc: 0.9425 - val_loss: 1.8205 - val_acc: 0.6858\n",
      "Epoch 85/500\n",
      "3302/3302 [==============================] - 2s 617us/step - loss: 0.1407 - acc: 0.9646 - val_loss: 1.8165 - val_acc: 0.6831\n",
      "Epoch 86/500\n",
      "3302/3302 [==============================] - 2s 596us/step - loss: 0.0861 - acc: 0.9785 - val_loss: 1.9973 - val_acc: 0.6749\n",
      "Epoch 87/500\n",
      "3302/3302 [==============================] - 2s 598us/step - loss: 0.1299 - acc: 0.9676 - val_loss: 2.2351 - val_acc: 0.6566\n",
      "Epoch 88/500\n",
      "3302/3302 [==============================] - 2s 598us/step - loss: 0.1060 - acc: 0.9709 - val_loss: 2.0205 - val_acc: 0.6703\n",
      "Epoch 89/500\n",
      "3302/3302 [==============================] - 2s 590us/step - loss: 0.0642 - acc: 0.9815 - val_loss: 1.9958 - val_acc: 0.6731\n",
      "Epoch 90/500\n",
      "3302/3302 [==============================] - 2s 606us/step - loss: 0.0404 - acc: 0.9900 - val_loss: 1.8789 - val_acc: 0.6932\n",
      "Epoch 91/500\n",
      "3302/3302 [==============================] - 2s 598us/step - loss: 0.0390 - acc: 0.9903 - val_loss: 2.2489 - val_acc: 0.6502\n",
      "Epoch 92/500\n",
      "3302/3302 [==============================] - 2s 612us/step - loss: 0.0730 - acc: 0.9800 - val_loss: 2.1238 - val_acc: 0.6858\n",
      "Epoch 93/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.0552 - acc: 0.9852 - val_loss: 2.1544 - val_acc: 0.6685\n",
      "Epoch 94/500\n",
      "3302/3302 [==============================] - 2s 610us/step - loss: 0.0754 - acc: 0.9764 - val_loss: 2.1367 - val_acc: 0.6721\n",
      "Epoch 95/500\n",
      "3302/3302 [==============================] - 2s 591us/step - loss: 0.0561 - acc: 0.9830 - val_loss: 1.9674 - val_acc: 0.6822\n",
      "Epoch 96/500\n",
      "3302/3302 [==============================] - 2s 609us/step - loss: 0.0798 - acc: 0.9830 - val_loss: 1.7645 - val_acc: 0.6822\n",
      "Epoch 97/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.0627 - acc: 0.9855 - val_loss: 1.8464 - val_acc: 0.7078\n",
      "Epoch 98/500\n",
      "3302/3302 [==============================] - 2s 594us/step - loss: 0.0579 - acc: 0.9882 - val_loss: 2.1247 - val_acc: 0.6858\n",
      "Epoch 99/500\n",
      "3302/3302 [==============================] - 2s 592us/step - loss: 0.0615 - acc: 0.9864 - val_loss: 1.8659 - val_acc: 0.7041\n",
      "Epoch 100/500\n",
      "3302/3302 [==============================] - 2s 606us/step - loss: 0.0650 - acc: 0.9836 - val_loss: 1.9705 - val_acc: 0.6785\n",
      "Epoch 101/500\n",
      "3302/3302 [==============================] - 2s 601us/step - loss: 0.0355 - acc: 0.9906 - val_loss: 2.1598 - val_acc: 0.6767\n",
      "Epoch 102/500\n",
      "3302/3302 [==============================] - 2s 617us/step - loss: 0.0170 - acc: 0.9939 - val_loss: 2.1744 - val_acc: 0.6968\n",
      "Epoch 103/500\n",
      "3302/3302 [==============================] - 2s 585us/step - loss: 0.0386 - acc: 0.9897 - val_loss: 2.1037 - val_acc: 0.6922\n",
      "Epoch 104/500\n",
      "3302/3302 [==============================] - 2s 589us/step - loss: 0.0437 - acc: 0.9858 - val_loss: 2.2274 - val_acc: 0.6858\n",
      "Epoch 105/500\n",
      "3302/3302 [==============================] - 2s 604us/step - loss: 0.0534 - acc: 0.9852 - val_loss: 2.2141 - val_acc: 0.6804\n",
      "Epoch 106/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.0781 - acc: 0.9809 - val_loss: 2.3555 - val_acc: 0.6521\n",
      "Epoch 107/500\n",
      "3302/3302 [==============================] - 2s 601us/step - loss: 0.1208 - acc: 0.9670 - val_loss: 3.0317 - val_acc: 0.6073\n",
      "Epoch 108/500\n",
      "3302/3302 [==============================] - 2s 598us/step - loss: 0.1367 - acc: 0.9621 - val_loss: 1.8803 - val_acc: 0.6822\n",
      "Epoch 109/500\n",
      "3302/3302 [==============================] - 2s 596us/step - loss: 0.1074 - acc: 0.9709 - val_loss: 1.8847 - val_acc: 0.6904\n",
      "Epoch 110/500\n",
      "3302/3302 [==============================] - 2s 618us/step - loss: 0.1064 - acc: 0.9727 - val_loss: 1.8947 - val_acc: 0.6749\n",
      "Epoch 111/500\n",
      "3302/3302 [==============================] - 2s 608us/step - loss: 0.0452 - acc: 0.9855 - val_loss: 1.9044 - val_acc: 0.7005\n",
      "Epoch 112/500\n",
      "3302/3302 [==============================] - 2s 612us/step - loss: 0.0397 - acc: 0.9867 - val_loss: 2.1282 - val_acc: 0.6804\n",
      "Epoch 113/500\n",
      "3302/3302 [==============================] - 2s 594us/step - loss: 0.0290 - acc: 0.9912 - val_loss: 2.1198 - val_acc: 0.7050\n",
      "Epoch 114/500\n",
      "3302/3302 [==============================] - 2s 603us/step - loss: 0.0622 - acc: 0.9824 - val_loss: 2.4049 - val_acc: 0.6612\n",
      "Epoch 115/500\n",
      "3302/3302 [==============================] - 2s 610us/step - loss: 0.0794 - acc: 0.9821 - val_loss: 2.1996 - val_acc: 0.6731\n",
      "Epoch 116/500\n",
      "3302/3302 [==============================] - 2s 596us/step - loss: 0.0687 - acc: 0.9833 - val_loss: 2.3154 - val_acc: 0.6712\n",
      "Epoch 117/500\n",
      "3302/3302 [==============================] - 2s 606us/step - loss: 0.0840 - acc: 0.9818 - val_loss: 2.2835 - val_acc: 0.6758\n",
      "Epoch 118/500\n",
      "3302/3302 [==============================] - 2s 603us/step - loss: 0.1147 - acc: 0.9764 - val_loss: 2.2647 - val_acc: 0.6658\n",
      "Epoch 119/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 0.1763 - acc: 0.9606 - val_loss: 2.4822 - val_acc: 0.6402\n",
      "Epoch 120/500\n",
      "3302/3302 [==============================] - 2s 594us/step - loss: 0.0692 - acc: 0.9803 - val_loss: 2.3507 - val_acc: 0.6521\n",
      "Epoch 121/500\n",
      "3302/3302 [==============================] - 2s 614us/step - loss: 0.0501 - acc: 0.9870 - val_loss: 2.4672 - val_acc: 0.6639\n",
      "Epoch 122/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.0607 - acc: 0.9839 - val_loss: 2.2276 - val_acc: 0.6813\n",
      "Epoch 123/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 0.0487 - acc: 0.9867 - val_loss: 2.2621 - val_acc: 0.6804\n",
      "Epoch 124/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 0.1582 - acc: 0.9573 - val_loss: 2.5410 - val_acc: 0.6566\n",
      "Epoch 125/500\n",
      "3302/3302 [==============================] - 2s 608us/step - loss: 0.2246 - acc: 0.9382 - val_loss: 2.3128 - val_acc: 0.6493\n",
      "Epoch 126/500\n",
      "3302/3302 [==============================] - 2s 600us/step - loss: 0.0956 - acc: 0.9740 - val_loss: 2.2735 - val_acc: 0.6658\n",
      "Epoch 127/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 0.0497 - acc: 0.9864 - val_loss: 2.4621 - val_acc: 0.6694\n",
      "Epoch 128/500\n",
      "3302/3302 [==============================] - 2s 610us/step - loss: 0.0612 - acc: 0.9843 - val_loss: 2.3902 - val_acc: 0.6694\n",
      "Epoch 129/500\n",
      "3302/3302 [==============================] - 2s 609us/step - loss: 0.0814 - acc: 0.9812 - val_loss: 2.3556 - val_acc: 0.6566\n",
      "Epoch 130/500\n",
      "3302/3302 [==============================] - 2s 616us/step - loss: 0.1924 - acc: 0.9455 - val_loss: 1.9173 - val_acc: 0.6868\n",
      "Epoch 131/500\n",
      "3302/3302 [==============================] - 2s 599us/step - loss: 0.0834 - acc: 0.9740 - val_loss: 2.2374 - val_acc: 0.6840\n",
      "Epoch 132/500\n",
      "3302/3302 [==============================] - 2s 611us/step - loss: 0.0551 - acc: 0.9830 - val_loss: 2.2827 - val_acc: 0.6913\n",
      "Epoch 133/500\n",
      "3302/3302 [==============================] - 2s 583us/step - loss: 0.1278 - acc: 0.9718 - val_loss: 2.6702 - val_acc: 0.6539\n",
      "Epoch 134/500\n",
      "3302/3302 [==============================] - 2s 613us/step - loss: 0.0766 - acc: 0.9803 - val_loss: 2.2756 - val_acc: 0.6731\n",
      "Epoch 135/500\n",
      "3302/3302 [==============================] - 2s 612us/step - loss: 0.0918 - acc: 0.9703 - val_loss: 2.2670 - val_acc: 0.6712\n",
      "Epoch 136/500\n",
      "3302/3302 [==============================] - 2s 590us/step - loss: 0.0669 - acc: 0.9821 - val_loss: 2.2026 - val_acc: 0.6721\n",
      "Epoch 137/500\n",
      "3302/3302 [==============================] - 2s 591us/step - loss: 0.0437 - acc: 0.9879 - val_loss: 2.3029 - val_acc: 0.6886\n",
      "Epoch 138/500\n",
      "3302/3302 [==============================] - 2s 588us/step - loss: 0.0284 - acc: 0.9903 - val_loss: 2.1308 - val_acc: 0.7014\n",
      "Epoch 139/500\n",
      "3302/3302 [==============================] - 2s 610us/step - loss: 0.0265 - acc: 0.9906 - val_loss: 2.2565 - val_acc: 0.6886\n",
      "Epoch 140/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.0347 - acc: 0.9909 - val_loss: 2.3525 - val_acc: 0.6804\n",
      "Epoch 141/500\n",
      "3302/3302 [==============================] - 2s 610us/step - loss: 0.0250 - acc: 0.9921 - val_loss: 2.4741 - val_acc: 0.6767\n",
      "Epoch 142/500\n",
      "3302/3302 [==============================] - 2s 604us/step - loss: 0.0271 - acc: 0.9927 - val_loss: 2.5655 - val_acc: 0.6758\n",
      "Epoch 143/500\n",
      "3302/3302 [==============================] - 2s 614us/step - loss: 0.0119 - acc: 0.9949 - val_loss: 2.5620 - val_acc: 0.6804\n",
      "Epoch 144/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 0.0194 - acc: 0.9939 - val_loss: 2.6540 - val_acc: 0.6749\n",
      "Epoch 145/500\n",
      "3302/3302 [==============================] - 2s 596us/step - loss: 0.0357 - acc: 0.9912 - val_loss: 2.3106 - val_acc: 0.6977\n",
      "Epoch 146/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.0269 - acc: 0.9909 - val_loss: 2.3931 - val_acc: 0.6877\n",
      "Epoch 147/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 0.0447 - acc: 0.9879 - val_loss: 2.5185 - val_acc: 0.6685\n",
      "Epoch 148/500\n",
      "3302/3302 [==============================] - 2s 609us/step - loss: 0.0281 - acc: 0.9903 - val_loss: 2.3707 - val_acc: 0.6813\n",
      "Epoch 149/500\n",
      "3302/3302 [==============================] - 2s 595us/step - loss: 0.0245 - acc: 0.9942 - val_loss: 2.7510 - val_acc: 0.6575\n",
      "Epoch 150/500\n",
      "3302/3302 [==============================] - 2s 617us/step - loss: 0.0584 - acc: 0.9873 - val_loss: 2.3741 - val_acc: 0.6849\n",
      "Epoch 151/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 0.0471 - acc: 0.9873 - val_loss: 2.6703 - val_acc: 0.6685\n",
      "Epoch 152/500\n",
      "3302/3302 [==============================] - 2s 594us/step - loss: 0.0347 - acc: 0.9879 - val_loss: 2.3706 - val_acc: 0.6932\n",
      "Epoch 153/500\n",
      "3302/3302 [==============================] - 2s 594us/step - loss: 0.0762 - acc: 0.9809 - val_loss: 2.7573 - val_acc: 0.6594\n",
      "Epoch 154/500\n",
      "3302/3302 [==============================] - 2s 590us/step - loss: 0.1437 - acc: 0.9624 - val_loss: 2.4609 - val_acc: 0.6521\n",
      "Epoch 155/500\n",
      "3302/3302 [==============================] - 2s 597us/step - loss: 0.0555 - acc: 0.9815 - val_loss: 2.5399 - val_acc: 0.6521\n",
      "Epoch 156/500\n",
      "3302/3302 [==============================] - 2s 600us/step - loss: 0.0783 - acc: 0.9818 - val_loss: 3.1761 - val_acc: 0.6119\n",
      "Epoch 157/500\n",
      "3302/3302 [==============================] - 2s 606us/step - loss: 0.3579 - acc: 0.9297 - val_loss: 2.4904 - val_acc: 0.6402\n",
      "Epoch 158/500\n",
      "3302/3302 [==============================] - 2s 610us/step - loss: 0.1084 - acc: 0.9721 - val_loss: 2.2573 - val_acc: 0.6740\n",
      "Epoch 159/500\n",
      "3302/3302 [==============================] - 2s 616us/step - loss: 0.0540 - acc: 0.9864 - val_loss: 2.4889 - val_acc: 0.6785\n",
      "Epoch 160/500\n",
      "3302/3302 [==============================] - 2s 600us/step - loss: 0.0426 - acc: 0.9882 - val_loss: 2.3954 - val_acc: 0.6932\n",
      "Epoch 161/500\n",
      "3302/3302 [==============================] - 2s 606us/step - loss: 0.0546 - acc: 0.9879 - val_loss: 2.4940 - val_acc: 0.6731\n",
      "Epoch 162/500\n",
      "3302/3302 [==============================] - 2s 603us/step - loss: 0.0452 - acc: 0.9888 - val_loss: 2.7606 - val_acc: 0.6603\n",
      "Epoch 163/500\n",
      "3302/3302 [==============================] - 2s 604us/step - loss: 0.1538 - acc: 0.9646 - val_loss: 2.4980 - val_acc: 0.6612\n",
      "Epoch 164/500\n",
      "3302/3302 [==============================] - 2s 596us/step - loss: 0.1346 - acc: 0.9637 - val_loss: 2.4061 - val_acc: 0.6639\n",
      "Epoch 165/500\n",
      "3302/3302 [==============================] - 2s 599us/step - loss: 0.0629 - acc: 0.9861 - val_loss: 2.3794 - val_acc: 0.6758\n",
      "Epoch 166/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 0.0638 - acc: 0.9830 - val_loss: 2.2765 - val_acc: 0.6785\n",
      "Epoch 167/500\n",
      "3302/3302 [==============================] - 2s 608us/step - loss: 0.0677 - acc: 0.9858 - val_loss: 2.4859 - val_acc: 0.6703\n",
      "Epoch 168/500\n",
      "3302/3302 [==============================] - 2s 611us/step - loss: 0.0486 - acc: 0.9879 - val_loss: 2.2527 - val_acc: 0.7087\n",
      "Epoch 169/500\n",
      "3302/3302 [==============================] - 2s 599us/step - loss: 0.0431 - acc: 0.9888 - val_loss: 2.4336 - val_acc: 0.6831\n",
      "Epoch 170/500\n",
      "3302/3302 [==============================] - 2s 595us/step - loss: 0.0357 - acc: 0.9918 - val_loss: 2.4863 - val_acc: 0.6776\n",
      "Epoch 171/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.0203 - acc: 0.9949 - val_loss: 2.6063 - val_acc: 0.6694\n",
      "Epoch 172/500\n",
      "3302/3302 [==============================] - 2s 595us/step - loss: 0.0374 - acc: 0.9891 - val_loss: 2.4480 - val_acc: 0.6913\n",
      "Epoch 173/500\n",
      "3302/3302 [==============================] - 2s 595us/step - loss: 0.0711 - acc: 0.9858 - val_loss: 2.5893 - val_acc: 0.6703\n",
      "Epoch 174/500\n",
      "3302/3302 [==============================] - 2s 601us/step - loss: 0.0347 - acc: 0.9906 - val_loss: 2.3577 - val_acc: 0.6849\n",
      "Epoch 175/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 0.1119 - acc: 0.9730 - val_loss: 2.7969 - val_acc: 0.6594\n",
      "Epoch 176/500\n",
      "3302/3302 [==============================] - 2s 610us/step - loss: 0.1137 - acc: 0.9767 - val_loss: 2.7765 - val_acc: 0.6566\n",
      "Epoch 177/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.0417 - acc: 0.9867 - val_loss: 2.3065 - val_acc: 0.6877\n",
      "Epoch 178/500\n",
      "3302/3302 [==============================] - 2s 595us/step - loss: 0.0178 - acc: 0.9952 - val_loss: 2.4560 - val_acc: 0.6922\n",
      "Epoch 179/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 0.0243 - acc: 0.9933 - val_loss: 2.6811 - val_acc: 0.6685\n",
      "Epoch 180/500\n",
      "3302/3302 [==============================] - 2s 610us/step - loss: 0.0472 - acc: 0.9900 - val_loss: 2.7662 - val_acc: 0.6648\n",
      "Epoch 181/500\n",
      "3302/3302 [==============================] - 2s 593us/step - loss: 0.0318 - acc: 0.9942 - val_loss: 2.6131 - val_acc: 0.6721\n",
      "Epoch 182/500\n",
      "3302/3302 [==============================] - 2s 606us/step - loss: 0.1128 - acc: 0.9764 - val_loss: 2.6189 - val_acc: 0.6630\n",
      "Epoch 183/500\n",
      "3302/3302 [==============================] - 2s 598us/step - loss: 0.2610 - acc: 0.9476 - val_loss: 2.3020 - val_acc: 0.6676\n",
      "Epoch 184/500\n",
      "3302/3302 [==============================] - 2s 593us/step - loss: 0.0722 - acc: 0.9794 - val_loss: 2.2602 - val_acc: 0.6740\n",
      "Epoch 185/500\n",
      "3302/3302 [==============================] - 2s 600us/step - loss: 0.0390 - acc: 0.9894 - val_loss: 2.4138 - val_acc: 0.6795\n",
      "Epoch 186/500\n",
      "3302/3302 [==============================] - 2s 608us/step - loss: 0.0249 - acc: 0.9918 - val_loss: 2.2911 - val_acc: 0.6676\n",
      "Epoch 187/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 0.0309 - acc: 0.9915 - val_loss: 2.3133 - val_acc: 0.6986\n",
      "Epoch 188/500\n",
      "3302/3302 [==============================] - 2s 588us/step - loss: 0.0459 - acc: 0.9885 - val_loss: 2.3579 - val_acc: 0.6922\n",
      "Epoch 189/500\n",
      "3302/3302 [==============================] - 2s 615us/step - loss: 0.0530 - acc: 0.9858 - val_loss: 2.4196 - val_acc: 0.6557\n",
      "Epoch 190/500\n",
      "3302/3302 [==============================] - 2s 590us/step - loss: 0.0561 - acc: 0.9870 - val_loss: 2.5564 - val_acc: 0.6858\n",
      "Epoch 191/500\n",
      "3302/3302 [==============================] - 2s 593us/step - loss: 0.0451 - acc: 0.9867 - val_loss: 2.7805 - val_acc: 0.6785\n",
      "Epoch 192/500\n",
      "3302/3302 [==============================] - 2s 591us/step - loss: 0.0699 - acc: 0.9849 - val_loss: 2.5230 - val_acc: 0.6822\n",
      "Epoch 193/500\n",
      "3302/3302 [==============================] - 2s 595us/step - loss: 0.0499 - acc: 0.9864 - val_loss: 2.5267 - val_acc: 0.6877\n",
      "Epoch 194/500\n",
      "3302/3302 [==============================] - 2s 604us/step - loss: 0.0375 - acc: 0.9894 - val_loss: 2.2031 - val_acc: 0.7151\n",
      "Epoch 195/500\n",
      "3302/3302 [==============================] - 2s 596us/step - loss: 0.0318 - acc: 0.9903 - val_loss: 2.3072 - val_acc: 0.7041\n",
      "Epoch 196/500\n",
      "3302/3302 [==============================] - 2s 596us/step - loss: 0.0445 - acc: 0.9906 - val_loss: 2.3665 - val_acc: 0.6858\n",
      "Epoch 197/500\n",
      "3302/3302 [==============================] - 2s 610us/step - loss: 0.0363 - acc: 0.9915 - val_loss: 2.2486 - val_acc: 0.7023\n",
      "Epoch 198/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 0.0507 - acc: 0.9876 - val_loss: 2.4505 - val_acc: 0.7059\n",
      "Epoch 199/500\n",
      "3302/3302 [==============================] - 2s 596us/step - loss: 0.0615 - acc: 0.9852 - val_loss: 2.4262 - val_acc: 0.6904\n",
      "Epoch 200/500\n",
      "3302/3302 [==============================] - 2s 593us/step - loss: 0.0374 - acc: 0.9894 - val_loss: 2.8100 - val_acc: 0.6648\n",
      "Epoch 201/500\n",
      "3302/3302 [==============================] - 2s 611us/step - loss: 0.0429 - acc: 0.9891 - val_loss: 2.4044 - val_acc: 0.6968\n",
      "Epoch 202/500\n",
      "3302/3302 [==============================] - 2s 609us/step - loss: 0.0205 - acc: 0.9921 - val_loss: 2.3770 - val_acc: 0.7050\n",
      "Epoch 203/500\n",
      "3302/3302 [==============================] - 2s 599us/step - loss: 0.0325 - acc: 0.9909 - val_loss: 2.6786 - val_acc: 0.6877\n",
      "Epoch 204/500\n",
      "3302/3302 [==============================] - 2s 608us/step - loss: 0.0727 - acc: 0.9836 - val_loss: 2.9546 - val_acc: 0.6603\n",
      "Epoch 205/500\n",
      "3302/3302 [==============================] - 2s 592us/step - loss: 0.0962 - acc: 0.9830 - val_loss: 2.6632 - val_acc: 0.6932\n",
      "Epoch 206/500\n",
      "3302/3302 [==============================] - 2s 620us/step - loss: 0.0956 - acc: 0.9812 - val_loss: 2.4512 - val_acc: 0.6904\n",
      "Epoch 207/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.0460 - acc: 0.9852 - val_loss: 2.4172 - val_acc: 0.6977\n",
      "Epoch 208/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.0201 - acc: 0.9939 - val_loss: 2.4127 - val_acc: 0.6995\n",
      "Epoch 209/500\n",
      "3302/3302 [==============================] - 2s 604us/step - loss: 0.0245 - acc: 0.9939 - val_loss: 2.7103 - val_acc: 0.6877\n",
      "Epoch 210/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 0.0177 - acc: 0.9945 - val_loss: 2.8058 - val_acc: 0.6776\n",
      "Epoch 211/500\n",
      "3302/3302 [==============================] - 2s 597us/step - loss: 0.0399 - acc: 0.9906 - val_loss: 2.9586 - val_acc: 0.6630\n",
      "Epoch 212/500\n",
      "3302/3302 [==============================] - 2s 601us/step - loss: 0.0461 - acc: 0.9903 - val_loss: 2.6966 - val_acc: 0.6922\n",
      "Epoch 213/500\n",
      "3302/3302 [==============================] - 2s 608us/step - loss: 0.0341 - acc: 0.9936 - val_loss: 2.5751 - val_acc: 0.6977\n",
      "Epoch 214/500\n",
      "3302/3302 [==============================] - 2s 584us/step - loss: 0.0262 - acc: 0.9921 - val_loss: 2.5830 - val_acc: 0.7005\n",
      "Epoch 215/500\n",
      "3302/3302 [==============================] - 2s 599us/step - loss: 0.0338 - acc: 0.9924 - val_loss: 2.7213 - val_acc: 0.6849\n",
      "Epoch 216/500\n",
      "3302/3302 [==============================] - 2s 609us/step - loss: 0.0394 - acc: 0.9897 - val_loss: 2.7312 - val_acc: 0.6804\n",
      "Epoch 217/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 0.0320 - acc: 0.9930 - val_loss: 2.4777 - val_acc: 0.7096\n",
      "Epoch 218/500\n",
      "3302/3302 [==============================] - 2s 601us/step - loss: 0.0329 - acc: 0.9933 - val_loss: 2.6773 - val_acc: 0.6950\n",
      "Epoch 219/500\n",
      "3302/3302 [==============================] - 2s 600us/step - loss: 0.0579 - acc: 0.9876 - val_loss: 2.7232 - val_acc: 0.6795\n",
      "Epoch 220/500\n",
      "3302/3302 [==============================] - 2s 591us/step - loss: 0.0466 - acc: 0.9885 - val_loss: 2.4973 - val_acc: 0.7059\n",
      "Epoch 221/500\n",
      "3302/3302 [==============================] - 2s 594us/step - loss: 0.0255 - acc: 0.9918 - val_loss: 2.5495 - val_acc: 0.6922\n",
      "Epoch 222/500\n",
      "3302/3302 [==============================] - 2s 609us/step - loss: 0.0314 - acc: 0.9897 - val_loss: 2.8203 - val_acc: 0.6676\n",
      "Epoch 223/500\n",
      "3302/3302 [==============================] - 2s 606us/step - loss: 0.0414 - acc: 0.9897 - val_loss: 2.8203 - val_acc: 0.6594\n",
      "Epoch 224/500\n",
      "3302/3302 [==============================] - 2s 595us/step - loss: 0.0181 - acc: 0.9939 - val_loss: 2.7313 - val_acc: 0.6831\n",
      "Epoch 225/500\n",
      "3302/3302 [==============================] - 2s 600us/step - loss: 0.0232 - acc: 0.9936 - val_loss: 2.6005 - val_acc: 0.6849\n",
      "Epoch 226/500\n",
      "3302/3302 [==============================] - 2s 589us/step - loss: 0.0367 - acc: 0.9909 - val_loss: 3.0611 - val_acc: 0.6420\n",
      "Epoch 227/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 0.0946 - acc: 0.9809 - val_loss: 2.6921 - val_acc: 0.6831\n",
      "Epoch 228/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.1318 - acc: 0.9737 - val_loss: 2.8730 - val_acc: 0.6795\n",
      "Epoch 229/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.0790 - acc: 0.9815 - val_loss: 3.0986 - val_acc: 0.6694\n",
      "Epoch 230/500\n",
      "3302/3302 [==============================] - 2s 601us/step - loss: 0.0608 - acc: 0.9867 - val_loss: 3.2699 - val_acc: 0.6411\n",
      "Epoch 231/500\n",
      "3302/3302 [==============================] - 2s 608us/step - loss: 0.1694 - acc: 0.9631 - val_loss: 3.1593 - val_acc: 0.6648\n",
      "Epoch 232/500\n",
      "3302/3302 [==============================] - 2s 587us/step - loss: 0.3321 - acc: 0.9385 - val_loss: 2.8123 - val_acc: 0.6795\n",
      "Epoch 233/500\n",
      "3302/3302 [==============================] - 2s 604us/step - loss: 0.1451 - acc: 0.9730 - val_loss: 2.7011 - val_acc: 0.6731\n",
      "Epoch 234/500\n",
      "3302/3302 [==============================] - 2s 600us/step - loss: 0.1548 - acc: 0.9670 - val_loss: 2.5807 - val_acc: 0.6877\n",
      "Epoch 235/500\n",
      "3302/3302 [==============================] - 2s 598us/step - loss: 0.1313 - acc: 0.9718 - val_loss: 2.5975 - val_acc: 0.6694\n",
      "Epoch 236/500\n",
      "3302/3302 [==============================] - 2s 593us/step - loss: 0.0799 - acc: 0.9846 - val_loss: 2.8975 - val_acc: 0.6822\n",
      "Epoch 237/500\n",
      "3302/3302 [==============================] - 2s 596us/step - loss: 0.0764 - acc: 0.9846 - val_loss: 2.7670 - val_acc: 0.6804\n",
      "Epoch 238/500\n",
      "3302/3302 [==============================] - 2s 600us/step - loss: 0.0653 - acc: 0.9876 - val_loss: 2.4791 - val_acc: 0.7078\n",
      "Epoch 239/500\n",
      "3302/3302 [==============================] - 2s 590us/step - loss: 0.1474 - acc: 0.9688 - val_loss: 2.6901 - val_acc: 0.6877\n",
      "Epoch 240/500\n",
      "3302/3302 [==============================] - 2s 598us/step - loss: 0.0661 - acc: 0.9858 - val_loss: 2.7300 - val_acc: 0.6977\n",
      "Epoch 241/500\n",
      "3302/3302 [==============================] - 2s 613us/step - loss: 0.0867 - acc: 0.9852 - val_loss: 2.5985 - val_acc: 0.7041\n",
      "Epoch 242/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.0294 - acc: 0.9915 - val_loss: 2.5534 - val_acc: 0.7142\n",
      "Epoch 243/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.0363 - acc: 0.9915 - val_loss: 2.6999 - val_acc: 0.6758\n",
      "Epoch 244/500\n",
      "3302/3302 [==============================] - 2s 589us/step - loss: 0.0447 - acc: 0.9903 - val_loss: 2.7008 - val_acc: 0.6895\n",
      "Epoch 245/500\n",
      "3302/3302 [==============================] - 2s 613us/step - loss: 0.0319 - acc: 0.9936 - val_loss: 2.7381 - val_acc: 0.6959\n",
      "Epoch 246/500\n",
      "3302/3302 [==============================] - 2s 599us/step - loss: 0.0774 - acc: 0.9870 - val_loss: 2.7664 - val_acc: 0.6913\n",
      "Epoch 247/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.0413 - acc: 0.9900 - val_loss: 2.9447 - val_acc: 0.6868\n",
      "Epoch 248/500\n",
      "3302/3302 [==============================] - 2s 616us/step - loss: 0.0215 - acc: 0.9945 - val_loss: 2.7965 - val_acc: 0.6895\n",
      "Epoch 249/500\n",
      "3302/3302 [==============================] - 2s 595us/step - loss: 0.0349 - acc: 0.9952 - val_loss: 3.1942 - val_acc: 0.6621\n",
      "Epoch 250/500\n",
      "3302/3302 [==============================] - 2s 610us/step - loss: 0.0250 - acc: 0.9942 - val_loss: 2.9620 - val_acc: 0.6804\n",
      "Epoch 251/500\n",
      "3302/3302 [==============================] - 2s 593us/step - loss: 0.0178 - acc: 0.9961 - val_loss: 3.0705 - val_acc: 0.6721\n",
      "Epoch 252/500\n",
      "3302/3302 [==============================] - 2s 600us/step - loss: 0.0306 - acc: 0.9933 - val_loss: 3.2511 - val_acc: 0.6557\n",
      "Epoch 253/500\n",
      "3302/3302 [==============================] - 2s 589us/step - loss: 0.0468 - acc: 0.9909 - val_loss: 2.8465 - val_acc: 0.7041\n",
      "Epoch 254/500\n",
      "3302/3302 [==============================] - 2s 600us/step - loss: 0.0393 - acc: 0.9918 - val_loss: 2.6638 - val_acc: 0.6968\n",
      "Epoch 255/500\n",
      "3302/3302 [==============================] - 2s 609us/step - loss: 0.0384 - acc: 0.9915 - val_loss: 2.8524 - val_acc: 0.6840\n",
      "Epoch 256/500\n",
      "3302/3302 [==============================] - 2s 609us/step - loss: 0.0354 - acc: 0.9924 - val_loss: 2.9177 - val_acc: 0.6840\n",
      "Epoch 257/500\n",
      "3302/3302 [==============================] - 2s 600us/step - loss: 0.0306 - acc: 0.9912 - val_loss: 3.1594 - val_acc: 0.6721\n",
      "Epoch 258/500\n",
      "3302/3302 [==============================] - 2s 609us/step - loss: 0.0557 - acc: 0.9888 - val_loss: 3.0670 - val_acc: 0.6694\n",
      "Epoch 259/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 0.0436 - acc: 0.9906 - val_loss: 2.9902 - val_acc: 0.6795\n",
      "Epoch 260/500\n",
      "3302/3302 [==============================] - 2s 594us/step - loss: 0.0656 - acc: 0.9882 - val_loss: 2.8095 - val_acc: 0.6904\n",
      "Epoch 261/500\n",
      "3302/3302 [==============================] - 2s 601us/step - loss: 0.0823 - acc: 0.9833 - val_loss: 3.0261 - val_acc: 0.6721\n",
      "Epoch 262/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 0.1048 - acc: 0.9827 - val_loss: 3.0711 - val_acc: 0.6740\n",
      "Epoch 263/500\n",
      "3302/3302 [==============================] - 2s 601us/step - loss: 0.0922 - acc: 0.9849 - val_loss: 2.9816 - val_acc: 0.6813\n",
      "Epoch 264/500\n",
      "3302/3302 [==============================] - 2s 608us/step - loss: 0.0952 - acc: 0.9855 - val_loss: 2.9635 - val_acc: 0.6895\n",
      "Epoch 265/500\n",
      "3302/3302 [==============================] - 2s 624us/step - loss: 0.0679 - acc: 0.9882 - val_loss: 3.0165 - val_acc: 0.6804\n",
      "Epoch 266/500\n",
      "3302/3302 [==============================] - 2s 586us/step - loss: 0.0753 - acc: 0.9846 - val_loss: 3.1274 - val_acc: 0.6858\n",
      "Epoch 267/500\n",
      "3302/3302 [==============================] - 2s 603us/step - loss: 0.3094 - acc: 0.9515 - val_loss: 3.4711 - val_acc: 0.6475\n",
      "Epoch 268/500\n",
      "3302/3302 [==============================] - 2s 606us/step - loss: 0.2300 - acc: 0.9615 - val_loss: 3.1621 - val_acc: 0.6703\n",
      "Epoch 269/500\n",
      "3302/3302 [==============================] - 2s 611us/step - loss: 0.1521 - acc: 0.9761 - val_loss: 3.2929 - val_acc: 0.6721\n",
      "Epoch 270/500\n",
      "3302/3302 [==============================] - 2s 608us/step - loss: 0.0892 - acc: 0.9852 - val_loss: 2.9815 - val_acc: 0.6959\n",
      "Epoch 271/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.0792 - acc: 0.9873 - val_loss: 3.1483 - val_acc: 0.6886\n",
      "Epoch 272/500\n",
      "3302/3302 [==============================] - 2s 592us/step - loss: 0.1035 - acc: 0.9873 - val_loss: 2.9545 - val_acc: 0.6904\n",
      "Epoch 273/500\n",
      "3302/3302 [==============================] - 2s 595us/step - loss: 0.0942 - acc: 0.9852 - val_loss: 2.9544 - val_acc: 0.6886\n",
      "Epoch 274/500\n",
      "3302/3302 [==============================] - 2s 600us/step - loss: 0.0925 - acc: 0.9821 - val_loss: 2.8393 - val_acc: 0.6904\n",
      "Epoch 275/500\n",
      "3302/3302 [==============================] - 2s 612us/step - loss: 0.0381 - acc: 0.9894 - val_loss: 2.9434 - val_acc: 0.6840\n",
      "Epoch 276/500\n",
      "3302/3302 [==============================] - 2s 594us/step - loss: 0.0380 - acc: 0.9936 - val_loss: 2.8878 - val_acc: 0.6986\n",
      "Epoch 277/500\n",
      "3302/3302 [==============================] - 2s 610us/step - loss: 0.0873 - acc: 0.9858 - val_loss: 3.3240 - val_acc: 0.6539\n",
      "Epoch 278/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 0.0768 - acc: 0.9855 - val_loss: 3.0420 - val_acc: 0.6776\n",
      "Epoch 279/500\n",
      "3302/3302 [==============================] - 2s 596us/step - loss: 0.0459 - acc: 0.9909 - val_loss: 2.8885 - val_acc: 0.6858\n",
      "Epoch 280/500\n",
      "3302/3302 [==============================] - 2s 609us/step - loss: 0.3593 - acc: 0.9522 - val_loss: 3.2028 - val_acc: 0.6703\n",
      "Epoch 281/500\n",
      "3302/3302 [==============================] - 2s 594us/step - loss: 0.1717 - acc: 0.9712 - val_loss: 3.0759 - val_acc: 0.6831\n",
      "Epoch 282/500\n",
      "3302/3302 [==============================] - 2s 587us/step - loss: 0.1501 - acc: 0.9782 - val_loss: 3.3329 - val_acc: 0.6658\n",
      "Epoch 283/500\n",
      "3302/3302 [==============================] - 2s 599us/step - loss: 0.1714 - acc: 0.9727 - val_loss: 3.3132 - val_acc: 0.6676\n",
      "Epoch 284/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 0.0855 - acc: 0.9849 - val_loss: 2.9711 - val_acc: 0.6858\n",
      "Epoch 285/500\n",
      "3302/3302 [==============================] - 2s 594us/step - loss: 0.0669 - acc: 0.9879 - val_loss: 3.1369 - val_acc: 0.6858\n",
      "Epoch 286/500\n",
      "3302/3302 [==============================] - 2s 622us/step - loss: 0.0665 - acc: 0.9891 - val_loss: 3.1434 - val_acc: 0.6886\n",
      "Epoch 287/500\n",
      "3302/3302 [==============================] - 2s 587us/step - loss: 0.0732 - acc: 0.9873 - val_loss: 3.4430 - val_acc: 0.6685\n",
      "Epoch 288/500\n",
      "3302/3302 [==============================] - 2s 613us/step - loss: 0.0791 - acc: 0.9867 - val_loss: 3.1859 - val_acc: 0.6950\n",
      "Epoch 289/500\n",
      "3302/3302 [==============================] - 2s 589us/step - loss: 0.0785 - acc: 0.9903 - val_loss: 3.1943 - val_acc: 0.6950\n",
      "Epoch 290/500\n",
      "3302/3302 [==============================] - 2s 596us/step - loss: 0.0558 - acc: 0.9903 - val_loss: 2.8516 - val_acc: 0.7059\n",
      "Epoch 291/500\n",
      "3302/3302 [==============================] - 2s 589us/step - loss: 0.1055 - acc: 0.9818 - val_loss: 2.8661 - val_acc: 0.7105\n",
      "Epoch 292/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.0815 - acc: 0.9852 - val_loss: 2.9039 - val_acc: 0.6913\n",
      "Epoch 293/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 0.0560 - acc: 0.9864 - val_loss: 2.5858 - val_acc: 0.7288\n",
      "Epoch 294/500\n",
      "3302/3302 [==============================] - 2s 600us/step - loss: 0.0349 - acc: 0.9915 - val_loss: 2.7096 - val_acc: 0.7160\n",
      "Epoch 295/500\n",
      "3302/3302 [==============================] - 2s 604us/step - loss: 0.0411 - acc: 0.9912 - val_loss: 2.9368 - val_acc: 0.7050\n",
      "Epoch 296/500\n",
      "3302/3302 [==============================] - 2s 592us/step - loss: 0.0479 - acc: 0.9924 - val_loss: 2.7473 - val_acc: 0.7151\n",
      "Epoch 297/500\n",
      "3302/3302 [==============================] - 2s 609us/step - loss: 0.0285 - acc: 0.9936 - val_loss: 2.7960 - val_acc: 0.7096\n",
      "Epoch 298/500\n",
      "3302/3302 [==============================] - 2s 608us/step - loss: 0.0211 - acc: 0.9949 - val_loss: 2.8122 - val_acc: 0.7114\n",
      "Epoch 299/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.0176 - acc: 0.9961 - val_loss: 2.8490 - val_acc: 0.7032\n",
      "Epoch 300/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 0.0303 - acc: 0.9933 - val_loss: 2.9431 - val_acc: 0.7032\n",
      "Epoch 301/500\n",
      "3302/3302 [==============================] - 2s 601us/step - loss: 0.0583 - acc: 0.9900 - val_loss: 3.1779 - val_acc: 0.6758\n",
      "Epoch 302/500\n",
      "3302/3302 [==============================] - 2s 604us/step - loss: 0.0481 - acc: 0.9903 - val_loss: 2.7302 - val_acc: 0.7114\n",
      "Epoch 303/500\n",
      "3302/3302 [==============================] - 2s 600us/step - loss: 0.0494 - acc: 0.9909 - val_loss: 2.9511 - val_acc: 0.7178\n",
      "Epoch 304/500\n",
      "3302/3302 [==============================] - 2s 611us/step - loss: 0.0305 - acc: 0.9942 - val_loss: 2.8673 - val_acc: 0.7224\n",
      "Epoch 305/500\n",
      "3302/3302 [==============================] - 2s 606us/step - loss: 0.0323 - acc: 0.9936 - val_loss: 2.9791 - val_acc: 0.6922\n",
      "Epoch 306/500\n",
      "3302/3302 [==============================] - 2s 592us/step - loss: 0.0264 - acc: 0.9945 - val_loss: 2.8626 - val_acc: 0.7215\n",
      "Epoch 307/500\n",
      "3302/3302 [==============================] - 2s 609us/step - loss: 0.0162 - acc: 0.9970 - val_loss: 2.9788 - val_acc: 0.7059\n",
      "Epoch 308/500\n",
      "3302/3302 [==============================] - 2s 613us/step - loss: 0.0097 - acc: 0.9970 - val_loss: 2.9806 - val_acc: 0.7059\n",
      "Epoch 309/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 0.0370 - acc: 0.9939 - val_loss: 3.1556 - val_acc: 0.6922\n",
      "Epoch 310/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 0.0517 - acc: 0.9915 - val_loss: 3.2861 - val_acc: 0.6822\n",
      "Epoch 311/500\n",
      "3302/3302 [==============================] - 2s 609us/step - loss: 0.0566 - acc: 0.9903 - val_loss: 3.3826 - val_acc: 0.6767\n",
      "Epoch 312/500\n",
      "3302/3302 [==============================] - 2s 600us/step - loss: 0.0765 - acc: 0.9858 - val_loss: 3.0698 - val_acc: 0.6922\n",
      "Epoch 313/500\n",
      "3302/3302 [==============================] - 2s 589us/step - loss: 0.0834 - acc: 0.9864 - val_loss: 3.1232 - val_acc: 0.7032\n",
      "Epoch 314/500\n",
      "3302/3302 [==============================] - 2s 619us/step - loss: 0.0516 - acc: 0.9894 - val_loss: 3.0253 - val_acc: 0.7041\n",
      "Epoch 315/500\n",
      "3302/3302 [==============================] - 2s 611us/step - loss: 0.0476 - acc: 0.9903 - val_loss: 3.1907 - val_acc: 0.6904\n",
      "Epoch 316/500\n",
      "3302/3302 [==============================] - 2s 595us/step - loss: 0.0669 - acc: 0.9870 - val_loss: 3.2101 - val_acc: 0.6959\n",
      "Epoch 317/500\n",
      "3302/3302 [==============================] - 2s 618us/step - loss: 0.0471 - acc: 0.9915 - val_loss: 3.2985 - val_acc: 0.6822\n",
      "Epoch 318/500\n",
      "3302/3302 [==============================] - 2s 594us/step - loss: 0.0669 - acc: 0.9894 - val_loss: 3.5497 - val_acc: 0.6530\n",
      "Epoch 319/500\n",
      "3302/3302 [==============================] - 2s 591us/step - loss: 0.0675 - acc: 0.9864 - val_loss: 3.2668 - val_acc: 0.6813\n",
      "Epoch 320/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.0797 - acc: 0.9897 - val_loss: 3.2603 - val_acc: 0.6913\n",
      "Epoch 321/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 0.0435 - acc: 0.9897 - val_loss: 3.0908 - val_acc: 0.6977\n",
      "Epoch 322/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 0.0512 - acc: 0.9915 - val_loss: 3.0941 - val_acc: 0.6913\n",
      "Epoch 323/500\n",
      "3302/3302 [==============================] - 2s 596us/step - loss: 0.1029 - acc: 0.9833 - val_loss: 3.2092 - val_acc: 0.6758\n",
      "Epoch 324/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.0941 - acc: 0.9824 - val_loss: 3.3314 - val_acc: 0.6676\n",
      "Epoch 325/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 0.0688 - acc: 0.9873 - val_loss: 3.1485 - val_acc: 0.6840\n",
      "Epoch 326/500\n",
      "3302/3302 [==============================] - 2s 600us/step - loss: 0.0947 - acc: 0.9824 - val_loss: 3.1155 - val_acc: 0.6822\n",
      "Epoch 327/500\n",
      "3302/3302 [==============================] - 2s 590us/step - loss: 0.0957 - acc: 0.9864 - val_loss: 3.1309 - val_acc: 0.6740\n",
      "Epoch 328/500\n",
      "3302/3302 [==============================] - 2s 587us/step - loss: 0.0883 - acc: 0.9821 - val_loss: 2.9931 - val_acc: 0.6977\n",
      "Epoch 329/500\n",
      "3302/3302 [==============================] - 2s 615us/step - loss: 0.1226 - acc: 0.9779 - val_loss: 2.9222 - val_acc: 0.6959\n",
      "Epoch 330/500\n",
      "3302/3302 [==============================] - 2s 593us/step - loss: 0.0566 - acc: 0.9882 - val_loss: 3.1056 - val_acc: 0.6758\n",
      "Epoch 331/500\n",
      "3302/3302 [==============================] - 2s 606us/step - loss: 0.0495 - acc: 0.9927 - val_loss: 3.2080 - val_acc: 0.6767\n",
      "Epoch 332/500\n",
      "3302/3302 [==============================] - 2s 620us/step - loss: 0.3210 - acc: 0.9491 - val_loss: 3.3885 - val_acc: 0.6447\n",
      "Epoch 333/500\n",
      "3302/3302 [==============================] - 2s 594us/step - loss: 0.1258 - acc: 0.9782 - val_loss: 3.1974 - val_acc: 0.6712\n",
      "Epoch 334/500\n",
      "3302/3302 [==============================] - 2s 603us/step - loss: 0.1556 - acc: 0.9773 - val_loss: 3.3514 - val_acc: 0.6639\n",
      "Epoch 335/500\n",
      "3302/3302 [==============================] - 2s 608us/step - loss: 0.1174 - acc: 0.9833 - val_loss: 3.1748 - val_acc: 0.6831\n",
      "Epoch 336/500\n",
      "3302/3302 [==============================] - 2s 606us/step - loss: 0.0655 - acc: 0.9879 - val_loss: 3.1497 - val_acc: 0.6840\n",
      "Epoch 337/500\n",
      "3302/3302 [==============================] - 2s 582us/step - loss: 0.0904 - acc: 0.9876 - val_loss: 3.0561 - val_acc: 0.6941\n",
      "Epoch 338/500\n",
      "3302/3302 [==============================] - 2s 596us/step - loss: 0.0708 - acc: 0.9897 - val_loss: 3.2650 - val_acc: 0.6712\n",
      "Epoch 339/500\n",
      "3302/3302 [==============================] - 2s 597us/step - loss: 0.0669 - acc: 0.9888 - val_loss: 3.0823 - val_acc: 0.6840\n",
      "Epoch 340/500\n",
      "3302/3302 [==============================] - 2s 606us/step - loss: 0.0482 - acc: 0.9921 - val_loss: 3.1746 - val_acc: 0.6886\n",
      "Epoch 341/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.0496 - acc: 0.9912 - val_loss: 3.1918 - val_acc: 0.6868\n",
      "Epoch 342/500\n",
      "3302/3302 [==============================] - 2s 601us/step - loss: 0.0462 - acc: 0.9915 - val_loss: 3.2518 - val_acc: 0.6932\n",
      "Epoch 343/500\n",
      "3302/3302 [==============================] - 2s 604us/step - loss: 0.0505 - acc: 0.9927 - val_loss: 3.2736 - val_acc: 0.6685\n",
      "Epoch 344/500\n",
      "3302/3302 [==============================] - 2s 600us/step - loss: 0.2368 - acc: 0.9679 - val_loss: 3.4443 - val_acc: 0.6584\n",
      "Epoch 345/500\n",
      "3302/3302 [==============================] - 2s 622us/step - loss: 0.1253 - acc: 0.9806 - val_loss: 3.2938 - val_acc: 0.6676\n",
      "Epoch 346/500\n",
      "3302/3302 [==============================] - 2s 596us/step - loss: 0.1138 - acc: 0.9824 - val_loss: 2.9518 - val_acc: 0.6913\n",
      "Epoch 347/500\n",
      "3302/3302 [==============================] - 2s 608us/step - loss: 0.0483 - acc: 0.9891 - val_loss: 3.1243 - val_acc: 0.6922\n",
      "Epoch 348/500\n",
      "3302/3302 [==============================] - 2s 599us/step - loss: 0.0401 - acc: 0.9900 - val_loss: 3.0804 - val_acc: 0.6886\n",
      "Epoch 349/500\n",
      "3302/3302 [==============================] - 2s 597us/step - loss: 0.0573 - acc: 0.9888 - val_loss: 3.1509 - val_acc: 0.6941\n",
      "Epoch 350/500\n",
      "3302/3302 [==============================] - 2s 612us/step - loss: 0.0405 - acc: 0.9921 - val_loss: 3.1873 - val_acc: 0.6804\n",
      "Epoch 351/500\n",
      "3302/3302 [==============================] - 2s 599us/step - loss: 0.0463 - acc: 0.9921 - val_loss: 3.0700 - val_acc: 0.6968\n",
      "Epoch 352/500\n",
      "3302/3302 [==============================] - 2s 599us/step - loss: 0.0650 - acc: 0.9909 - val_loss: 3.3601 - val_acc: 0.6721\n",
      "Epoch 353/500\n",
      "3302/3302 [==============================] - 2s 598us/step - loss: 0.0614 - acc: 0.9903 - val_loss: 3.4448 - val_acc: 0.6594\n",
      "Epoch 354/500\n",
      "3302/3302 [==============================] - 2s 599us/step - loss: 0.0883 - acc: 0.9864 - val_loss: 3.4033 - val_acc: 0.6776\n",
      "Epoch 355/500\n",
      "3302/3302 [==============================] - 2s 593us/step - loss: 0.0270 - acc: 0.9924 - val_loss: 3.3386 - val_acc: 0.6895\n",
      "Epoch 356/500\n",
      "3302/3302 [==============================] - 2s 606us/step - loss: 0.0796 - acc: 0.9849 - val_loss: 3.6012 - val_acc: 0.6612\n",
      "Epoch 357/500\n",
      "3302/3302 [==============================] - 2s 592us/step - loss: 0.0776 - acc: 0.9879 - val_loss: 3.5424 - val_acc: 0.6785\n",
      "Epoch 358/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 0.0651 - acc: 0.9897 - val_loss: 3.5202 - val_acc: 0.6767\n",
      "Epoch 359/500\n",
      "3302/3302 [==============================] - 2s 599us/step - loss: 0.0706 - acc: 0.9906 - val_loss: 3.8085 - val_acc: 0.6648\n",
      "Epoch 360/500\n",
      "3302/3302 [==============================] - 2s 598us/step - loss: 0.0944 - acc: 0.9861 - val_loss: 4.4190 - val_acc: 0.6000\n",
      "Epoch 361/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 0.0616 - acc: 0.9918 - val_loss: 3.5059 - val_acc: 0.6703\n",
      "Epoch 362/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.0349 - acc: 0.9936 - val_loss: 3.3039 - val_acc: 0.6913\n",
      "Epoch 363/500\n",
      "3302/3302 [==============================] - 2s 601us/step - loss: 0.0658 - acc: 0.9876 - val_loss: 3.4477 - val_acc: 0.6822\n",
      "Epoch 364/500\n",
      "3302/3302 [==============================] - 2s 595us/step - loss: 0.0674 - acc: 0.9870 - val_loss: 3.6519 - val_acc: 0.6795\n",
      "Epoch 365/500\n",
      "3302/3302 [==============================] - 2s 589us/step - loss: 0.0808 - acc: 0.9870 - val_loss: 3.6155 - val_acc: 0.6658\n",
      "Epoch 366/500\n",
      "3302/3302 [==============================] - 2s 594us/step - loss: 0.0988 - acc: 0.9858 - val_loss: 3.6787 - val_acc: 0.6767\n",
      "Epoch 367/500\n",
      "3302/3302 [==============================] - 2s 594us/step - loss: 0.0691 - acc: 0.9885 - val_loss: 3.8313 - val_acc: 0.6721\n",
      "Epoch 368/500\n",
      "3302/3302 [==============================] - 2s 601us/step - loss: 0.1098 - acc: 0.9843 - val_loss: 3.6986 - val_acc: 0.6886\n",
      "Epoch 369/500\n",
      "3302/3302 [==============================] - 2s 601us/step - loss: 0.1069 - acc: 0.9830 - val_loss: 3.9737 - val_acc: 0.6594\n",
      "Epoch 370/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 0.1141 - acc: 0.9824 - val_loss: 3.4619 - val_acc: 0.6877\n",
      "Epoch 371/500\n",
      "3302/3302 [==============================] - 2s 627us/step - loss: 0.0969 - acc: 0.9830 - val_loss: 3.6417 - val_acc: 0.6703\n",
      "Epoch 372/500\n",
      "3302/3302 [==============================] - 2s 598us/step - loss: 0.1605 - acc: 0.9794 - val_loss: 3.6734 - val_acc: 0.6868\n",
      "Epoch 373/500\n",
      "3302/3302 [==============================] - 2s 599us/step - loss: 0.0671 - acc: 0.9897 - val_loss: 3.6495 - val_acc: 0.6886\n",
      "Epoch 374/500\n",
      "3302/3302 [==============================] - 2s 610us/step - loss: 0.0603 - acc: 0.9915 - val_loss: 3.7870 - val_acc: 0.6703\n",
      "Epoch 375/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 0.0627 - acc: 0.9912 - val_loss: 3.6471 - val_acc: 0.6703\n",
      "Epoch 376/500\n",
      "3302/3302 [==============================] - 2s 608us/step - loss: 0.0715 - acc: 0.9873 - val_loss: 3.8997 - val_acc: 0.6630\n",
      "Epoch 377/500\n",
      "3302/3302 [==============================] - 2s 604us/step - loss: 0.0610 - acc: 0.9900 - val_loss: 3.7936 - val_acc: 0.6630\n",
      "Epoch 378/500\n",
      "3302/3302 [==============================] - 2s 598us/step - loss: 0.0672 - acc: 0.9888 - val_loss: 3.6915 - val_acc: 0.6667\n",
      "Epoch 379/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 0.0344 - acc: 0.9939 - val_loss: 3.5006 - val_acc: 0.6822\n",
      "Epoch 380/500\n",
      "3302/3302 [==============================] - 2s 598us/step - loss: 0.0371 - acc: 0.9918 - val_loss: 3.2948 - val_acc: 0.7005\n",
      "Epoch 381/500\n",
      "3302/3302 [==============================] - 2s 601us/step - loss: 0.0462 - acc: 0.9918 - val_loss: 3.5158 - val_acc: 0.6804\n",
      "Epoch 382/500\n",
      "3302/3302 [==============================] - 2s 604us/step - loss: 0.1116 - acc: 0.9858 - val_loss: 3.4804 - val_acc: 0.6831\n",
      "Epoch 383/500\n",
      "3302/3302 [==============================] - 2s 608us/step - loss: 0.0466 - acc: 0.9927 - val_loss: 3.6257 - val_acc: 0.6685\n",
      "Epoch 384/500\n",
      "3302/3302 [==============================] - 2s 594us/step - loss: 0.1270 - acc: 0.9806 - val_loss: 3.8438 - val_acc: 0.6731\n",
      "Epoch 385/500\n",
      "3302/3302 [==============================] - 2s 600us/step - loss: 0.2470 - acc: 0.9664 - val_loss: 4.2693 - val_acc: 0.6557\n",
      "Epoch 386/500\n",
      "3302/3302 [==============================] - 2s 611us/step - loss: 0.2252 - acc: 0.9755 - val_loss: 4.0892 - val_acc: 0.6584\n",
      "Epoch 387/500\n",
      "3302/3302 [==============================] - 2s 604us/step - loss: 0.1625 - acc: 0.9785 - val_loss: 3.4176 - val_acc: 0.7096\n",
      "Epoch 388/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 0.1082 - acc: 0.9858 - val_loss: 3.5810 - val_acc: 0.6849\n",
      "Epoch 389/500\n",
      "3302/3302 [==============================] - 2s 613us/step - loss: 0.1958 - acc: 0.9788 - val_loss: 4.2366 - val_acc: 0.6447\n",
      "Epoch 390/500\n",
      "3302/3302 [==============================] - 2s 599us/step - loss: 0.1448 - acc: 0.9839 - val_loss: 4.0812 - val_acc: 0.6703\n",
      "Epoch 391/500\n",
      "3302/3302 [==============================] - 2s 597us/step - loss: 0.1099 - acc: 0.9846 - val_loss: 3.7894 - val_acc: 0.6895\n",
      "Epoch 392/500\n",
      "3302/3302 [==============================] - 2s 599us/step - loss: 0.0579 - acc: 0.9912 - val_loss: 3.8301 - val_acc: 0.6840\n",
      "Epoch 393/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 0.0516 - acc: 0.9927 - val_loss: 3.7125 - val_acc: 0.6913\n",
      "Epoch 394/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.0674 - acc: 0.9912 - val_loss: 3.7752 - val_acc: 0.6877\n",
      "Epoch 395/500\n",
      "3302/3302 [==============================] - 2s 612us/step - loss: 0.0652 - acc: 0.9924 - val_loss: 3.6126 - val_acc: 0.6977\n",
      "Epoch 396/500\n",
      "3302/3302 [==============================] - 2s 594us/step - loss: 0.0398 - acc: 0.9952 - val_loss: 3.4854 - val_acc: 0.6904\n",
      "Epoch 397/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 0.0998 - acc: 0.9888 - val_loss: 3.8783 - val_acc: 0.6740\n",
      "Epoch 398/500\n",
      "3302/3302 [==============================] - 2s 601us/step - loss: 0.0570 - acc: 0.9927 - val_loss: 3.9891 - val_acc: 0.6749\n",
      "Epoch 399/500\n",
      "3302/3302 [==============================] - 2s 600us/step - loss: 0.1193 - acc: 0.9876 - val_loss: 4.2935 - val_acc: 0.6530\n",
      "Epoch 400/500\n",
      "3302/3302 [==============================] - 2s 604us/step - loss: 0.1370 - acc: 0.9846 - val_loss: 3.9746 - val_acc: 0.6676\n",
      "Epoch 401/500\n",
      "3302/3302 [==============================] - 2s 616us/step - loss: 0.0716 - acc: 0.9894 - val_loss: 4.1725 - val_acc: 0.6603\n",
      "Epoch 402/500\n",
      "3302/3302 [==============================] - 2s 598us/step - loss: 0.0467 - acc: 0.9912 - val_loss: 3.6786 - val_acc: 0.6932\n",
      "Epoch 403/500\n",
      "3302/3302 [==============================] - 2s 614us/step - loss: 0.0868 - acc: 0.9876 - val_loss: 3.9411 - val_acc: 0.6877\n",
      "Epoch 404/500\n",
      "3302/3302 [==============================] - 2s 603us/step - loss: 0.1169 - acc: 0.9839 - val_loss: 3.6801 - val_acc: 0.7023\n",
      "Epoch 405/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 0.1256 - acc: 0.9852 - val_loss: 3.8462 - val_acc: 0.6904\n",
      "Epoch 406/500\n",
      "3302/3302 [==============================] - 2s 608us/step - loss: 0.1161 - acc: 0.9843 - val_loss: 3.7095 - val_acc: 0.7014\n",
      "Epoch 407/500\n",
      "3302/3302 [==============================] - 2s 611us/step - loss: 0.1195 - acc: 0.9818 - val_loss: 4.1217 - val_acc: 0.6658\n",
      "Epoch 408/500\n",
      "3302/3302 [==============================] - 2s 604us/step - loss: 0.0872 - acc: 0.9882 - val_loss: 3.7340 - val_acc: 0.7050\n",
      "Epoch 409/500\n",
      "3302/3302 [==============================] - 2s 593us/step - loss: 0.0573 - acc: 0.9921 - val_loss: 3.5807 - val_acc: 0.7023\n",
      "Epoch 410/500\n",
      "3302/3302 [==============================] - 2s 594us/step - loss: 0.0265 - acc: 0.9961 - val_loss: 3.5997 - val_acc: 0.7023\n",
      "Epoch 411/500\n",
      "3302/3302 [==============================] - 2s 606us/step - loss: 0.0331 - acc: 0.9949 - val_loss: 3.8763 - val_acc: 0.6904\n",
      "Epoch 412/500\n",
      "3302/3302 [==============================] - 2s 597us/step - loss: 0.1770 - acc: 0.9806 - val_loss: 3.9206 - val_acc: 0.6822\n",
      "Epoch 413/500\n",
      "3302/3302 [==============================] - 2s 598us/step - loss: 0.0479 - acc: 0.9942 - val_loss: 3.9286 - val_acc: 0.6840\n",
      "Epoch 414/500\n",
      "3302/3302 [==============================] - 2s 604us/step - loss: 0.0464 - acc: 0.9936 - val_loss: 3.8207 - val_acc: 0.6913\n",
      "Epoch 415/500\n",
      "3302/3302 [==============================] - 2s 607us/step - loss: 0.0686 - acc: 0.9897 - val_loss: 3.9281 - val_acc: 0.6831\n",
      "Epoch 416/500\n",
      "3302/3302 [==============================] - 2s 612us/step - loss: 0.0977 - acc: 0.9852 - val_loss: 4.0123 - val_acc: 0.6694\n",
      "Epoch 417/500\n",
      "3302/3302 [==============================] - 2s 616us/step - loss: 0.1043 - acc: 0.9882 - val_loss: 3.8744 - val_acc: 0.6822\n",
      "Epoch 418/500\n",
      "3302/3302 [==============================] - 2s 593us/step - loss: 0.0987 - acc: 0.9891 - val_loss: 3.7618 - val_acc: 0.6968\n",
      "Epoch 419/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.1068 - acc: 0.9873 - val_loss: 3.8066 - val_acc: 0.6886\n",
      "Epoch 420/500\n",
      "3302/3302 [==============================] - 2s 609us/step - loss: 0.2273 - acc: 0.9758 - val_loss: 4.2192 - val_acc: 0.6530\n",
      "Epoch 421/500\n",
      "3302/3302 [==============================] - 2s 595us/step - loss: 0.1360 - acc: 0.9821 - val_loss: 4.0600 - val_acc: 0.6822\n",
      "Epoch 422/500\n",
      "3302/3302 [==============================] - 2s 614us/step - loss: 0.1308 - acc: 0.9830 - val_loss: 3.8724 - val_acc: 0.6886\n",
      "Epoch 423/500\n",
      "3302/3302 [==============================] - 2s 603us/step - loss: 0.1124 - acc: 0.9855 - val_loss: 4.1260 - val_acc: 0.6776\n",
      "Epoch 424/500\n",
      "3302/3302 [==============================] - 2s 597us/step - loss: 0.1200 - acc: 0.9891 - val_loss: 3.9449 - val_acc: 0.6886\n",
      "Epoch 425/500\n",
      "3302/3302 [==============================] - 2s 600us/step - loss: 0.2941 - acc: 0.9712 - val_loss: 4.0346 - val_acc: 0.6886\n",
      "Epoch 426/500\n",
      "3302/3302 [==============================] - 2s 596us/step - loss: 0.1452 - acc: 0.9815 - val_loss: 4.4623 - val_acc: 0.6584\n",
      "Epoch 427/500\n",
      "3302/3302 [==============================] - 2s 587us/step - loss: 0.2343 - acc: 0.9791 - val_loss: 4.5298 - val_acc: 0.6566\n",
      "Epoch 428/500\n",
      "3302/3302 [==============================] - 2s 609us/step - loss: 0.1826 - acc: 0.9794 - val_loss: 4.4047 - val_acc: 0.6758\n",
      "Epoch 429/500\n",
      "3302/3302 [==============================] - 2s 600us/step - loss: 0.1497 - acc: 0.9849 - val_loss: 4.0929 - val_acc: 0.6941\n",
      "Epoch 430/500\n",
      "3302/3302 [==============================] - 2s 599us/step - loss: 0.1067 - acc: 0.9906 - val_loss: 4.0037 - val_acc: 0.6986\n",
      "Epoch 431/500\n",
      "3302/3302 [==============================] - 2s 601us/step - loss: 0.1944 - acc: 0.9827 - val_loss: 4.4901 - val_acc: 0.6575\n",
      "Epoch 432/500\n",
      "3302/3302 [==============================] - 2s 608us/step - loss: 0.2296 - acc: 0.9782 - val_loss: 4.0662 - val_acc: 0.6868\n",
      "Epoch 433/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 0.1566 - acc: 0.9836 - val_loss: 3.6660 - val_acc: 0.7105\n",
      "Epoch 434/500\n",
      "3302/3302 [==============================] - 2s 614us/step - loss: 0.1160 - acc: 0.9894 - val_loss: 3.6885 - val_acc: 0.7132\n",
      "Epoch 435/500\n",
      "3302/3302 [==============================] - 2s 599us/step - loss: 0.1718 - acc: 0.9833 - val_loss: 4.3315 - val_acc: 0.6740\n",
      "Epoch 436/500\n",
      "3302/3302 [==============================] - 2s 600us/step - loss: 0.2741 - acc: 0.9764 - val_loss: 3.7883 - val_acc: 0.7105\n",
      "Epoch 437/500\n",
      "3302/3302 [==============================] - 2s 609us/step - loss: 0.0849 - acc: 0.9891 - val_loss: 4.3166 - val_acc: 0.6685\n",
      "Epoch 438/500\n",
      "3302/3302 [==============================] - 2s 608us/step - loss: 0.1429 - acc: 0.9843 - val_loss: 4.0323 - val_acc: 0.6904\n",
      "Epoch 439/500\n",
      "3302/3302 [==============================] - 2s 609us/step - loss: 0.1475 - acc: 0.9839 - val_loss: 4.5817 - val_acc: 0.6511\n",
      "Epoch 440/500\n",
      "3302/3302 [==============================] - 2s 595us/step - loss: 0.2068 - acc: 0.9809 - val_loss: 4.2773 - val_acc: 0.6849\n",
      "Epoch 441/500\n",
      "3302/3302 [==============================] - 2s 610us/step - loss: 0.1655 - acc: 0.9824 - val_loss: 4.1888 - val_acc: 0.6785\n",
      "Epoch 442/500\n",
      "3302/3302 [==============================] - 2s 604us/step - loss: 0.2475 - acc: 0.9767 - val_loss: 4.5360 - val_acc: 0.6630\n",
      "Epoch 443/500\n",
      "3302/3302 [==============================] - 2s 617us/step - loss: 0.3262 - acc: 0.9700 - val_loss: 4.1367 - val_acc: 0.6831\n",
      "Epoch 444/500\n",
      "3302/3302 [==============================] - 2s 587us/step - loss: 0.1898 - acc: 0.9809 - val_loss: 4.3286 - val_acc: 0.6612\n",
      "Epoch 445/500\n",
      "3302/3302 [==============================] - 2s 603us/step - loss: 0.5962 - acc: 0.9458 - val_loss: 4.3907 - val_acc: 0.6740\n",
      "Epoch 446/500\n",
      "3302/3302 [==============================] - 2s 575us/step - loss: 0.3583 - acc: 0.9688 - val_loss: 4.2172 - val_acc: 0.6895\n",
      "Epoch 447/500\n",
      "3302/3302 [==============================] - 2s 600us/step - loss: 0.2693 - acc: 0.9743 - val_loss: 4.8963 - val_acc: 0.6347\n",
      "Epoch 448/500\n",
      "3302/3302 [==============================] - 2s 594us/step - loss: 0.3434 - acc: 0.9676 - val_loss: 4.8725 - val_acc: 0.6475\n",
      "Epoch 449/500\n",
      "3302/3302 [==============================] - 2s 609us/step - loss: 0.2639 - acc: 0.9752 - val_loss: 4.6010 - val_acc: 0.6639\n",
      "Epoch 450/500\n",
      "3302/3302 [==============================] - 2s 621us/step - loss: 0.2174 - acc: 0.9809 - val_loss: 4.4066 - val_acc: 0.6721\n",
      "Epoch 451/500\n",
      "3302/3302 [==============================] - 2s 600us/step - loss: 0.2332 - acc: 0.9785 - val_loss: 4.4291 - val_acc: 0.6886\n",
      "Epoch 452/500\n",
      "3302/3302 [==============================] - 2s 621us/step - loss: 0.2207 - acc: 0.9800 - val_loss: 4.6405 - val_acc: 0.6731\n",
      "Epoch 453/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.2543 - acc: 0.9785 - val_loss: 4.7061 - val_acc: 0.6621\n",
      "Epoch 454/500\n",
      "3302/3302 [==============================] - 2s 603us/step - loss: 0.2260 - acc: 0.9803 - val_loss: 4.6079 - val_acc: 0.6740\n",
      "Epoch 455/500\n",
      "3302/3302 [==============================] - 2s 609us/step - loss: 0.1716 - acc: 0.9861 - val_loss: 4.2718 - val_acc: 0.6986\n",
      "Epoch 456/500\n",
      "3302/3302 [==============================] - 2s 591us/step - loss: 0.1660 - acc: 0.9846 - val_loss: 4.3687 - val_acc: 0.6904\n",
      "Epoch 457/500\n",
      "3302/3302 [==============================] - 2s 598us/step - loss: 0.1343 - acc: 0.9897 - val_loss: 4.4890 - val_acc: 0.6831\n",
      "Epoch 458/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 0.3488 - acc: 0.9724 - val_loss: 4.9679 - val_acc: 0.6493\n",
      "Epoch 459/500\n",
      "3302/3302 [==============================] - 2s 598us/step - loss: 0.2858 - acc: 0.9764 - val_loss: 4.5641 - val_acc: 0.6813\n",
      "Epoch 460/500\n",
      "3302/3302 [==============================] - 2s 611us/step - loss: 0.4020 - acc: 0.9682 - val_loss: 4.9693 - val_acc: 0.6521\n",
      "Epoch 461/500\n",
      "3302/3302 [==============================] - 2s 622us/step - loss: 0.1721 - acc: 0.9836 - val_loss: 4.2698 - val_acc: 0.6941\n",
      "Epoch 462/500\n",
      "3302/3302 [==============================] - 2s 609us/step - loss: 0.3986 - acc: 0.9667 - val_loss: 5.0802 - val_acc: 0.6475\n",
      "Epoch 463/500\n",
      "3302/3302 [==============================] - 2s 636us/step - loss: 0.4285 - acc: 0.9649 - val_loss: 4.5739 - val_acc: 0.6740\n",
      "Epoch 464/500\n",
      "3302/3302 [==============================] - 2s 604us/step - loss: 0.2429 - acc: 0.9764 - val_loss: 4.3845 - val_acc: 0.6932\n",
      "Epoch 465/500\n",
      "3302/3302 [==============================] - 2s 603us/step - loss: 0.1603 - acc: 0.9855 - val_loss: 4.5412 - val_acc: 0.6795\n",
      "Epoch 466/500\n",
      "3302/3302 [==============================] - 2s 594us/step - loss: 1.2459 - acc: 0.9028 - val_loss: 5.1550 - val_acc: 0.6521\n",
      "Epoch 467/500\n",
      "3302/3302 [==============================] - 2s 591us/step - loss: 0.5203 - acc: 0.9594 - val_loss: 4.8908 - val_acc: 0.6484\n",
      "Epoch 468/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 0.4123 - acc: 0.9673 - val_loss: 5.1749 - val_acc: 0.6457\n",
      "Epoch 469/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 0.4040 - acc: 0.9640 - val_loss: 4.6461 - val_acc: 0.6694\n",
      "Epoch 470/500\n",
      "3302/3302 [==============================] - 2s 586us/step - loss: 0.2439 - acc: 0.9782 - val_loss: 4.6179 - val_acc: 0.6731\n",
      "Epoch 471/500\n",
      "3302/3302 [==============================] - 2s 598us/step - loss: 0.2826 - acc: 0.9764 - val_loss: 4.9149 - val_acc: 0.6658\n",
      "Epoch 472/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 0.2333 - acc: 0.9806 - val_loss: 4.3268 - val_acc: 0.6950\n",
      "Epoch 473/500\n",
      "3302/3302 [==============================] - 2s 604us/step - loss: 0.3483 - acc: 0.9740 - val_loss: 4.9067 - val_acc: 0.6584\n",
      "Epoch 474/500\n",
      "3302/3302 [==============================] - 2s 610us/step - loss: 0.1751 - acc: 0.9858 - val_loss: 4.7116 - val_acc: 0.6676\n",
      "Epoch 475/500\n",
      "3302/3302 [==============================] - 2s 596us/step - loss: 0.1921 - acc: 0.9852 - val_loss: 4.6543 - val_acc: 0.6749\n",
      "Epoch 476/500\n",
      "3302/3302 [==============================] - 2s 583us/step - loss: 0.3326 - acc: 0.9718 - val_loss: 5.1503 - val_acc: 0.6502\n",
      "Epoch 477/500\n",
      "3302/3302 [==============================] - 2s 595us/step - loss: 0.3376 - acc: 0.9727 - val_loss: 4.8406 - val_acc: 0.6639\n",
      "Epoch 478/500\n",
      "3302/3302 [==============================] - 2s 606us/step - loss: 0.3647 - acc: 0.9724 - val_loss: 4.5752 - val_acc: 0.6785\n",
      "Epoch 479/500\n",
      "3302/3302 [==============================] - 2s 591us/step - loss: 0.2744 - acc: 0.9785 - val_loss: 4.6676 - val_acc: 0.6776\n",
      "Epoch 480/500\n",
      "3302/3302 [==============================] - 2s 591us/step - loss: 0.2786 - acc: 0.9779 - val_loss: 4.3811 - val_acc: 0.6968\n",
      "Epoch 481/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.1801 - acc: 0.9852 - val_loss: 4.2240 - val_acc: 0.7132\n",
      "Epoch 482/500\n",
      "3302/3302 [==============================] - 2s 586us/step - loss: 0.1519 - acc: 0.9867 - val_loss: 4.0456 - val_acc: 0.7233\n",
      "Epoch 483/500\n",
      "3302/3302 [==============================] - 2s 591us/step - loss: 0.1230 - acc: 0.9906 - val_loss: 4.2285 - val_acc: 0.7023\n",
      "Epoch 484/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 0.1562 - acc: 0.9864 - val_loss: 4.4416 - val_acc: 0.6950\n",
      "Epoch 485/500\n",
      "3302/3302 [==============================] - 2s 597us/step - loss: 0.1562 - acc: 0.9861 - val_loss: 4.3968 - val_acc: 0.6877\n",
      "Epoch 486/500\n",
      "3302/3302 [==============================] - 2s 600us/step - loss: 0.1482 - acc: 0.9867 - val_loss: 4.2785 - val_acc: 0.7078\n",
      "Epoch 487/500\n",
      "3302/3302 [==============================] - 2s 613us/step - loss: 0.1744 - acc: 0.9858 - val_loss: 4.5148 - val_acc: 0.7005\n",
      "Epoch 488/500\n",
      "3302/3302 [==============================] - 2s 606us/step - loss: 0.2438 - acc: 0.9815 - val_loss: 4.5115 - val_acc: 0.6977\n",
      "Epoch 489/500\n",
      "3302/3302 [==============================] - 2s 608us/step - loss: 0.2510 - acc: 0.9797 - val_loss: 5.0391 - val_acc: 0.6530\n",
      "Epoch 490/500\n",
      "3302/3302 [==============================] - 2s 603us/step - loss: 0.2501 - acc: 0.9806 - val_loss: 4.2465 - val_acc: 0.7114\n",
      "Epoch 491/500\n",
      "3302/3302 [==============================] - 2s 602us/step - loss: 0.1794 - acc: 0.9861 - val_loss: 4.1951 - val_acc: 0.7096\n",
      "Epoch 492/500\n",
      "3302/3302 [==============================] - 2s 605us/step - loss: 0.1831 - acc: 0.9861 - val_loss: 4.5005 - val_acc: 0.6922\n",
      "Epoch 493/500\n",
      "3302/3302 [==============================] - 2s 596us/step - loss: 0.1986 - acc: 0.9846 - val_loss: 4.8561 - val_acc: 0.6758\n",
      "Epoch 494/500\n",
      "3302/3302 [==============================] - 2s 608us/step - loss: 0.2561 - acc: 0.9806 - val_loss: 4.7149 - val_acc: 0.6785\n",
      "Epoch 495/500\n",
      "3302/3302 [==============================] - 2s 593us/step - loss: 0.2969 - acc: 0.9770 - val_loss: 4.7624 - val_acc: 0.6813\n",
      "Epoch 496/500\n",
      "3302/3302 [==============================] - 2s 595us/step - loss: 0.3161 - acc: 0.9779 - val_loss: 4.7514 - val_acc: 0.6840\n",
      "Epoch 497/500\n",
      "3302/3302 [==============================] - 2s 593us/step - loss: 0.2623 - acc: 0.9812 - val_loss: 4.6943 - val_acc: 0.6868\n",
      "Epoch 498/500\n",
      "3302/3302 [==============================] - 2s 608us/step - loss: 0.2451 - acc: 0.9803 - val_loss: 4.6787 - val_acc: 0.6922\n",
      "Epoch 499/500\n",
      "3302/3302 [==============================] - 2s 609us/step - loss: 0.2124 - acc: 0.9812 - val_loss: 4.2354 - val_acc: 0.7205\n",
      "Epoch 500/500\n",
      "3302/3302 [==============================] - 2s 590us/step - loss: 0.2983 - acc: 0.9770 - val_loss: 4.7589 - val_acc: 0.6877\n"
     ]
    }
   ],
   "source": [
    "tf.get_default_graph()\n",
    "########## HYPER PARAMETERS\n",
    "batch_size = 50\n",
    "epochs = 500\n",
    "optimizer = optimizers.Adam()\n",
    "#optimizer = tf.keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.1)\n",
    "#init=tf.global_variables_initializer()\n",
    "\n",
    "########## HYPER PARAMETERS\n",
    "########## MODEL ARCHITECTURE\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Conv2D(84, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(27,15,1)))\n",
    "model.add(Conv2D(84, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='valid', data_format='channels_last'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Conv2D(42, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(42, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='valid', data_format='channels_last'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(400, activation=('relu'), use_bias=True))\n",
    "model.add(Dense(100, activation=('relu'), use_bias=True))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "########## MODEL ARCHITECTURE\n",
    "####TENSORBOARD\n",
    "config = \"\"\n",
    "for layer in model.layers:\n",
    "    config += str(layer.output).split('\\\"')[1].split(\"/\")[0] + str(layer.output_shape) + \"\\n\\n\"\n",
    "#### END TENSORBOARD\n",
    "\n",
    "\n",
    "# Print summary\n",
    "model.summary()\n",
    "readable_timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%d_%H%M%S')\n",
    "tensorflowfolder = \"/srv/share/tensorboardfiles/KnuckleFinger_Robin_\" + readable_timestamp\n",
    "\n",
    "logger = LoggingTensorBoard(settings_str_to_log = config, log_dir=tensorflowfolder, histogram_freq=0, write_graph=True, write_images=True, update_freq = 'epoch')\n",
    "storer = ModelCheckpoint(\"./ModelSnapshots/KnuckleFingerGestures_Robin_\" + readable_timestamp + \".h5\", monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "# compile model for training\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train_one_hot,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test_one_hot),\n",
    "                    callbacks=[logger, storer])\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Confusion matrix: \n",
      "[[28  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  1  1  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 20  0  0  0  0  0  0  0  0  0  0  0  1  0  0  1  1  1  0  0  1  0  0  0  0  0  0  0  0  0  0  0  7]\n",
      " [ 1  1 22  5  0  0  0  0  0  1  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9 16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 19  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 11  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  4 22  0  0  0  0  1  0  0  0  1  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  1  0  7  4  1  0  0  0  1  3  0  0  0  0  0  0  0  1  0 10  2  0  1  0  0  2  0  1  0  0]\n",
      " [ 0  2  0  0  3  3  4 15  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  1  2  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  0 22  2  0  1  0  0  1  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1 23  0  2  1  0  0  0  0  0  0  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  0  0  0  0 24  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  3  0  0  0  0 21  0  0  1  0  0  0  0  0  0  0  0  0  0  1  1  0  4  1  0  0  1  0]\n",
      " [ 0  0  0  0  1  0  1  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  6  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  1  1  0  0 24  1  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  2  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  0  2 28  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  2  1  0  0  0  1  0 23  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0]\n",
      " [ 3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 19]\n",
      " [ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 18  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0 11]\n",
      " [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3 24  0  0  0  0  1  0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  3  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10 17  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  3  3  3  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0 18  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 27  0  0  0  0  0  0  0  2  0  0  0  0]\n",
      " [ 0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 17  0  0  0  0  0  0  2  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  4  0 19  5  0  0  0  0  7  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1 12 15  0  0  1  0  1  0  1  2  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 33  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1 28  0  0  1  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  1  0  0  0  1  0  1  0  0 29  2  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  1  0  1  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  2  0 25  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0 28  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  1  0  0  1  0  2  0  0  1 31  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  6  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0 29  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0 29  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29]]\n"
     ]
    }
   ],
   "source": [
    "# use model for inference to get test accuracy\n",
    "y_test_pred = model.predict(x_test)\n",
    "y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "\n",
    "#print ('\\n Summary of the precision, recall, F1 score for each class:')\n",
    "#print (sklearn.metrics.classification_report(y_test, y_test_pred))\n",
    "\n",
    "print ('\\n Confusion matrix: ')\n",
    "np.set_printoptions(threshold=np.nan, linewidth=160)\n",
    "print (sklearn.metrics.confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.plot(history.history['acc'], label=\"Training Accuracy\")\n",
    "#plt.plot(history.history['val_acc'], label=\"Test Accuracy\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(\"./gestures_training_2.png\",bbox_inches='tight',transparent=True, pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save(\"Gestures.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
