{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import *\n",
    "from keras import optimizers\n",
    "from keras import utils\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Importing matplotlib to plot images.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Importing SK-learn to calculate precision and recall\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, LeaveOneGroupOut\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "# Used for graph export\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.framework import graph_io\n",
    "from keras import backend as K\n",
    "\n",
    "import pickle as pkl\n",
    "import h5py\n",
    "\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import telegram\n",
    "from keras.callbacks import Callback\n",
    "from callbacks import TelegramCallback\n",
    "from callbacks.TelegramData import TelegramData\n",
    "\n",
    "\n",
    "# create callback\n",
    "config = {\n",
    "    'token': TelegramData.Token,   # paste your bot token\n",
    "    'telegram_id': TelegramData.ID,   \n",
    "    'model_name': \"JAN_CNN_01\" # paste your telegram_id\n",
    "}\n",
    "\n",
    "tg_callback = TelegramCallback(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggingTensorBoard(TensorBoard):    \n",
    "\n",
    "    def __init__(self, log_dir, settings_str_to_log, **kwargs):\n",
    "        super(LoggingTensorBoard, self).__init__(log_dir, **kwargs)\n",
    "\n",
    "        self.settings_str = settings_str_to_log\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        TensorBoard.on_train_begin(self, logs=logs)\n",
    "\n",
    "        tensor =  tf.convert_to_tensor(self.settings_str)\n",
    "        summary = tf.summary.text (\"Run_Settings\", tensor)\n",
    "\n",
    "        with  tf.Session() as sess:\n",
    "            s = sess.run(summary)\n",
    "            self.writer.add_summary(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  9  6  4 14 17 16 12  3 10 18  5] [13  8 11 15  7]\n"
     ]
    }
   ],
   "source": [
    "# the data, split between train and test sets\n",
    "dfAll = pd.read_pickle(\"PklData/df_blobs.pkl\")\n",
    "\n",
    "lst = dfAll.userID.unique()\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(lst)\n",
    "test_ids = lst[-5:]\n",
    "train_ids = lst[:-5]\n",
    "print(train_ids, test_ids)\n",
    "\n",
    "df_train = dfAll[dfAll.userID.isin(train_ids)]\n",
    "df_test = dfAll[dfAll.userID.isin(test_ids) & (dfAll.Version == \"Normal\")]\n",
    "\n",
    "df_train2 = df_train[['Blobs', 'InputMethod']].copy()\n",
    "df_test2 = df_test[['Blobs', 'InputMethod']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.vstack(df_train2.Blobs)\n",
    "x_test = np.vstack(df_test2.Blobs)\n",
    "y_train = df_train2.InputMethod.values\n",
    "y_test = df_test2.InputMethod.values\n",
    "\n",
    "x_train = x_train.reshape(-1, 27, 15, 1)\n",
    "x_test = x_test.reshape(-1, 27, 15, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices (one-hot notation)\n",
    "num_classes = 2\n",
    "y_train_one_hot = utils.to_categorical(df_train2.InputMethod, num_classes)\n",
    "y_test_one_hot = utils.to_categorical(df_test2.InputMethod, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Label for image 1 is: 0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 1\n",
    "plt.imshow(x_train[i].reshape(27, 15)) #np.sqrt(784) = 28\n",
    "plt.title(\"Label for image %i is: %s\" % (i, y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If GPU is not available: \n",
    "# GPU_USE = '/cpu:0'\n",
    "#config = tf.ConfigProto(device_count = {\"GPU\": 1})\n",
    "\n",
    "\n",
    "# If GPU is available: \n",
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = True\n",
    "config.allow_soft_placement = True\n",
    "config.gpu_options.allow_growth=True\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "\n",
    "# Limit the maximum memory used\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "\n",
    "# set session config\n",
    "tf.keras.backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 27, 15, 128)       1280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 27, 15, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 27, 15, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 27, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 14, 8, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 14, 8, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 8, 64)         36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 14, 8, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 14, 8, 32)         18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 14, 8, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 7, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 7, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 140)               125580    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 140)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 70)                9870      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 70)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 142       \n",
      "=================================================================\n",
      "Total params: 267,208\n",
      "Trainable params: 266,632\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n",
      "KnuckleFinger_Jan_v320190301_105902\n",
      "Train on 285412 samples, validate on 22341 samples\n",
      "Epoch 1/10000\n",
      "285412/285412 [==============================] - 71s 249us/step - loss: 2.4521 - acc: 0.8769 - val_loss: 0.8610 - val_acc: 0.7972\n",
      "Epoch 2/10000\n",
      "285412/285412 [==============================] - 68s 239us/step - loss: 0.6770 - acc: 0.9025 - val_loss: 0.6289 - val_acc: 0.9094\n",
      "Epoch 3/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.6354 - acc: 0.9100 - val_loss: 0.5848 - val_acc: 0.9161\n",
      "Epoch 4/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.6122 - acc: 0.9143 - val_loss: 0.6919 - val_acc: 0.8510\n",
      "Epoch 5/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.6020 - acc: 0.9149 - val_loss: 0.5302 - val_acc: 0.9342\n",
      "Epoch 6/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5959 - acc: 0.9158 - val_loss: 0.5502 - val_acc: 0.9209\n",
      "Epoch 7/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5891 - acc: 0.9162 - val_loss: 0.5718 - val_acc: 0.9138\n",
      "Epoch 8/10000\n",
      "285412/285412 [==============================] - 71s 250us/step - loss: 0.5846 - acc: 0.9177 - val_loss: 0.5805 - val_acc: 0.9109\n",
      "Epoch 9/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5841 - acc: 0.9172 - val_loss: 0.6487 - val_acc: 0.8926\n",
      "Epoch 10/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5830 - acc: 0.9177 - val_loss: 0.5507 - val_acc: 0.9231\n",
      "Epoch 11/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5805 - acc: 0.9174 - val_loss: 0.5290 - val_acc: 0.9234\n",
      "Epoch 12/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5801 - acc: 0.9174 - val_loss: 0.5329 - val_acc: 0.9249\n",
      "Epoch 13/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5786 - acc: 0.9191 - val_loss: 0.5744 - val_acc: 0.8953\n",
      "Epoch 14/10000\n",
      "285412/285412 [==============================] - 71s 250us/step - loss: 0.5736 - acc: 0.9188 - val_loss: 0.5099 - val_acc: 0.9368\n",
      "Epoch 15/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5740 - acc: 0.9184 - val_loss: 0.5270 - val_acc: 0.9253\n",
      "Epoch 16/10000\n",
      "285412/285412 [==============================] - 71s 251us/step - loss: 0.5696 - acc: 0.9194 - val_loss: 1.0345 - val_acc: 0.6445\n",
      "Epoch 17/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5772 - acc: 0.9164 - val_loss: 0.5127 - val_acc: 0.9353\n",
      "Epoch 18/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5676 - acc: 0.9182 - val_loss: 0.5663 - val_acc: 0.9207\n",
      "Epoch 19/10000\n",
      "285412/285412 [==============================] - 71s 250us/step - loss: 0.5721 - acc: 0.9170 - val_loss: 0.5426 - val_acc: 0.9292\n",
      "Epoch 20/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5726 - acc: 0.9159 - val_loss: 0.5338 - val_acc: 0.9306\n",
      "Epoch 21/10000\n",
      "285412/285412 [==============================] - 71s 250us/step - loss: 0.5729 - acc: 0.9168 - val_loss: 0.5316 - val_acc: 0.9332\n",
      "Epoch 22/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5793 - acc: 0.9139 - val_loss: 0.6512 - val_acc: 0.8280\n",
      "Epoch 23/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5768 - acc: 0.9152 - val_loss: 0.5353 - val_acc: 0.9318\n",
      "Epoch 24/10000\n",
      "285412/285412 [==============================] - 71s 249us/step - loss: 0.5733 - acc: 0.9148 - val_loss: 0.4962 - val_acc: 0.9346\n",
      "Epoch 25/10000\n",
      "285412/285412 [==============================] - 72s 253us/step - loss: 0.5683 - acc: 0.9149 - val_loss: 0.5358 - val_acc: 0.9301\n",
      "Epoch 26/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5728 - acc: 0.9143 - val_loss: 0.6645 - val_acc: 0.8970\n",
      "Epoch 27/10000\n",
      "285412/285412 [==============================] - 71s 250us/step - loss: 0.5728 - acc: 0.9149 - val_loss: 0.5612 - val_acc: 0.8954\n",
      "Epoch 28/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5728 - acc: 0.9142 - val_loss: 0.5260 - val_acc: 0.9133\n",
      "Epoch 29/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5682 - acc: 0.9151 - val_loss: 0.5267 - val_acc: 0.9284\n",
      "Epoch 30/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5660 - acc: 0.9158 - val_loss: 0.5157 - val_acc: 0.9212\n",
      "Epoch 31/10000\n",
      "285412/285412 [==============================] - 71s 250us/step - loss: 0.5741 - acc: 0.9138 - val_loss: 0.5383 - val_acc: 0.9001\n",
      "Epoch 32/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5644 - acc: 0.9149 - val_loss: 0.6061 - val_acc: 0.8502\n",
      "Epoch 33/10000\n",
      "285412/285412 [==============================] - 72s 253us/step - loss: 0.5668 - acc: 0.9146 - val_loss: 0.6122 - val_acc: 0.8655\n",
      "Epoch 34/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5665 - acc: 0.9152 - val_loss: 0.5548 - val_acc: 0.9013\n",
      "Epoch 35/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5654 - acc: 0.9141 - val_loss: 0.5716 - val_acc: 0.8933\n",
      "Epoch 36/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5640 - acc: 0.9144 - val_loss: 0.5667 - val_acc: 0.8862\n",
      "Epoch 37/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5669 - acc: 0.9136 - val_loss: 0.5191 - val_acc: 0.9173\n",
      "Epoch 38/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5668 - acc: 0.9150 - val_loss: 0.6727 - val_acc: 0.8852\n",
      "Epoch 39/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5582 - acc: 0.9151 - val_loss: 0.5281 - val_acc: 0.9109\n",
      "Epoch 40/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5611 - acc: 0.9139 - val_loss: 0.5063 - val_acc: 0.9337\n",
      "Epoch 41/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5642 - acc: 0.9148 - val_loss: 0.5598 - val_acc: 0.9224\n",
      "Epoch 42/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5616 - acc: 0.9152 - val_loss: 0.5077 - val_acc: 0.9312\n",
      "Epoch 43/10000\n",
      "285412/285412 [==============================] - 72s 253us/step - loss: 0.5612 - acc: 0.9160 - val_loss: 0.5384 - val_acc: 0.8966\n",
      "Epoch 44/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5597 - acc: 0.9162 - val_loss: 0.5114 - val_acc: 0.9292\n",
      "Epoch 45/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5584 - acc: 0.9158 - val_loss: 0.5810 - val_acc: 0.9237\n",
      "Epoch 46/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5559 - acc: 0.9170 - val_loss: 0.5309 - val_acc: 0.9077\n",
      "Epoch 47/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5603 - acc: 0.9138 - val_loss: 0.5192 - val_acc: 0.9267\n",
      "Epoch 48/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5585 - acc: 0.9161 - val_loss: 0.4991 - val_acc: 0.9282\n",
      "Epoch 49/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5569 - acc: 0.9159 - val_loss: 0.5265 - val_acc: 0.9134\n",
      "Epoch 50/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5555 - acc: 0.9154 - val_loss: 0.4977 - val_acc: 0.9314\n",
      "Epoch 51/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5616 - acc: 0.9156 - val_loss: 0.5062 - val_acc: 0.9308\n",
      "Epoch 52/10000\n",
      "285412/285412 [==============================] - 72s 253us/step - loss: 0.5584 - acc: 0.9153 - val_loss: 0.5027 - val_acc: 0.9200\n",
      "Epoch 53/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5593 - acc: 0.9142 - val_loss: 0.5569 - val_acc: 0.9163\n",
      "Epoch 54/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5611 - acc: 0.9149 - val_loss: 0.4928 - val_acc: 0.9373\n",
      "Epoch 55/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5583 - acc: 0.9152 - val_loss: 0.5276 - val_acc: 0.9048\n",
      "Epoch 56/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5591 - acc: 0.9136 - val_loss: 0.5672 - val_acc: 0.8790\n",
      "Epoch 57/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5572 - acc: 0.9134 - val_loss: 0.4947 - val_acc: 0.9312\n",
      "Epoch 58/10000\n",
      "285412/285412 [==============================] - 72s 253us/step - loss: 0.5585 - acc: 0.9136 - val_loss: 0.5425 - val_acc: 0.9193\n",
      "Epoch 59/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5603 - acc: 0.9132 - val_loss: 0.5526 - val_acc: 0.9352\n",
      "Epoch 60/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5605 - acc: 0.9125 - val_loss: 0.5029 - val_acc: 0.9307\n",
      "Epoch 61/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5583 - acc: 0.9126 - val_loss: 0.5409 - val_acc: 0.9188\n",
      "Epoch 62/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5573 - acc: 0.9139 - val_loss: 0.4827 - val_acc: 0.9303\n",
      "Epoch 63/10000\n",
      "285412/285412 [==============================] - 71s 250us/step - loss: 0.5596 - acc: 0.9137 - val_loss: 0.5013 - val_acc: 0.9329\n",
      "Epoch 64/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5541 - acc: 0.9149 - val_loss: 0.5242 - val_acc: 0.9233\n",
      "Epoch 65/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5559 - acc: 0.9150 - val_loss: 0.6046 - val_acc: 0.8684\n",
      "Epoch 66/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5595 - acc: 0.9127 - val_loss: 0.5099 - val_acc: 0.9262\n",
      "Epoch 67/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5611 - acc: 0.9121 - val_loss: 0.5498 - val_acc: 0.9231\n",
      "Epoch 68/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5581 - acc: 0.9118 - val_loss: 0.4877 - val_acc: 0.9376\n",
      "Epoch 69/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5551 - acc: 0.9112 - val_loss: 0.5535 - val_acc: 0.9151\n",
      "Epoch 70/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5549 - acc: 0.9119 - val_loss: 0.5613 - val_acc: 0.9125\n",
      "Epoch 71/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5518 - acc: 0.9126 - val_loss: 0.6144 - val_acc: 0.8543\n",
      "Epoch 72/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5556 - acc: 0.9114 - val_loss: 0.5486 - val_acc: 0.9182\n",
      "Epoch 73/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5549 - acc: 0.9122 - val_loss: 0.6072 - val_acc: 0.8806\n",
      "Epoch 74/10000\n",
      "285412/285412 [==============================] - 71s 250us/step - loss: 0.5526 - acc: 0.9116 - val_loss: 0.6329 - val_acc: 0.8760\n",
      "Epoch 75/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5558 - acc: 0.9114 - val_loss: 0.4967 - val_acc: 0.9255\n",
      "Epoch 76/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5559 - acc: 0.9119 - val_loss: 0.5216 - val_acc: 0.9213\n",
      "Epoch 77/10000\n",
      "285412/285412 [==============================] - 72s 253us/step - loss: 0.5553 - acc: 0.9117 - val_loss: 0.5148 - val_acc: 0.9210\n",
      "Epoch 78/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5568 - acc: 0.9129 - val_loss: 0.4983 - val_acc: 0.9323\n",
      "Epoch 79/10000\n",
      "285412/285412 [==============================] - 71s 250us/step - loss: 0.5569 - acc: 0.9116 - val_loss: 0.5223 - val_acc: 0.9248\n",
      "Epoch 80/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5561 - acc: 0.9121 - val_loss: 0.4877 - val_acc: 0.9322\n",
      "Epoch 81/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5544 - acc: 0.9127 - val_loss: 0.4892 - val_acc: 0.9346\n",
      "Epoch 82/10000\n",
      "285412/285412 [==============================] - 71s 250us/step - loss: 0.5535 - acc: 0.9121 - val_loss: 0.5191 - val_acc: 0.9276\n",
      "Epoch 83/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5551 - acc: 0.9132 - val_loss: 0.5116 - val_acc: 0.9278\n",
      "Epoch 84/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5521 - acc: 0.9130 - val_loss: 0.5748 - val_acc: 0.9054\n",
      "Epoch 85/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5532 - acc: 0.9137 - val_loss: 0.4907 - val_acc: 0.9321\n",
      "Epoch 86/10000\n",
      "285412/285412 [==============================] - 71s 250us/step - loss: 0.5532 - acc: 0.9132 - val_loss: 0.4954 - val_acc: 0.9332\n",
      "Epoch 87/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5557 - acc: 0.9128 - val_loss: 0.4881 - val_acc: 0.9294\n",
      "Epoch 88/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5535 - acc: 0.9137 - val_loss: 0.4919 - val_acc: 0.9240\n",
      "Epoch 89/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5540 - acc: 0.9133 - val_loss: 0.5291 - val_acc: 0.9301\n",
      "Epoch 90/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5561 - acc: 0.9120 - val_loss: 0.4884 - val_acc: 0.9267\n",
      "Epoch 91/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5549 - acc: 0.9128 - val_loss: 0.5598 - val_acc: 0.8838\n",
      "Epoch 92/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5552 - acc: 0.9114 - val_loss: 0.4973 - val_acc: 0.9304\n",
      "Epoch 93/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5535 - acc: 0.9122 - val_loss: 0.4836 - val_acc: 0.9369\n",
      "Epoch 94/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5535 - acc: 0.9131 - val_loss: 0.5269 - val_acc: 0.8951\n",
      "Epoch 95/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5574 - acc: 0.9118 - val_loss: 0.5260 - val_acc: 0.9086\n",
      "Epoch 96/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5558 - acc: 0.9124 - val_loss: 0.4913 - val_acc: 0.9260\n",
      "Epoch 97/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5548 - acc: 0.9124 - val_loss: 0.5559 - val_acc: 0.9121\n",
      "Epoch 98/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5530 - acc: 0.9138 - val_loss: 0.4717 - val_acc: 0.9377\n",
      "Epoch 99/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5526 - acc: 0.9126 - val_loss: 0.4989 - val_acc: 0.9323\n",
      "Epoch 100/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5535 - acc: 0.9135 - val_loss: 0.4951 - val_acc: 0.9397\n",
      "Epoch 101/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5515 - acc: 0.9122 - val_loss: 0.4876 - val_acc: 0.9308\n",
      "Epoch 102/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5530 - acc: 0.9147 - val_loss: 0.5033 - val_acc: 0.9319\n",
      "Epoch 103/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5541 - acc: 0.9126 - val_loss: 0.5083 - val_acc: 0.9308\n",
      "Epoch 104/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5550 - acc: 0.9128 - val_loss: 0.5220 - val_acc: 0.9353\n",
      "Epoch 105/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5529 - acc: 0.9144 - val_loss: 0.5189 - val_acc: 0.9308\n",
      "Epoch 106/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5567 - acc: 0.9128 - val_loss: 0.5523 - val_acc: 0.8979\n",
      "Epoch 107/10000\n",
      "285412/285412 [==============================] - 71s 250us/step - loss: 0.5529 - acc: 0.9141 - val_loss: 0.4739 - val_acc: 0.9345\n",
      "Epoch 108/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5533 - acc: 0.9131 - val_loss: 0.5423 - val_acc: 0.8890\n",
      "Epoch 109/10000\n",
      "285412/285412 [==============================] - 71s 249us/step - loss: 0.5577 - acc: 0.9127 - val_loss: 0.5280 - val_acc: 0.9219\n",
      "Epoch 110/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5526 - acc: 0.9131 - val_loss: 0.5765 - val_acc: 0.9005\n",
      "Epoch 111/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5493 - acc: 0.9143 - val_loss: 0.5245 - val_acc: 0.9167\n",
      "Epoch 112/10000\n",
      "285412/285412 [==============================] - 71s 250us/step - loss: 0.5556 - acc: 0.9136 - val_loss: 0.4948 - val_acc: 0.9279\n",
      "Epoch 113/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5568 - acc: 0.9137 - val_loss: 0.5116 - val_acc: 0.9252\n",
      "Epoch 114/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5514 - acc: 0.9142 - val_loss: 0.5355 - val_acc: 0.9178\n",
      "Epoch 115/10000\n",
      "285412/285412 [==============================] - 71s 250us/step - loss: 0.5541 - acc: 0.9144 - val_loss: 0.4983 - val_acc: 0.9201\n",
      "Epoch 116/10000\n",
      "285412/285412 [==============================] - 71s 250us/step - loss: 0.5537 - acc: 0.9138 - val_loss: 0.5540 - val_acc: 0.9096\n",
      "Epoch 117/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5548 - acc: 0.9127 - val_loss: 0.5261 - val_acc: 0.9109\n",
      "Epoch 118/10000\n",
      "285412/285412 [==============================] - 71s 250us/step - loss: 0.5553 - acc: 0.9136 - val_loss: 0.5061 - val_acc: 0.9281\n",
      "Epoch 119/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5596 - acc: 0.9135 - val_loss: 0.5209 - val_acc: 0.9091\n",
      "Epoch 120/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5524 - acc: 0.9136 - val_loss: 0.5098 - val_acc: 0.9360\n",
      "Epoch 121/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5552 - acc: 0.9132 - val_loss: 0.4899 - val_acc: 0.9284\n",
      "Epoch 122/10000\n",
      "285412/285412 [==============================] - 72s 253us/step - loss: 0.5539 - acc: 0.9137 - val_loss: 0.7661 - val_acc: 0.7638\n",
      "Epoch 123/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5546 - acc: 0.9139 - val_loss: 0.5583 - val_acc: 0.9097\n",
      "Epoch 124/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5530 - acc: 0.9145 - val_loss: 0.4801 - val_acc: 0.9373\n",
      "Epoch 125/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5521 - acc: 0.9137 - val_loss: 0.5096 - val_acc: 0.9266\n",
      "Epoch 126/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5529 - acc: 0.9130 - val_loss: 0.4985 - val_acc: 0.9296\n",
      "Epoch 127/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5512 - acc: 0.9137 - val_loss: 0.5023 - val_acc: 0.9281\n",
      "Epoch 128/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5570 - acc: 0.9126 - val_loss: 0.6039 - val_acc: 0.8894\n",
      "Epoch 129/10000\n",
      "285412/285412 [==============================] - 71s 250us/step - loss: 0.5534 - acc: 0.9137 - val_loss: 0.5124 - val_acc: 0.9264\n",
      "Epoch 130/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5545 - acc: 0.9129 - val_loss: 0.5099 - val_acc: 0.9323\n",
      "Epoch 131/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5548 - acc: 0.9146 - val_loss: 0.5225 - val_acc: 0.9287\n",
      "Epoch 132/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5514 - acc: 0.9140 - val_loss: 0.4960 - val_acc: 0.9366\n",
      "Epoch 133/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5528 - acc: 0.9137 - val_loss: 0.4866 - val_acc: 0.9362\n",
      "Epoch 134/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5527 - acc: 0.9124 - val_loss: 0.4850 - val_acc: 0.9291\n",
      "Epoch 135/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5515 - acc: 0.9128 - val_loss: 0.5178 - val_acc: 0.9270\n",
      "Epoch 136/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5573 - acc: 0.9127 - val_loss: 0.5162 - val_acc: 0.9214\n",
      "Epoch 137/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5553 - acc: 0.9125 - val_loss: 0.4895 - val_acc: 0.9350\n",
      "Epoch 138/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5598 - acc: 0.9119 - val_loss: 0.5551 - val_acc: 0.9210\n",
      "Epoch 139/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5537 - acc: 0.9124 - val_loss: 0.5288 - val_acc: 0.9290\n",
      "Epoch 140/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5547 - acc: 0.9122 - val_loss: 0.5142 - val_acc: 0.9206\n",
      "Epoch 141/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5555 - acc: 0.9128 - val_loss: 0.4819 - val_acc: 0.9348\n",
      "Epoch 142/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5550 - acc: 0.9127 - val_loss: 0.4934 - val_acc: 0.9386\n",
      "Epoch 143/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5543 - acc: 0.9119 - val_loss: 0.5468 - val_acc: 0.8845\n",
      "Epoch 144/10000\n",
      "285412/285412 [==============================] - 71s 250us/step - loss: 0.5547 - acc: 0.9119 - val_loss: 0.5461 - val_acc: 0.9136\n",
      "Epoch 145/10000\n",
      "285412/285412 [==============================] - 71s 250us/step - loss: 0.5591 - acc: 0.9110 - val_loss: 0.5573 - val_acc: 0.8872\n",
      "Epoch 146/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5566 - acc: 0.9126 - val_loss: 0.4959 - val_acc: 0.9317\n",
      "Epoch 147/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5526 - acc: 0.9119 - val_loss: 0.4884 - val_acc: 0.9383\n",
      "Epoch 148/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5521 - acc: 0.9126 - val_loss: 0.4889 - val_acc: 0.9301\n",
      "Epoch 149/10000\n",
      "285412/285412 [==============================] - 71s 250us/step - loss: 0.5495 - acc: 0.9132 - val_loss: 0.5340 - val_acc: 0.9254\n",
      "Epoch 150/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5536 - acc: 0.9136 - val_loss: 0.4909 - val_acc: 0.9326\n",
      "Epoch 151/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5557 - acc: 0.9131 - val_loss: 0.4832 - val_acc: 0.9335\n",
      "Epoch 152/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5513 - acc: 0.9129 - val_loss: 0.5490 - val_acc: 0.8966\n",
      "Epoch 153/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5546 - acc: 0.9129 - val_loss: 0.4859 - val_acc: 0.9306\n",
      "Epoch 154/10000\n",
      "285412/285412 [==============================] - 72s 251us/step - loss: 0.5547 - acc: 0.9143 - val_loss: 0.4883 - val_acc: 0.9315\n",
      "Epoch 155/10000\n",
      "285412/285412 [==============================] - 71s 250us/step - loss: 0.5540 - acc: 0.9127 - val_loss: 0.5145 - val_acc: 0.9286\n",
      "Epoch 156/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5506 - acc: 0.9137 - val_loss: 0.4990 - val_acc: 0.9397\n",
      "Epoch 157/10000\n",
      "285412/285412 [==============================] - 72s 252us/step - loss: 0.5548 - acc: 0.9126 - val_loss: 0.5352 - val_acc: 0.9304\n",
      "Epoch 158/10000\n",
      "285412/285412 [==============================] - 71s 249us/step - loss: 0.5513 - acc: 0.9122 - val_loss: 0.5348 - val_acc: 0.9154\n",
      "Epoch 159/10000\n",
      " 89600/285412 [========>.....................] - ETA: 47s - loss: 0.5503 - acc: 0.9131"
     ]
    }
   ],
   "source": [
    "\n",
    "tf.get_default_graph()\n",
    "########## HYPER PARAMETERS\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 10000\n",
    "optimizer = optimizers.Adam(lr=0.001)\n",
    "\n",
    "l1v = 0.007\n",
    "l2v = 0.014\n",
    "#optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.1)\n",
    "#init=tf.global_variables_initializer()\n",
    "\n",
    "########## HYPER PARAMETERS\n",
    "########## MODEL ARCHITECTURE\n",
    "model = Sequential()\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(27,15,1), \n",
    "                 kernel_regularizer=regularizers.l1_l2(l1v,l2v)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', \n",
    "                 kernel_regularizer=regularizers.l1_l2(l1v,l2v)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='same', data_format='channels_last'))\n",
    "model.add(Dropout(0.45))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1_l2(l1v,l2v)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1_l2(l1v,l2v)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='same', data_format='channels_last'))\n",
    "model.add(Dropout(0.45))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(140, activation='relu', kernel_regularizer=regularizers.l1_l2(l1v,l2v), use_bias=True))\n",
    "model.add(Dropout(0.55))\n",
    "model.add(Dense(70, activation='relu', kernel_regularizer=regularizers.l1_l2(l1v,l2v), use_bias=True))\n",
    "model.add(Dropout(0.55))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "########## MODEL ARCHITECTURE\n",
    "####TENSORBOARD\n",
    "config = \"\"\n",
    "for layer in model.layers:\n",
    "    config += str(layer.output).split('\\\"')[1].split(\"/\")[0] + str(layer.output_shape) + \"\\n\\n\"\n",
    "#### END TENSORBOARD\n",
    "config += \"batchsize: \" + str(batch_size) + \"\\n\\n\" + \"epochs: \" + str(epochs) + \"\\n\\n\"\n",
    "\n",
    "# Print summary\n",
    "model.summary()\n",
    "current_name = \"KnuckleFinger_Jan_v3\"\n",
    "readable_timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%d_%H%M%S')\n",
    "tensorflowfolder = \"/srv/share/tensorboardfiles/\" + current_name + readable_timestamp\n",
    "print(current_name + readable_timestamp)\n",
    "logger = LoggingTensorBoard(settings_str_to_log = config, log_dir=tensorflowfolder, histogram_freq=0,\n",
    "                            write_graph=True, write_images=True, update_freq = 'epoch')\n",
    "storer = ModelCheckpoint(\"./ModelSnapshots/\" + current_name + readable_timestamp + \".h5\", monitor='val_loss', verbose=0,\n",
    "                         save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=140, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.95, \n",
    "                                            min_lr=0.00001)\n",
    "# compile model for training\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train_one_hot,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test_one_hot),\n",
    "                    callbacks=[logger, storer, learning_rate_reduction, tg_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use model for inference to get test accuracy\n",
    "y_test_pred = model.predict(x_test)\n",
    "y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "\n",
    "print ('\\n Summary of the precision, recall, F1 score for each class:')\n",
    "print (sklearn.metrics.classification_report(y_test, y_test_pred))\n",
    "\n",
    "print ('\\n Confusion matrix: ')\n",
    "print (sklearn.metrics.confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['val_acc'], label=\"Test Accuracy\")\n",
    "plt.plot(history.history['acc'], label=\"Training Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig(\"final_CNN.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save(\"CNN_01_03_19_final.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x_train, batch_size=500, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prediction=np.round(model.predict(x_train))\n",
    "train_prediction=train_prediction.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_prediction=np.round(model.predict(x_test))\n",
    "val_prediction=val_prediction.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = val_prediction - y_test_one_hot\n",
    "indices = []\n",
    "for i in range(len(val_prediction)):\n",
    "    if np.count_nonzero(delta[i]) > 0:\n",
    "        indices += [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagelist = []\n",
    "for data_point in indices:\n",
    "    print(data_point)\n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    #plt.subplot(figsize=(6,6))\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    \n",
    "    \n",
    "    data_point = indices[0]\n",
    "    data = df_train.Blobs.iloc[data_point]\n",
    "    label = \"Knuckle\" if train_prediction[data_point][0] == 1 else \"Finger\"  \n",
    "    ax.set_title(\"Input method: \" + str(df_train.InputMethod.iloc[data_point]) + \"\\n\" + \"Label as: \"  + label)\n",
    "    #plt.imsave(\"PredictionImages/\" + str(data_point)+\".pdf\", data, cmap='gray', vmin=0, vmax=255)\n",
    "    plt.imsave(\"PredictionImages/{}.png\".format(data_point), data, cmap=\"gray\", vmin=0, vmax=255)\n",
    "    #plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
