{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import *\n",
    "from keras import optimizers\n",
    "from keras import utils\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Importing matplotlib to plot images.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Importing SK-learn to calculate precision and recall\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, LeaveOneGroupOut\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "# Used for graph export\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.framework import graph_io\n",
    "from keras import backend as K\n",
    "\n",
    "import pickle as pkl\n",
    "import h5py\n",
    "\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import telegram\n",
    "from keras.callbacks import Callback\n",
    "from callbacks import TelegramCallback\n",
    "from callbacks.TelegramData import TelegramData\n",
    "\n",
    "\n",
    "# create callback\n",
    "config = {\n",
    "    'token': TelegramData.Token,   # paste your bot token\n",
    "    'telegram_id': TelegramData.ID,                                   # paste your telegram_id\n",
    "}\n",
    "\n",
    "tg_callback = TelegramCallback(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggingTensorBoard(TensorBoard):    \n",
    "\n",
    "    def __init__(self, log_dir, settings_str_to_log, **kwargs):\n",
    "        super(LoggingTensorBoard, self).__init__(log_dir, **kwargs)\n",
    "\n",
    "        self.settings_str = settings_str_to_log\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        TensorBoard.on_train_begin(self, logs=logs)\n",
    "\n",
    "        tensor =  tf.convert_to_tensor(self.settings_str)\n",
    "        summary = tf.summary.text (\"Run_Settings\", tensor)\n",
    "\n",
    "        with  tf.Session() as sess:\n",
    "            s = sess.run(summary)\n",
    "            self.writer.add_summary(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [3, 4, 5, 6, 7, 8]\n",
    "test_ids = [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, split between train and test sets\n",
    "dfAll = pd.read_pickle(\"PklData/df_blobs.pkl\")\n",
    "#df_train = dfAll[(dfAll.userID != 1) | (dfAll.userID != 2)]\n",
    "#df_test = dfAll[(dfAll.userID == 1) | (dfAll.userID == 2)]\n",
    "df_train = dfAll[dfAll.userID.isin(train_ids)]\n",
    "df_test = dfAll[dfAll.userID.isin(test_ids)]\n",
    "\n",
    "df_test = df_test.reset_index()\n",
    "df_train = df_train.reset_index()\n",
    "#Create InputMethod Column and fill it with Knuckel / Finger\n",
    "def f(row):\n",
    "    if row['TaskID'] < 17:\n",
    "        #val = \"Knuckle\"\n",
    "        val = 0\n",
    "    elif row['TaskID'] >= 17:\n",
    "        #val = \"Finger\"\n",
    "        val = 1\n",
    "    return val\n",
    "df_train['InputMethod'] = df_train.apply(f, axis=1)\n",
    "df_test['InputMethod'] = df_test.apply(f, axis=1)\n",
    "df_train2 = df_train[['Blobs', 'InputMethod']].copy()\n",
    "df_test2 = df_test[['Blobs', 'InputMethod']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.vstack(df_train2.Blobs)\n",
    "x_test = np.vstack(df_test2.Blobs)\n",
    "y_train = df_train2.InputMethod.values\n",
    "y_test = df_test2.InputMethod.values\n",
    "\n",
    "x_train = x_train.reshape(-1, 27, 15, 1)\n",
    "x_test = x_test.reshape(-1, 27, 15, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices (one-hot notation)\n",
    "num_classes = 2\n",
    "y_train_one_hot = utils.to_categorical(df_train2.InputMethod, num_classes)\n",
    "y_test_one_hot = utils.to_categorical(df_test2.InputMethod, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Label for image 1 is: 1')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAAEICAYAAADcAi8nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADgRJREFUeJzt3X+s1fV9x/Hni9+KtIoKERBRR9xYk7LFYRe2Fn/QWhOHzeKmWRdcsLitxC0xnbbLqmuWhSxzXbN0/aFSma1tnNbKOipSUmeadSo0zuLUQeTXBQYiINjYIvjeH9/v3b5c7j3n3PPjnjf3vB7JyTnf8/l+z/d9zn2d78/zvR9FBGbdNqbbBZiBg2hJOIiWgoNoKTiIloKDaCmkCqKkpyXd2u5pVfiapEOSnmutSpA0W9Jbksa2+lpZdPs9dSSIkrZLuqYTr92k3wAWA7MiYkGrLxYROyPirIg40XppnSPpSkk/kPSmpO21xm3Xe5L0PknrJB2Q1PBB6lRLxA66CNgeET8d7oSSxnWgnpHyU2AV8KkRnOc7wCPAsuFMNKJBlHSOpO9Ker1cTX5X0qwBo10q6bnyW/yEpKmV6T8g6d8lHZb0n5IWNTDPZcD9wK+Xq56/LJ//hKStkg5KWiNpRmWakPRJSVuALYO85pxynHHl8NOS/qqs7S1J/yLpXEnfkHRE0vOS5lSm/4KkXWXbJkm/WWk7Q9Lq8vN5WdKfSeqrtM+Q9Fj5GW6TdPtQ7z0inouIh4DXGvicBr6nWyS9JuloOZ/fq/ca5TxfjYgHgJcaGb86YdtvwHbgmkGePxf4beBMYArwz8B3Ku1PA7uB9wGTgceAr5dtM4E3gOsovkCLy+HzK9PeOkQ9twA/rAxfBRwAfhWYCPwD8EylPYD1wFTgjEFeb045zrjKvLcClwLvBf4L+G/gGmAc8E/A1yrTf7z8LMYBdwD/A0wq21YC/wacA8wCXgT6yrYxwCbgs8AE4BKKkH2kzt/jGoo1Qq1x/u89lZ/9EeCysu0C4JfLx7OBw8DsOq/3C0W8GszMSAZxkPHmA4cGBHFlZXgecAwYC9wJPDRg+nXA0iaC+ADwN5XhsyhWKXMqQbyqkT9aZd5/Xmm/F/heZfh64IUar3cIeH/5+KRgAbdWgngFsHPAtJ+uhryNQTxMsdA45YvYYAaGFcSRXjWfKekrknZIOgI8A5w9YE9tV+XxDmA8cB7Fdt6N5Wr5sKTDFDshFzRRyozytQGIiLcolq4zh6ijEfsqj98eZPis/gFJd5Sr3TfL9/FeivfYX1t13tXHFwEzBnwGnwGmD7PWmqLYlv5d4A+BvZL+VdIvtnMeA430zsodwGXAFRHxHuCD5fOqjHNh5fFsiiXVAYo/yEMRcXblNjkiVjZRxx6KP2oxc2kyxapyd2WcjvwsqdwevBP4HeCciDgbeJP//wz2UqyS+1U/j13AtgGfwZSIuK7ddUbEuohYTPFFfwW4r93zqOpkEMdLmlS5jaPYLnwbOFzuhNw9yHQflzRP0pnA54BHozik8HXgekkfkTS2fM1Fg+zsNOJh4A8kzZc0Efhr4NmI2N7MGx2mKcBx4HVgnKTPAu+ptD8CfLrcsZsJrKi0PQcckXRnuVMztjxc8muDzUjSGEmTKNYqKj+zCfUKlDRd0m+VX9CfA28BDR3WUWESxTYs5Twn1puuk0FcSxG6/ts9wN8DZ1As4f4DeHKQ6R4CHqTcgAduB4iIXcASilXR6xRLh0/RxHuIiA3AX1DsDO2l2Mm4abiv06R1wPcodmZ2AD/j5NXv54A+YBvwfeBRijBQfiGvp9i23kbxOd5PsWofzAcpPvu1FGuXt4GnGqhxDMXaaw9wEPgQ8Mdw0oHv2UNMe1E5n/695reBV+vNUOWGpSUl6Y+AmyLiQ92upZN65YD2aUPSBZIWlqvVyyiWTI93u65OO53PGoxWE4CvABdTHEL5FvCPXa1oBHjVbCl41WwpjOiqeYImxiQmj+QsbYQd5dCBiDh/uNO1FERJ1wJfoDgFd3+9g8uTmMwVurqVGdZpr7OAfzf1r7ZGhe/Hozvqj3WqplfN5Wm5LwIfpTgnfLOkec2+nvW2VrYRFwBbI+K1iDhGsXe3pD1lWa9pJYgzOfmMQB8n/2gAAEnLJW2UtPGd4gSB2SlaCeJgG2ynHAuKiK9GxOURcfl46p5ytB7VShD7OPmXIbMozk2aDVsrQXwemCvp4vIXHTcBa9pTlvWapg/fRMRxSSsofk0yFlgVEcO7TmGgMbWvZBx7/rk124/Nq/2LsAkvbq/ZfuKNgzXbrXNaOo4YEWspfmJk1hKf4rMUHERLwUG0FBxES8FBtBQcREsh1aUCGl+7nENXXVKz/Uf3frlm+yWP3lazfe7tz9Zst87xEtFScBAtBQfRUnAQLQUH0VJwEC0FB9FSSHUcsZ4pO39Ws/0TuxbWbJ/4xqjpjWLU8RLRUnAQLQUH0VJwEC0FB9FScBAtBQfRUkh1HDGOHavZPu6lbTXbd98yo2b7xQe31mz3P63rHi8RLQUH0VJwEC0FB9FScBAtBQfRUnAQLYVUxxGp0wvWicNv1p6+Xrul1Wo/K9uBoxTHgo9HxOXtKMp6TzuWiFdGxIE2vI71MG8jWgqtBjGApyRtkrS8HQVZb2p11bwwIvZImgasl/RKRDxTHaEM6HKASZzZ4uxstGppiRgRe8r7/RS9rC8YZBx3+GN1tdIp5GRJU/ofAx8GNrerMOstrayapwOPq+i6dhzwcEQ82ZaqrOe00uHPa8D721iL9TAfvrEUHERLwUG0FBxES8FBtBQcREvBQbQUHERLwUG0FBxES8FBtBQcREvBQbQUHERLwUG0FBxES8FBtBQcREvBQbQUHERLwUG0FBxES8FBtBQcREvBQbQUHERLwUG0FBxES8FBtBQcREvBQbQUHERLoW4QJa2StF/S5spzUyWtl7SlvD+ns2XaaNfIEvFB4NoBz90FbIiIucCGctisaXWDWHZXcXDA00uA1eXj1cANba7Lekyz24jTI2IvQHk/bagRJS2XtFHSxnf4eZOzs9Gu4zsr7mfFGtFsEPdJugCgvN/fvpKsFzUbxDXA0vLxUuCJ9pRjvaqRwzffBH4EXCapT9IyYCWwWNIWYHE5bNa0uh3+RMTNQzRd3eZarIf5zIql4CBaCg6ipeAgWgoOoqXgIFoKDqKl4CBaCg6ipeAgWgoOoqXgIFoKDqKl4CBaCg6ipeAgWgoOoqXgIFoKDqKl4CBaCg6ipeAgWgoOoqXgIFoKDqKl4CBaCg6ipeAgWgoOoqXgIFoKDqKl0Gw/K/dI2i3phfJ2XWfLtNGu2X5WAD4fEfPL29r2lmW9ptl+VszaqpVtxBWSXixX3UN2geZ+VqwRzQbxS8ClwHxgL3DvUCO6nxVrRFNBjIh9EXEiIt4F7gMWtLcs6zVNBbG/s5/Sx4DNQ41r1oi63VuU/awsAs6T1AfcDSySNB8IYDtwWwdrtB7QbD8rD3SgFuthPrNiKTiIloKDaCk4iJaCg2gpOIiWgoNoKTiIloKDaCk4iJaCg2gpOIiWgoNoKTiIloKDaCk4iJaCg2gpOIiWgoNoKTiIloKDaCk4iJaCg2gpOIiWgoNoKTiIloKDaCk4iJaCg2gpOIiWgoNoKTTSz8qFkn4g6WVJL0n6k/L5qZLWS9pS3g/5D93N6mlkiXgcuCMifgn4APBJSfOAu4ANETEX2FAOmzWlkX5W9kbEj8vHR4GXgZnAEmB1Odpq4IZOFWmj37C2ESXNAX4FeBaYHhF7oQgrMG2IadzPitXVcBAlnQU8BvxpRBxpdDr3s2KNaCiIksZThPAbEfHt8ul9/d1clPf7O1Oi9YJG9ppF0YvAyxHxd5WmNcDS8vFS4In2l2e9om73FsBC4PeBn0h6oXzuM8BK4BFJy4CdwI2dKdF6QSP9rPwQ0BDNV7e3HOtVPrNiKTiIloKDaCk4iJaCg2gpOIiWgoNoKTiIloKDaCk4iJaCg2gpOIiWgoNoKTiIloKDaCk4iJaCg2gpOIiWgoNoKTiIloKDaCk4iJaCg2gpOIiWgoNoKTiIloKDaCk4iJaCg2gpOIiWgoNoKTiIlkIrHf7cI2m3pBfK23WdL9dGq0b+dXF/hz8/ljQF2CRpfdn2+Yj4286VZ72ikX9dvBfo70/lqKT+Dn/M2qaVDn8AVkh6UdKqofric4c/1ohWOvz5EnApMJ9iiXnvYNO5wx9rRNMd/kTEvog4ERHvAvcBCzpXpo12TXf409/rVOljwOb2l2e9opUOf26WNB8IYDtwW0cqtJ7QSoc/a9tfjvUqn1mxFBxES8FBtBQcREvBQbQUHERLQRExcjOTXgd2VJ46DzgwYgUMX/b6IF+NF0XE+cOdaESDeMrMpY0RcXnXCqgje31wetTYCK+aLQUH0VLodhC/2uX515O9Pjg9aqyrq9uIZv26vUQ0AxxES6IrQZR0raRXJW2VdFc3aqhH0nZJPykvld2YoJ5VkvZL2lx5bqqk9ZK2lPeDXjd0OhjxIEoaC3wR+Cgwj+IHtvNGuo4GXRkR85Mcp3sQuHbAc3cBGyJiLrChHD4tdWOJuADYGhGvRcQx4FvAki7UcVqJiGeAgwOeXgKsLh+vBm4Y0aLaqBtBnAnsqgz3kfM66QCekrRJ0vJuFzOE6eV15/3Xn0/rcj1Na+SalXYb7LKDjMeQFkbEHknTgPWSXimXStYB3Vgi9gEXVoZnAXu6UEdNEbGnvN8PPE7Oy2X39V9NWd7v73I9TetGEJ8H5kq6WNIE4CZgTRfqGJKkyeX/+UHSZODD5Lxcdg2wtHy8FHiii7W0ZMRXzRFxXNIKYB0wFlgVES+NdB11TAceLy7pZhzwcEQ82c2CJH0TWAScJ6kPuBtYCTwiaRmwE7ixexW2xqf4LAWfWbEUHERLwUG0FBxES8FBtBQcREvBQbQU/hcdcNahNnkF+QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 1\n",
    "plt.imshow(x_train[i].reshape(27, 15)) #np.sqrt(784) = 28\n",
    "plt.title(\"Label for image %i is: %s\" % (i, y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If GPU is not available: \n",
    "# GPU_USE = '/cpu:0'\n",
    "# config = tf.ConfigProto(device_count = {\"GPU\": 0})\n",
    "\n",
    "\n",
    "# If GPU is available: \n",
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = True\n",
    "config.allow_soft_placement = True\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "\n",
    "# Limit the maximum memory used\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.1\n",
    "\n",
    "# set session config\n",
    "tf.keras.backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 27, 15, 84)        840       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 27, 15, 84)        63588     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 7, 84)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 13, 7, 84)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 7, 42)         31794     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 7, 42)         15918     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 3, 42)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 3, 42)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 756)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 400)               302800    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               40100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 455,242\n",
      "Trainable params: 455,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 53098 samples, validate on 19209 samples\n",
      "Epoch 1/550\n",
      "53098/53098 [==============================] - 11s 201us/step - loss: 0.2954 - acc: 0.8769 - val_loss: 0.4294 - val_acc: 0.8191\n",
      "Epoch 2/550\n",
      "53098/53098 [==============================] - 9s 175us/step - loss: 0.1607 - acc: 0.9367 - val_loss: 0.3089 - val_acc: 0.8802\n",
      "Epoch 3/550\n",
      "53098/53098 [==============================] - 10s 192us/step - loss: 0.1361 - acc: 0.9472 - val_loss: 0.2882 - val_acc: 0.8913\n",
      "Epoch 4/550\n",
      "53098/53098 [==============================] - 10s 179us/step - loss: 0.1193 - acc: 0.9535 - val_loss: 0.3427 - val_acc: 0.8699\n",
      "Epoch 5/550\n",
      "53098/53098 [==============================] - 9s 176us/step - loss: 0.1124 - acc: 0.9565 - val_loss: 0.3033 - val_acc: 0.8879\n",
      "Epoch 6/550\n",
      "53098/53098 [==============================] - 9s 177us/step - loss: 0.0984 - acc: 0.9625 - val_loss: 0.3902 - val_acc: 0.8565\n",
      "Epoch 7/550\n",
      "53098/53098 [==============================] - 10s 179us/step - loss: 0.0928 - acc: 0.9640 - val_loss: 0.2918 - val_acc: 0.8968\n",
      "Epoch 8/550\n",
      "53098/53098 [==============================] - 10s 180us/step - loss: 0.0890 - acc: 0.9658 - val_loss: 0.3856 - val_acc: 0.8817\n",
      "Epoch 9/550\n",
      "53098/53098 [==============================] - 9s 173us/step - loss: 0.0818 - acc: 0.9690 - val_loss: 0.3043 - val_acc: 0.8950\n",
      "Epoch 10/550\n",
      "53098/53098 [==============================] - 10s 193us/step - loss: 0.0807 - acc: 0.9697 - val_loss: 0.3083 - val_acc: 0.8936\n",
      "Epoch 11/550\n",
      "53098/53098 [==============================] - 11s 203us/step - loss: 0.0744 - acc: 0.9719 - val_loss: 0.3676 - val_acc: 0.8789\n",
      "Epoch 12/550\n",
      "53098/53098 [==============================] - 11s 199us/step - loss: 0.0724 - acc: 0.9727 - val_loss: 0.2765 - val_acc: 0.9142\n",
      "Epoch 13/550\n",
      "53098/53098 [==============================] - 10s 183us/step - loss: 0.0704 - acc: 0.9729 - val_loss: 0.2661 - val_acc: 0.9126\n",
      "Epoch 14/550\n",
      "53098/53098 [==============================] - 10s 190us/step - loss: 0.0671 - acc: 0.9745 - val_loss: 0.4978 - val_acc: 0.8529\n",
      "Epoch 15/550\n",
      "53098/53098 [==============================] - 10s 182us/step - loss: 0.0663 - acc: 0.9750 - val_loss: 0.2824 - val_acc: 0.9025\n",
      "Epoch 16/550\n",
      "53098/53098 [==============================] - 10s 188us/step - loss: 0.0616 - acc: 0.9765 - val_loss: 0.3284 - val_acc: 0.9016\n",
      "Epoch 17/550\n",
      "53098/53098 [==============================] - 10s 195us/step - loss: 0.0601 - acc: 0.9773 - val_loss: 0.3072 - val_acc: 0.9051\n",
      "Epoch 18/550\n",
      "53098/53098 [==============================] - 10s 186us/step - loss: 0.0583 - acc: 0.9782 - val_loss: 0.3484 - val_acc: 0.8915\n",
      "Epoch 19/550\n",
      "53098/53098 [==============================] - 10s 194us/step - loss: 0.0571 - acc: 0.9785 - val_loss: 0.3453 - val_acc: 0.9012\n",
      "Epoch 20/550\n",
      "53098/53098 [==============================] - 11s 207us/step - loss: 0.0545 - acc: 0.9793 - val_loss: 0.3668 - val_acc: 0.8936\n",
      "Epoch 21/550\n",
      "53098/53098 [==============================] - 10s 184us/step - loss: 0.0517 - acc: 0.9803 - val_loss: 0.2645 - val_acc: 0.9179\n",
      "Epoch 22/550\n",
      "53098/53098 [==============================] - 10s 190us/step - loss: 0.0520 - acc: 0.9804 - val_loss: 0.3663 - val_acc: 0.8926\n",
      "Epoch 23/550\n",
      "53098/53098 [==============================] - 10s 181us/step - loss: 0.0540 - acc: 0.9793 - val_loss: 0.3266 - val_acc: 0.9055\n",
      "Epoch 24/550\n",
      "53098/53098 [==============================] - 10s 192us/step - loss: 0.0480 - acc: 0.9817 - val_loss: 0.2968 - val_acc: 0.9028\n",
      "Epoch 25/550\n",
      "53098/53098 [==============================] - 11s 206us/step - loss: 0.0460 - acc: 0.9823 - val_loss: 0.3184 - val_acc: 0.9059\n",
      "Epoch 26/550\n",
      "53098/53098 [==============================] - 11s 203us/step - loss: 0.0462 - acc: 0.9823 - val_loss: 0.3502 - val_acc: 0.8932\n",
      "Epoch 27/550\n",
      "53098/53098 [==============================] - 11s 200us/step - loss: 0.0429 - acc: 0.9843 - val_loss: 0.3379 - val_acc: 0.9060\n",
      "Epoch 28/550\n",
      "53098/53098 [==============================] - 10s 194us/step - loss: 0.0429 - acc: 0.9833 - val_loss: 0.3427 - val_acc: 0.9035\n",
      "Epoch 29/550\n",
      "53098/53098 [==============================] - 11s 210us/step - loss: 0.0393 - acc: 0.9847 - val_loss: 0.3456 - val_acc: 0.9074\n",
      "Epoch 30/550\n",
      "53098/53098 [==============================] - 10s 197us/step - loss: 0.0397 - acc: 0.9849 - val_loss: 0.3815 - val_acc: 0.8938\n",
      "Epoch 31/550\n",
      "53098/53098 [==============================] - 10s 187us/step - loss: 0.0401 - acc: 0.9844 - val_loss: 0.3412 - val_acc: 0.9079\n",
      "Epoch 32/550\n",
      "53098/53098 [==============================] - 10s 186us/step - loss: 0.0388 - acc: 0.9846 - val_loss: 0.3583 - val_acc: 0.9060\n",
      "Epoch 33/550\n",
      "53098/53098 [==============================] - 10s 194us/step - loss: 0.0355 - acc: 0.9863 - val_loss: 0.5118 - val_acc: 0.8889\n",
      "Epoch 34/550\n",
      "53098/53098 [==============================] - 10s 188us/step - loss: 0.0357 - acc: 0.9862 - val_loss: 0.3823 - val_acc: 0.8991\n",
      "Epoch 35/550\n",
      "53098/53098 [==============================] - 11s 198us/step - loss: 0.0331 - acc: 0.9878 - val_loss: 0.3759 - val_acc: 0.9032\n",
      "Epoch 36/550\n",
      "53098/53098 [==============================] - 10s 195us/step - loss: 0.0330 - acc: 0.9869 - val_loss: 0.3818 - val_acc: 0.8990\n",
      "Epoch 37/550\n",
      "53098/53098 [==============================] - 10s 186us/step - loss: 0.0298 - acc: 0.9885 - val_loss: 0.3737 - val_acc: 0.9076\n",
      "Epoch 38/550\n",
      "53098/53098 [==============================] - 10s 195us/step - loss: 0.0310 - acc: 0.9884 - val_loss: 0.4096 - val_acc: 0.9072\n",
      "Epoch 39/550\n",
      "53098/53098 [==============================] - 10s 184us/step - loss: 0.0302 - acc: 0.9885 - val_loss: 0.3703 - val_acc: 0.9112\n",
      "Epoch 40/550\n",
      "53098/53098 [==============================] - 10s 188us/step - loss: 0.0313 - acc: 0.9878 - val_loss: 0.4264 - val_acc: 0.8917\n",
      "Epoch 41/550\n",
      "53098/53098 [==============================] - 10s 182us/step - loss: 0.0289 - acc: 0.9890 - val_loss: 0.4010 - val_acc: 0.8988\n",
      "Epoch 42/550\n",
      "53098/53098 [==============================] - 10s 182us/step - loss: 0.0280 - acc: 0.9892 - val_loss: 0.4155 - val_acc: 0.9087\n",
      "Epoch 43/550\n",
      "53098/53098 [==============================] - 10s 184us/step - loss: 0.0281 - acc: 0.9889 - val_loss: 0.4054 - val_acc: 0.9018\n",
      "Epoch 44/550\n",
      "53098/53098 [==============================] - 10s 187us/step - loss: 0.0255 - acc: 0.9904 - val_loss: 0.5746 - val_acc: 0.8772\n",
      "Epoch 45/550\n",
      "53098/53098 [==============================] - 10s 186us/step - loss: 0.0247 - acc: 0.9903 - val_loss: 0.3997 - val_acc: 0.9128\n",
      "Epoch 46/550\n",
      "53098/53098 [==============================] - 10s 188us/step - loss: 0.0233 - acc: 0.9911 - val_loss: 0.4720 - val_acc: 0.8979\n",
      "Epoch 47/550\n",
      "53098/53098 [==============================] - 10s 182us/step - loss: 0.0258 - acc: 0.9897 - val_loss: 0.4437 - val_acc: 0.8969\n",
      "Epoch 48/550\n",
      "53098/53098 [==============================] - 10s 189us/step - loss: 0.0250 - acc: 0.9908 - val_loss: 0.3953 - val_acc: 0.9034\n",
      "Epoch 49/550\n",
      "53098/53098 [==============================] - 10s 182us/step - loss: 0.0230 - acc: 0.9911 - val_loss: 0.4329 - val_acc: 0.8937\n",
      "Epoch 50/550\n",
      "53098/53098 [==============================] - 10s 183us/step - loss: 0.0230 - acc: 0.9912 - val_loss: 0.4230 - val_acc: 0.9002\n",
      "Epoch 51/550\n",
      "53098/53098 [==============================] - 10s 180us/step - loss: 0.0224 - acc: 0.9912 - val_loss: 0.4122 - val_acc: 0.9035\n",
      "Epoch 52/550\n",
      "53098/53098 [==============================] - 10s 180us/step - loss: 0.0220 - acc: 0.9912 - val_loss: 0.4932 - val_acc: 0.8944\n",
      "Epoch 53/550\n",
      "53098/53098 [==============================] - 9s 175us/step - loss: 0.0197 - acc: 0.9925 - val_loss: 0.4171 - val_acc: 0.9097\n",
      "Epoch 54/550\n",
      "53098/53098 [==============================] - 10s 187us/step - loss: 0.0205 - acc: 0.9919 - val_loss: 0.4492 - val_acc: 0.9033\n",
      "Epoch 55/550\n",
      "53098/53098 [==============================] - 10s 184us/step - loss: 0.0202 - acc: 0.9927 - val_loss: 0.4777 - val_acc: 0.8987\n",
      "Epoch 56/550\n",
      "53098/53098 [==============================] - 10s 194us/step - loss: 0.0193 - acc: 0.9925 - val_loss: 0.5218 - val_acc: 0.8911\n",
      "Epoch 57/550\n",
      "53098/53098 [==============================] - 10s 183us/step - loss: 0.0182 - acc: 0.9931 - val_loss: 0.5598 - val_acc: 0.8952\n",
      "Epoch 58/550\n",
      "53098/53098 [==============================] - 10s 186us/step - loss: 0.0210 - acc: 0.9922 - val_loss: 0.4108 - val_acc: 0.9067\n",
      "Epoch 59/550\n",
      "53098/53098 [==============================] - 10s 190us/step - loss: 0.0190 - acc: 0.9932 - val_loss: 0.4644 - val_acc: 0.9037\n",
      "Epoch 60/550\n",
      "53098/53098 [==============================] - 10s 186us/step - loss: 0.0180 - acc: 0.9934 - val_loss: 0.4431 - val_acc: 0.9037\n",
      "Epoch 61/550\n",
      "53098/53098 [==============================] - 10s 188us/step - loss: 0.0182 - acc: 0.9930 - val_loss: 0.4397 - val_acc: 0.9114\n",
      "Epoch 62/550\n",
      "53098/53098 [==============================] - 10s 183us/step - loss: 0.0194 - acc: 0.9926 - val_loss: 0.4431 - val_acc: 0.8963\n",
      "Epoch 63/550\n",
      "53098/53098 [==============================] - 10s 186us/step - loss: 0.0184 - acc: 0.9930 - val_loss: 0.4683 - val_acc: 0.9033\n",
      "Epoch 64/550\n",
      "53098/53098 [==============================] - 10s 186us/step - loss: 0.0180 - acc: 0.9927 - val_loss: 0.4951 - val_acc: 0.8951\n",
      "Epoch 65/550\n",
      "53098/53098 [==============================] - 10s 184us/step - loss: 0.0165 - acc: 0.9938 - val_loss: 0.4051 - val_acc: 0.9114\n",
      "Epoch 66/550\n",
      "53098/53098 [==============================] - 10s 188us/step - loss: 0.0161 - acc: 0.9939 - val_loss: 0.5284 - val_acc: 0.9008\n",
      "Epoch 67/550\n",
      "53098/53098 [==============================] - 10s 187us/step - loss: 0.0158 - acc: 0.9939 - val_loss: 0.5436 - val_acc: 0.8966\n",
      "Epoch 68/550\n",
      "53098/53098 [==============================] - 9s 176us/step - loss: 0.0158 - acc: 0.9941 - val_loss: 0.5037 - val_acc: 0.8975\n",
      "Epoch 69/550\n",
      "53098/53098 [==============================] - 10s 181us/step - loss: 0.0170 - acc: 0.9937 - val_loss: 0.4883 - val_acc: 0.9030\n",
      "Epoch 70/550\n",
      "53098/53098 [==============================] - 10s 183us/step - loss: 0.0179 - acc: 0.9936 - val_loss: 0.5189 - val_acc: 0.9012\n",
      "Epoch 71/550\n",
      "53098/53098 [==============================] - 10s 184us/step - loss: 0.0155 - acc: 0.9944 - val_loss: 0.4674 - val_acc: 0.9097\n",
      "Epoch 72/550\n",
      "53098/53098 [==============================] - 10s 191us/step - loss: 0.0147 - acc: 0.9947 - val_loss: 0.4028 - val_acc: 0.9184\n",
      "Epoch 73/550\n",
      "53098/53098 [==============================] - 10s 191us/step - loss: 0.0151 - acc: 0.9950 - val_loss: 0.5915 - val_acc: 0.8944\n",
      "Epoch 74/550\n",
      "53098/53098 [==============================] - 10s 185us/step - loss: 0.0148 - acc: 0.9946 - val_loss: 0.5436 - val_acc: 0.8997\n",
      "Epoch 75/550\n",
      "53098/53098 [==============================] - 10s 186us/step - loss: 0.0142 - acc: 0.9949 - val_loss: 0.5121 - val_acc: 0.9035\n",
      "Epoch 76/550\n",
      "53098/53098 [==============================] - 10s 184us/step - loss: 0.0144 - acc: 0.9945 - val_loss: 0.5061 - val_acc: 0.9078\n",
      "Epoch 77/550\n",
      "53098/53098 [==============================] - 10s 188us/step - loss: 0.0154 - acc: 0.9944 - val_loss: 0.5266 - val_acc: 0.9008\n",
      "Epoch 78/550\n",
      "53098/53098 [==============================] - 10s 187us/step - loss: 0.0148 - acc: 0.9946 - val_loss: 0.5127 - val_acc: 0.9030\n",
      "Epoch 79/550\n",
      "53098/53098 [==============================] - 10s 185us/step - loss: 0.0149 - acc: 0.9944 - val_loss: 0.5426 - val_acc: 0.8946\n",
      "Epoch 80/550\n",
      "53098/53098 [==============================] - 10s 190us/step - loss: 0.0130 - acc: 0.9952 - val_loss: 0.6350 - val_acc: 0.8924\n",
      "Epoch 81/550\n",
      "53098/53098 [==============================] - 10s 181us/step - loss: 0.0140 - acc: 0.9947 - val_loss: 0.5273 - val_acc: 0.9035\n",
      "Epoch 82/550\n",
      "53098/53098 [==============================] - 10s 191us/step - loss: 0.0136 - acc: 0.9953 - val_loss: 0.5844 - val_acc: 0.8998\n",
      "Epoch 83/550\n",
      "53098/53098 [==============================] - 10s 183us/step - loss: 0.0129 - acc: 0.9953 - val_loss: 0.4866 - val_acc: 0.9074\n",
      "Epoch 84/550\n",
      "53098/53098 [==============================] - 10s 181us/step - loss: 0.0126 - acc: 0.9952 - val_loss: 0.6121 - val_acc: 0.8910\n",
      "Epoch 85/550\n",
      "53098/53098 [==============================] - 10s 186us/step - loss: 0.0131 - acc: 0.9949 - val_loss: 0.5736 - val_acc: 0.8995\n",
      "Epoch 86/550\n",
      "53098/53098 [==============================] - 10s 189us/step - loss: 0.0130 - acc: 0.9953 - val_loss: 0.4869 - val_acc: 0.9087\n",
      "Epoch 87/550\n",
      "53098/53098 [==============================] - 10s 184us/step - loss: 0.0126 - acc: 0.9955 - val_loss: 0.5281 - val_acc: 0.8933\n",
      "Epoch 88/550\n",
      "53098/53098 [==============================] - 10s 181us/step - loss: 0.0139 - acc: 0.9950 - val_loss: 0.6062 - val_acc: 0.9047\n",
      "Epoch 89/550\n",
      "53098/53098 [==============================] - 10s 184us/step - loss: 0.0135 - acc: 0.9950 - val_loss: 0.6217 - val_acc: 0.8995\n",
      "Epoch 90/550\n",
      "53098/53098 [==============================] - 10s 184us/step - loss: 0.0122 - acc: 0.9957 - val_loss: 0.5712 - val_acc: 0.9003\n",
      "Epoch 91/550\n",
      "53098/53098 [==============================] - 10s 189us/step - loss: 0.0122 - acc: 0.9954 - val_loss: 0.5033 - val_acc: 0.9030\n",
      "Epoch 92/550\n",
      "53098/53098 [==============================] - 10s 186us/step - loss: 0.0135 - acc: 0.9953 - val_loss: 0.5541 - val_acc: 0.9061\n",
      "Epoch 93/550\n",
      "53098/53098 [==============================] - 10s 184us/step - loss: 0.0104 - acc: 0.9961 - val_loss: 0.5185 - val_acc: 0.9025\n",
      "Epoch 94/550\n",
      "53098/53098 [==============================] - 10s 182us/step - loss: 0.0116 - acc: 0.9957 - val_loss: 0.5691 - val_acc: 0.9023\n",
      "Epoch 95/550\n",
      "53098/53098 [==============================] - 10s 191us/step - loss: 0.0110 - acc: 0.9962 - val_loss: 0.5750 - val_acc: 0.9003\n",
      "Epoch 96/550\n",
      "53098/53098 [==============================] - 10s 183us/step - loss: 0.0098 - acc: 0.9964 - val_loss: 0.6359 - val_acc: 0.8985\n",
      "Epoch 97/550\n",
      "53098/53098 [==============================] - 10s 183us/step - loss: 0.0127 - acc: 0.9958 - val_loss: 0.4512 - val_acc: 0.9027\n",
      "Epoch 98/550\n",
      "53098/53098 [==============================] - 10s 188us/step - loss: 0.0110 - acc: 0.9960 - val_loss: 0.5711 - val_acc: 0.9011\n",
      "Epoch 99/550\n",
      "53098/53098 [==============================] - 10s 185us/step - loss: 0.0106 - acc: 0.9965 - val_loss: 0.4854 - val_acc: 0.9131\n",
      "Epoch 100/550\n",
      "53098/53098 [==============================] - 10s 185us/step - loss: 0.0114 - acc: 0.9958 - val_loss: 0.5644 - val_acc: 0.8985\n",
      "Epoch 101/550\n",
      "53098/53098 [==============================] - 10s 185us/step - loss: 0.0125 - acc: 0.9953 - val_loss: 0.5208 - val_acc: 0.8971\n",
      "Epoch 102/550\n",
      "53098/53098 [==============================] - 10s 187us/step - loss: 0.0096 - acc: 0.9965 - val_loss: 0.5788 - val_acc: 0.9053\n",
      "Epoch 103/550\n",
      "53098/53098 [==============================] - 10s 185us/step - loss: 0.0129 - acc: 0.9953 - val_loss: 0.5895 - val_acc: 0.8977\n",
      "Epoch 104/550\n",
      "53098/53098 [==============================] - 10s 181us/step - loss: 0.0089 - acc: 0.9970 - val_loss: 0.6229 - val_acc: 0.8924\n",
      "Epoch 105/550\n",
      "53098/53098 [==============================] - 10s 186us/step - loss: 0.0111 - acc: 0.9960 - val_loss: 0.6370 - val_acc: 0.8954\n",
      "Epoch 106/550\n",
      "53098/53098 [==============================] - 10s 183us/step - loss: 0.0103 - acc: 0.9962 - val_loss: 0.5745 - val_acc: 0.9067\n",
      "Epoch 107/550\n",
      "53098/53098 [==============================] - 10s 181us/step - loss: 0.0116 - acc: 0.9958 - val_loss: 0.5825 - val_acc: 0.9119\n",
      "Epoch 108/550\n",
      "53098/53098 [==============================] - 10s 185us/step - loss: 0.0112 - acc: 0.9960 - val_loss: 0.4436 - val_acc: 0.9079\n",
      "Epoch 109/550\n",
      "53098/53098 [==============================] - 10s 188us/step - loss: 0.0116 - acc: 0.9960 - val_loss: 0.5101 - val_acc: 0.9036\n",
      "Epoch 110/550\n",
      "53098/53098 [==============================] - 10s 189us/step - loss: 0.0096 - acc: 0.9966 - val_loss: 0.5089 - val_acc: 0.9054\n",
      "Epoch 111/550\n",
      "53098/53098 [==============================] - 10s 189us/step - loss: 0.0100 - acc: 0.9965 - val_loss: 0.5147 - val_acc: 0.8980\n",
      "Epoch 112/550\n",
      "53098/53098 [==============================] - 10s 187us/step - loss: 0.0095 - acc: 0.9965 - val_loss: 0.6349 - val_acc: 0.9032\n",
      "Epoch 113/550\n",
      "53098/53098 [==============================] - 10s 186us/step - loss: 0.0107 - acc: 0.9963 - val_loss: 0.4839 - val_acc: 0.9091\n",
      "Epoch 114/550\n",
      "53098/53098 [==============================] - 10s 188us/step - loss: 0.0091 - acc: 0.9966 - val_loss: 0.6308 - val_acc: 0.8933\n",
      "Epoch 115/550\n",
      "53098/53098 [==============================] - 10s 185us/step - loss: 0.0072 - acc: 0.9974 - val_loss: 0.6363 - val_acc: 0.8923\n",
      "Epoch 116/550\n",
      "53098/53098 [==============================] - 10s 186us/step - loss: 0.0092 - acc: 0.9967 - val_loss: 0.6469 - val_acc: 0.8949\n",
      "Epoch 117/550\n",
      "53098/53098 [==============================] - 10s 190us/step - loss: 0.0097 - acc: 0.9965 - val_loss: 0.4990 - val_acc: 0.9091\n",
      "Epoch 118/550\n",
      "53098/53098 [==============================] - 10s 186us/step - loss: 0.0102 - acc: 0.9964 - val_loss: 0.5965 - val_acc: 0.9024\n",
      "Epoch 119/550\n",
      "53098/53098 [==============================] - 10s 187us/step - loss: 0.0115 - acc: 0.9956 - val_loss: 0.6044 - val_acc: 0.9016\n",
      "Epoch 120/550\n",
      "53098/53098 [==============================] - 10s 185us/step - loss: 0.0100 - acc: 0.9963 - val_loss: 0.5566 - val_acc: 0.9097\n",
      "Epoch 121/550\n",
      "53098/53098 [==============================] - 10s 188us/step - loss: 0.0108 - acc: 0.9962 - val_loss: 0.5895 - val_acc: 0.9035\n",
      "Epoch 122/550\n",
      "53098/53098 [==============================] - 10s 188us/step - loss: 0.0098 - acc: 0.9965 - val_loss: 0.5495 - val_acc: 0.9019\n",
      "Epoch 123/550\n",
      "53098/53098 [==============================] - 10s 184us/step - loss: 0.0082 - acc: 0.9971 - val_loss: 0.6175 - val_acc: 0.8970\n",
      "Epoch 124/550\n",
      "53098/53098 [==============================] - 10s 188us/step - loss: 0.0103 - acc: 0.9964 - val_loss: 0.6761 - val_acc: 0.8981\n",
      "Epoch 125/550\n",
      "53098/53098 [==============================] - 10s 190us/step - loss: 0.0085 - acc: 0.9969 - val_loss: 0.6794 - val_acc: 0.8973\n",
      "Epoch 126/550\n",
      "53098/53098 [==============================] - 10s 188us/step - loss: 0.0100 - acc: 0.9963 - val_loss: 0.6600 - val_acc: 0.9009\n",
      "Epoch 127/550\n",
      "53098/53098 [==============================] - 10s 184us/step - loss: 0.0083 - acc: 0.9967 - val_loss: 0.5438 - val_acc: 0.9113\n",
      "Epoch 128/550\n",
      "53098/53098 [==============================] - 10s 186us/step - loss: 0.0088 - acc: 0.9966 - val_loss: 0.6668 - val_acc: 0.8843\n",
      "Epoch 129/550\n",
      "53098/53098 [==============================] - 10s 186us/step - loss: 0.0100 - acc: 0.9964 - val_loss: 0.6542 - val_acc: 0.8972\n",
      "Epoch 130/550\n",
      "53098/53098 [==============================] - 10s 182us/step - loss: 0.0080 - acc: 0.9970 - val_loss: 0.8409 - val_acc: 0.8821\n",
      "Epoch 131/550\n",
      "53098/53098 [==============================] - 10s 180us/step - loss: 0.0094 - acc: 0.9969 - val_loss: 0.6050 - val_acc: 0.9044\n",
      "Epoch 132/550\n",
      "53098/53098 [==============================] - 10s 184us/step - loss: 0.0070 - acc: 0.9975 - val_loss: 0.6254 - val_acc: 0.9029\n",
      "Epoch 133/550\n",
      "53098/53098 [==============================] - 10s 186us/step - loss: 0.0092 - acc: 0.9969 - val_loss: 0.4939 - val_acc: 0.9118\n",
      "Epoch 134/550\n",
      "53098/53098 [==============================] - 10s 185us/step - loss: 0.0086 - acc: 0.9970 - val_loss: 0.5141 - val_acc: 0.9057\n",
      "Epoch 135/550\n",
      "53098/53098 [==============================] - 10s 186us/step - loss: 0.0080 - acc: 0.9971 - val_loss: 0.6639 - val_acc: 0.9049\n",
      "Epoch 136/550\n",
      "53098/53098 [==============================] - 10s 181us/step - loss: 0.0102 - acc: 0.9964 - val_loss: 0.5008 - val_acc: 0.8975\n",
      "Epoch 137/550\n",
      "53098/53098 [==============================] - 10s 186us/step - loss: 0.0080 - acc: 0.9972 - val_loss: 0.7125 - val_acc: 0.8924\n",
      "Epoch 138/550\n",
      "53098/53098 [==============================] - 10s 187us/step - loss: 0.0085 - acc: 0.9971 - val_loss: 0.6137 - val_acc: 0.9050\n",
      "Epoch 139/550\n",
      "53098/53098 [==============================] - 10s 187us/step - loss: 0.0082 - acc: 0.9970 - val_loss: 0.6864 - val_acc: 0.8991\n",
      "Epoch 140/550\n",
      "53098/53098 [==============================] - 10s 190us/step - loss: 0.0074 - acc: 0.9974 - val_loss: 0.6478 - val_acc: 0.8979\n",
      "Epoch 141/550\n",
      "53098/53098 [==============================] - 10s 186us/step - loss: 0.0097 - acc: 0.9970 - val_loss: 0.5942 - val_acc: 0.8966\n",
      "Epoch 142/550\n",
      "53098/53098 [==============================] - 10s 187us/step - loss: 0.0082 - acc: 0.9973 - val_loss: 0.6821 - val_acc: 0.9031\n",
      "Epoch 143/550\n",
      "53098/53098 [==============================] - 10s 185us/step - loss: 0.0088 - acc: 0.9968 - val_loss: 0.7391 - val_acc: 0.8988\n",
      "Epoch 144/550\n",
      "53098/53098 [==============================] - 10s 183us/step - loss: 0.0073 - acc: 0.9975 - val_loss: 0.6753 - val_acc: 0.9012\n",
      "Epoch 145/550\n",
      "53098/53098 [==============================] - 10s 180us/step - loss: 0.0081 - acc: 0.9971 - val_loss: 0.6654 - val_acc: 0.8956\n",
      "Epoch 146/550\n",
      "53098/53098 [==============================] - 10s 188us/step - loss: 0.0085 - acc: 0.9969 - val_loss: 0.6872 - val_acc: 0.8988\n",
      "Epoch 147/550\n",
      "53098/53098 [==============================] - 10s 183us/step - loss: 0.0081 - acc: 0.9971 - val_loss: 0.7560 - val_acc: 0.8994\n",
      "Epoch 148/550\n",
      "53098/53098 [==============================] - 10s 183us/step - loss: 0.0084 - acc: 0.9972 - val_loss: 0.6231 - val_acc: 0.8993\n",
      "Epoch 149/550\n",
      "53098/53098 [==============================] - 10s 187us/step - loss: 0.0091 - acc: 0.9968 - val_loss: 0.5629 - val_acc: 0.9126\n",
      "Epoch 150/550\n",
      "53098/53098 [==============================] - 10s 187us/step - loss: 0.0093 - acc: 0.9968 - val_loss: 0.4658 - val_acc: 0.9096\n",
      "Epoch 151/550\n",
      "53098/53098 [==============================] - 10s 187us/step - loss: 0.0085 - acc: 0.9971 - val_loss: 0.6139 - val_acc: 0.9049\n",
      "Epoch 152/550\n",
      "53098/53098 [==============================] - 10s 187us/step - loss: 0.0076 - acc: 0.9975 - val_loss: 0.6700 - val_acc: 0.9045\n",
      "Epoch 153/550\n",
      "53098/53098 [==============================] - 10s 186us/step - loss: 0.0070 - acc: 0.9975 - val_loss: 0.6934 - val_acc: 0.9063\n",
      "Epoch 154/550\n",
      "53098/53098 [==============================] - 10s 188us/step - loss: 0.0093 - acc: 0.9970 - val_loss: 0.7005 - val_acc: 0.9007\n",
      "Epoch 155/550\n",
      "53098/53098 [==============================] - 10s 190us/step - loss: 0.0092 - acc: 0.9971 - val_loss: 0.4740 - val_acc: 0.9111\n",
      "Epoch 156/550\n",
      "53098/53098 [==============================] - 10s 189us/step - loss: 0.0096 - acc: 0.9965 - val_loss: 0.5767 - val_acc: 0.9031\n",
      "Epoch 157/550\n",
      "53098/53098 [==============================] - 10s 188us/step - loss: 0.0081 - acc: 0.9972 - val_loss: 0.6641 - val_acc: 0.8991\n",
      "Epoch 158/550\n",
      "53098/53098 [==============================] - 10s 182us/step - loss: 0.0090 - acc: 0.9969 - val_loss: 0.6894 - val_acc: 0.8895\n",
      "Epoch 159/550\n",
      "53098/53098 [==============================] - 10s 187us/step - loss: 0.0076 - acc: 0.9974 - val_loss: 0.5828 - val_acc: 0.9096\n",
      "Epoch 160/550\n",
      "53098/53098 [==============================] - 10s 186us/step - loss: 0.0071 - acc: 0.9975 - val_loss: 0.6523 - val_acc: 0.8870\n",
      "Epoch 161/550\n",
      "53098/53098 [==============================] - 10s 185us/step - loss: 0.0061 - acc: 0.9979 - val_loss: 0.6758 - val_acc: 0.9114\n",
      "Epoch 162/550\n",
      "53098/53098 [==============================] - 10s 185us/step - loss: 0.0083 - acc: 0.9971 - val_loss: 0.5202 - val_acc: 0.9046\n",
      "Epoch 163/550\n",
      "53098/53098 [==============================] - 10s 186us/step - loss: 0.0070 - acc: 0.9976 - val_loss: 0.6194 - val_acc: 0.9001\n",
      "Epoch 164/550\n",
      "53098/53098 [==============================] - 10s 190us/step - loss: 0.0079 - acc: 0.9969 - val_loss: 0.6653 - val_acc: 0.9010\n",
      "Epoch 165/550\n",
      "53098/53098 [==============================] - 10s 185us/step - loss: 0.0067 - acc: 0.9978 - val_loss: 0.7695 - val_acc: 0.8903\n",
      "Epoch 166/550\n",
      "53098/53098 [==============================] - 10s 187us/step - loss: 0.0082 - acc: 0.9974 - val_loss: 0.5731 - val_acc: 0.9069\n",
      "Epoch 167/550\n",
      "53098/53098 [==============================] - 10s 184us/step - loss: 0.0078 - acc: 0.9975 - val_loss: 0.6176 - val_acc: 0.9054\n",
      "Epoch 168/550\n",
      "53098/53098 [==============================] - 10s 191us/step - loss: 0.0078 - acc: 0.9974 - val_loss: 0.7269 - val_acc: 0.8994\n",
      "Epoch 169/550\n",
      "53098/53098 [==============================] - 10s 190us/step - loss: 0.0070 - acc: 0.9978 - val_loss: 0.5803 - val_acc: 0.9042\n",
      "Epoch 170/550\n",
      "53098/53098 [==============================] - 10s 184us/step - loss: 0.0103 - acc: 0.9968 - val_loss: 0.5110 - val_acc: 0.9006\n",
      "Epoch 171/550\n",
      "53098/53098 [==============================] - 10s 187us/step - loss: 0.0080 - acc: 0.9971 - val_loss: 0.6738 - val_acc: 0.8996\n",
      "Epoch 172/550\n",
      "53098/53098 [==============================] - 10s 189us/step - loss: 0.0083 - acc: 0.9972 - val_loss: 0.7528 - val_acc: 0.9011\n",
      "Epoch 173/550\n",
      "53098/53098 [==============================] - 10s 184us/step - loss: 0.0061 - acc: 0.9982 - val_loss: 0.6347 - val_acc: 0.9053\n",
      "Epoch 174/550\n",
      "53098/53098 [==============================] - 10s 190us/step - loss: 0.0061 - acc: 0.9978 - val_loss: 0.7208 - val_acc: 0.9028\n",
      "Epoch 175/550\n",
      "53098/53098 [==============================] - 10s 185us/step - loss: 0.0069 - acc: 0.9978 - val_loss: 0.6711 - val_acc: 0.9052\n",
      "Epoch 176/550\n",
      "53098/53098 [==============================] - 10s 186us/step - loss: 0.0107 - acc: 0.9966 - val_loss: 0.5609 - val_acc: 0.8940\n",
      "Epoch 177/550\n",
      "53098/53098 [==============================] - 10s 184us/step - loss: 0.0098 - acc: 0.9969 - val_loss: 0.5991 - val_acc: 0.9012\n",
      "Epoch 178/550\n",
      "53098/53098 [==============================] - 10s 181us/step - loss: 0.0077 - acc: 0.9974 - val_loss: 0.6778 - val_acc: 0.9040\n",
      "Epoch 179/550\n",
      "53098/53098 [==============================] - 10s 183us/step - loss: 0.0054 - acc: 0.9982 - val_loss: 0.6939 - val_acc: 0.8956\n",
      "Epoch 180/550\n",
      "53098/53098 [==============================] - 10s 187us/step - loss: 0.0063 - acc: 0.9978 - val_loss: 0.7593 - val_acc: 0.8999\n",
      "Epoch 181/550\n",
      "53000/53098 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9980"
     ]
    }
   ],
   "source": [
    "\n",
    "tf.get_default_graph()\n",
    "########## HYPER PARAMETERS\n",
    "batch_size = 200\n",
    "epochs = 550\n",
    "optimizer = optimizers.Adam()\n",
    "#optimizer = tf.keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.1)\n",
    "#init=tf.global_variables_initializer()\n",
    "\n",
    "########## HYPER PARAMETERS\n",
    "########## MODEL ARCHITECTURE\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Conv2D(84, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(27,15,1)))\n",
    "model.add(Conv2D(84, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='valid', data_format='channels_last'))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Conv2D(42, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(42, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='valid', data_format='channels_last'))\n",
    "model.add(Dropout(0.08))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(400, activation=('relu'), use_bias=True))\n",
    "model.add(Dense(100, activation=('relu'), use_bias=True))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "########## MODEL ARCHITECTURE\n",
    "####TENSORBOARD\n",
    "config = \"\"\n",
    "for layer in model.layers:\n",
    "    config += str(layer.output).split('\\\"')[1].split(\"/\")[0] + str(layer.output_shape) + \"\\n\\n\"\n",
    "#### END TENSORBOARD\n",
    "config += \"batchsize: \" + str(batch_size) + \"\\n\\n\" + \"epochs: \" + str(epochs) + \"\\n\\n\"\n",
    "\n",
    "# Print summary\n",
    "model.summary()\n",
    "readable_timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%d_%H%M%S')\n",
    "tensorflowfolder = \"/srv/share/tensorboardfiles/KnuckleFinger_Jan_\" + readable_timestamp\n",
    "\n",
    "logger = LoggingTensorBoard(settings_str_to_log = config, log_dir=tensorflowfolder, histogram_freq=0, write_graph=True, write_images=True, update_freq = 'epoch')\n",
    "storer = ModelCheckpoint(\"./ModelSnapshots/KnuckleFinger_Jan_\" + readable_timestamp + \".h5\", monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "# compile model for training\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train_one_hot,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test_one_hot),\n",
    "                    callbacks=[logger, storer, tg_callback])\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use model for inference to get test accuracy\n",
    "y_test_pred = model.predict(x_test)\n",
    "y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "\n",
    "print ('\\n Summary of the precision, recall, F1 score for each class:')\n",
    "print (sklearn.metrics.classification_report(y_test, y_test_pred))\n",
    "\n",
    "print ('\\n Confusion matrix: ')\n",
    "print (sklearn.metrics.confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['val_acc'], label=\"Test Accuracy\")\n",
    "plt.plot(history.history['acc'], label=\"Training Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save(\"2312.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
