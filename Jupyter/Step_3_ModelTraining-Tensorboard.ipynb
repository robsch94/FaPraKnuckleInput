{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import *\n",
    "from keras import optimizers\n",
    "from keras import utils\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Importing matplotlib to plot images.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Importing SK-learn to calculate precision and recall\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, LeaveOneGroupOut\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "# Used for graph export\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.framework import graph_io\n",
    "from keras import backend as K\n",
    "\n",
    "import pickle as pkl\n",
    "import h5py\n",
    "\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import telegram\\nfrom keras.callbacks import Callback\\nfrom callbacks import TelegramCallback\\nfrom callbacks.TelegramData import TelegramData\\n\\n\\n# create callback\\nconfig = {\\n    'token': TelegramData.Token,   # paste your bot token\\n    'telegram_id': TelegramData.ID,                                   # paste your telegram_id\\n}\\n\\ntg_callback = TelegramCallback(config)\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import telegram\n",
    "from keras.callbacks import Callback\n",
    "from callbacks import TelegramCallback\n",
    "from callbacks.TelegramData import TelegramData\n",
    "\n",
    "\n",
    "# create callback\n",
    "config = {\n",
    "    'token': TelegramData.Token,   # paste your bot token\n",
    "    'telegram_id': TelegramData.ID,                                   # paste your telegram_id\n",
    "}\n",
    "\n",
    "tg_callback = TelegramCallback(config)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggingTensorBoard(TensorBoard):    \n",
    "\n",
    "    def __init__(self, log_dir, settings_str_to_log, **kwargs):\n",
    "        super(LoggingTensorBoard, self).__init__(log_dir, **kwargs)\n",
    "\n",
    "        self.settings_str = settings_str_to_log\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        TensorBoard.on_train_begin(self, logs=logs)\n",
    "\n",
    "        tensor =  tf.convert_to_tensor(self.settings_str)\n",
    "        summary = tf.summary.text (\"Run_Settings\", tensor)\n",
    "\n",
    "        with  tf.Session() as sess:\n",
    "            s = sess.run(summary)\n",
    "            self.writer.add_summary(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [1,2,3, 7, 8, 9, 10]\n",
    "test_ids = [4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['userID', 'TaskID', 'Version', 'Image', 'InputMethod'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAll.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, split between train and test sets\n",
    "dfAll = pd.read_pickle(\"PklData/df_blobs.pkl\")\n",
    "#df_train = dfAll[(dfAll.userID != 1) | (dfAll.userID != 2)]\n",
    "#df_test = dfAll[(dfAll.userID == 1) | (dfAll.userID == 2)]\n",
    "df_train = dfAll[dfAll.userID.isin(train_ids)]\n",
    "df_test = dfAll[dfAll.userID.isin(test_ids) & (dfAll.Version == \"Normal\")]\n",
    "\n",
    "df_test = df_test.reset_index()\n",
    "df_train = df_train.reset_index()\n",
    "#Create InputMethod Column and fill it with Knuckel / Finger\n",
    "\"\"\"def f(row):\n",
    "    if row['TaskID'] < 17:\n",
    "        #val = \"Knuckle\"\n",
    "        val = 0\n",
    "    elif row['TaskID'] >= 17:\n",
    "        #val = \"Finger\"\n",
    "        val = 1\n",
    "    return val\n",
    "df_train['InputMethod'] = df_train.apply(f, axis=1)\n",
    "df_test['InputMethod'] = df_test.apply(f, axis=1)\"\"\"\n",
    "df_train2 = df_train[['Blobs', 'InputMethod']].copy()\n",
    "df_test2 = df_test[['Blobs', 'InputMethod']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.vstack(df_train2.Blobs)\n",
    "x_test = np.vstack(df_test2.Blobs)\n",
    "y_train = df_train2.InputMethod.values\n",
    "y_test = df_test2.InputMethod.values\n",
    "\n",
    "x_train = x_train.reshape(-1, 27, 15, 1)\n",
    "x_test = x_test.reshape(-1, 27, 15, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices (one-hot notation)\n",
    "num_classes = 2\n",
    "y_train_one_hot = utils.to_categorical(df_train2.InputMethod, num_classes)\n",
    "y_test_one_hot = utils.to_categorical(df_test2.InputMethod, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Label for image 1 is: 0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 1\n",
    "plt.imshow(x_train[i].reshape(27, 15)) #np.sqrt(784) = 28\n",
    "plt.title(\"Label for image %i is: %s\" % (i, y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If GPU is not available: \n",
    "# GPU_USE = '/cpu:0'\n",
    "# config = tf.ConfigProto(device_count = {\"GPU\": 0})\n",
    "\n",
    "\n",
    "# If GPU is available: \n",
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = True\n",
    "config.allow_soft_placement = True\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "\n",
    "# Limit the maximum memory used\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.1\n",
    "\n",
    "# set session config\n",
    "tf.keras.backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 27, 15, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 27, 15, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 27, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 8, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 14, 8, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 14, 8, 32)         18464     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 14, 8, 32)         9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 14, 8, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 7, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               229632    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 328,450\n",
      "Trainable params: 328,258\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "KnuckleFinger_Jan_20190109_103953\n",
      "Train on 205448 samples, validate on 32544 samples\n",
      "Epoch 1/500\n",
      "205448/205448 [==============================] - 20s 96us/step - loss: 21.6921 - acc: 0.8030 - val_loss: 0.9049 - val_acc: 0.9004\n",
      "Epoch 2/500\n",
      "205448/205448 [==============================] - 19s 92us/step - loss: 0.9390 - acc: 0.8756 - val_loss: 0.9163 - val_acc: 0.8908\n",
      "Epoch 3/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.9088 - acc: 0.8911 - val_loss: 0.8735 - val_acc: 0.9103\n",
      "Epoch 4/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.8917 - acc: 0.8984 - val_loss: 0.8553 - val_acc: 0.9166\n",
      "Epoch 5/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.8773 - acc: 0.9058 - val_loss: 0.8448 - val_acc: 0.9220\n",
      "Epoch 6/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.8682 - acc: 0.9091 - val_loss: 0.8766 - val_acc: 0.9050\n",
      "Epoch 7/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.8631 - acc: 0.9108 - val_loss: 0.8421 - val_acc: 0.9225\n",
      "Epoch 8/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.8555 - acc: 0.9136 - val_loss: 0.8612 - val_acc: 0.9168\n",
      "Epoch 9/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.8507 - acc: 0.9159 - val_loss: 0.8335 - val_acc: 0.9254\n",
      "Epoch 10/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.8470 - acc: 0.9177 - val_loss: 0.8420 - val_acc: 0.9187\n",
      "Epoch 11/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.8437 - acc: 0.9193 - val_loss: 0.8240 - val_acc: 0.9305\n",
      "Epoch 12/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.8400 - acc: 0.9207 - val_loss: 0.8427 - val_acc: 0.9217\n",
      "Epoch 13/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.8372 - acc: 0.9225 - val_loss: 0.8377 - val_acc: 0.9250\n",
      "Epoch 14/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.8358 - acc: 0.9227 - val_loss: 0.8191 - val_acc: 0.9317\n",
      "Epoch 15/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.8332 - acc: 0.9242 - val_loss: 0.8300 - val_acc: 0.9283\n",
      "Epoch 16/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.8301 - acc: 0.9253 - val_loss: 0.8369 - val_acc: 0.9254\n",
      "Epoch 17/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.8279 - acc: 0.9265 - val_loss: 0.8531 - val_acc: 0.9161\n",
      "Epoch 18/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.8257 - acc: 0.9274 - val_loss: 0.8096 - val_acc: 0.9361\n",
      "Epoch 19/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.8249 - acc: 0.9282 - val_loss: 0.8272 - val_acc: 0.9288\n",
      "Epoch 20/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.8228 - acc: 0.9289 - val_loss: 0.8123 - val_acc: 0.9347\n",
      "Epoch 21/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.8208 - acc: 0.9295 - val_loss: 0.8326 - val_acc: 0.9277\n",
      "Epoch 22/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.8195 - acc: 0.9303 - val_loss: 0.8114 - val_acc: 0.9342\n",
      "Epoch 23/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.8184 - acc: 0.9308 - val_loss: 0.8219 - val_acc: 0.9284\n",
      "Epoch 24/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.8177 - acc: 0.9314 - val_loss: 0.8302 - val_acc: 0.9268\n",
      "Epoch 25/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.8154 - acc: 0.9325 - val_loss: 0.8351 - val_acc: 0.9242\n",
      "Epoch 26/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.8149 - acc: 0.9330 - val_loss: 0.8226 - val_acc: 0.9299\n",
      "Epoch 27/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.8145 - acc: 0.9327 - val_loss: 0.8239 - val_acc: 0.9324\n",
      "Epoch 28/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.8117 - acc: 0.9343 - val_loss: 0.8503 - val_acc: 0.9206\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "Epoch 29/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.7436 - acc: 0.9358 - val_loss: 0.7392 - val_acc: 0.9389\n",
      "Epoch 30/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.7432 - acc: 0.9359 - val_loss: 0.7560 - val_acc: 0.9330\n",
      "Epoch 31/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.7431 - acc: 0.9364 - val_loss: 0.7612 - val_acc: 0.9318\n",
      "Epoch 32/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.7419 - acc: 0.9368 - val_loss: 0.7507 - val_acc: 0.9335\n",
      "Epoch 33/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.7407 - acc: 0.9372 - val_loss: 0.7569 - val_acc: 0.9314\n",
      "Epoch 34/500\n",
      "205448/205448 [==============================] - 19s 94us/step - loss: 0.7395 - acc: 0.9380 - val_loss: 0.7587 - val_acc: 0.9296\n",
      "Epoch 35/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.7389 - acc: 0.9383 - val_loss: 0.7532 - val_acc: 0.9346\n",
      "Epoch 36/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.7375 - acc: 0.9387 - val_loss: 0.7610 - val_acc: 0.9299\n",
      "Epoch 37/500\n",
      "205448/205448 [==============================] - 19s 94us/step - loss: 0.7381 - acc: 0.9381 - val_loss: 0.7472 - val_acc: 0.9341\n",
      "Epoch 38/500\n",
      "205448/205448 [==============================] - 19s 94us/step - loss: 0.7361 - acc: 0.9396 - val_loss: 0.7821 - val_acc: 0.9244\n",
      "Epoch 39/500\n",
      "205448/205448 [==============================] - 19s 94us/step - loss: 0.7353 - acc: 0.9391 - val_loss: 0.7633 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0008100000384729356.\n",
      "Epoch 40/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.6757 - acc: 0.9403 - val_loss: 0.6892 - val_acc: 0.9361\n",
      "Epoch 41/500\n",
      "205448/205448 [==============================] - 19s 94us/step - loss: 0.6774 - acc: 0.9403 - val_loss: 0.6982 - val_acc: 0.9335\n",
      "Epoch 42/500\n",
      "205448/205448 [==============================] - 19s 94us/step - loss: 0.6762 - acc: 0.9409 - val_loss: 0.7154 - val_acc: 0.9274\n",
      "Epoch 43/500\n",
      "205448/205448 [==============================] - 19s 94us/step - loss: 0.6743 - acc: 0.9418 - val_loss: 0.6874 - val_acc: 0.9387\n",
      "Epoch 44/500\n",
      "205448/205448 [==============================] - 19s 94us/step - loss: 0.6741 - acc: 0.9419 - val_loss: 0.6985 - val_acc: 0.9330\n",
      "Epoch 45/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.6727 - acc: 0.9427 - val_loss: 0.6931 - val_acc: 0.9346\n",
      "Epoch 46/500\n",
      "205448/205448 [==============================] - 19s 94us/step - loss: 0.6720 - acc: 0.9429 - val_loss: 0.7059 - val_acc: 0.9315\n",
      "Epoch 47/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.6718 - acc: 0.9425 - val_loss: 0.7052 - val_acc: 0.9299\n",
      "Epoch 48/500\n",
      "205448/205448 [==============================] - 20s 97us/step - loss: 0.6708 - acc: 0.9432 - val_loss: 0.6923 - val_acc: 0.9338\n",
      "Epoch 49/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.6700 - acc: 0.9435 - val_loss: 0.6979 - val_acc: 0.9335\n",
      "Epoch 50/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.6701 - acc: 0.9433 - val_loss: 0.7024 - val_acc: 0.9315\n",
      "Epoch 51/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.6704 - acc: 0.9433 - val_loss: 0.7120 - val_acc: 0.9302\n",
      "Epoch 52/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.6685 - acc: 0.9441 - val_loss: 0.6964 - val_acc: 0.9311\n",
      "Epoch 53/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.6688 - acc: 0.9436 - val_loss: 0.6970 - val_acc: 0.9343\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.0007290000503417104.\n",
      "Epoch 54/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.6154 - acc: 0.9448 - val_loss: 0.6598 - val_acc: 0.9311\n",
      "Epoch 55/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.6152 - acc: 0.9459 - val_loss: 0.6519 - val_acc: 0.9331\n",
      "Epoch 56/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.6145 - acc: 0.9456 - val_loss: 0.6510 - val_acc: 0.9338\n",
      "Epoch 57/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.6143 - acc: 0.9457 - val_loss: 0.6463 - val_acc: 0.9318\n",
      "Epoch 58/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.6135 - acc: 0.9461 - val_loss: 0.6444 - val_acc: 0.9307\n",
      "Epoch 59/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.6128 - acc: 0.9461 - val_loss: 0.6372 - val_acc: 0.9377\n",
      "Epoch 60/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.6120 - acc: 0.9461 - val_loss: 0.6410 - val_acc: 0.9362\n",
      "Epoch 61/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.6120 - acc: 0.9465 - val_loss: 0.6292 - val_acc: 0.9382\n",
      "Epoch 62/500\n",
      "205448/205448 [==============================] - 19s 92us/step - loss: 0.6106 - acc: 0.9471 - val_loss: 0.6389 - val_acc: 0.9349\n",
      "Epoch 63/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.6120 - acc: 0.9465 - val_loss: 0.6420 - val_acc: 0.9363\n",
      "Epoch 64/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.6097 - acc: 0.9473 - val_loss: 0.6342 - val_acc: 0.9385\n",
      "Epoch 65/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.6088 - acc: 0.9481 - val_loss: 0.6541 - val_acc: 0.9326\n",
      "Epoch 66/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.6101 - acc: 0.9472 - val_loss: 0.6394 - val_acc: 0.9371\n",
      "Epoch 67/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.6092 - acc: 0.9472 - val_loss: 0.6437 - val_acc: 0.9357\n",
      "Epoch 68/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.6086 - acc: 0.9479 - val_loss: 0.6420 - val_acc: 0.9355\n",
      "Epoch 69/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.6082 - acc: 0.9480 - val_loss: 0.6309 - val_acc: 0.9381\n",
      "Epoch 70/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.6074 - acc: 0.9478 - val_loss: 0.6585 - val_acc: 0.9339\n",
      "Epoch 71/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.6073 - acc: 0.9487 - val_loss: 0.6447 - val_acc: 0.9351\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 0.0006561000715009868.\n",
      "Epoch 72/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.5603 - acc: 0.9488 - val_loss: 0.6195 - val_acc: 0.9308\n",
      "Epoch 73/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.5586 - acc: 0.9494 - val_loss: 0.6013 - val_acc: 0.9350\n",
      "Epoch 74/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.5580 - acc: 0.9500 - val_loss: 0.5880 - val_acc: 0.9385\n",
      "Epoch 75/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.5598 - acc: 0.9486 - val_loss: 0.6026 - val_acc: 0.9344\n",
      "Epoch 76/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.5582 - acc: 0.9496 - val_loss: 0.6055 - val_acc: 0.9336\n",
      "Epoch 77/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.5590 - acc: 0.9491 - val_loss: 0.5946 - val_acc: 0.9363\n",
      "Epoch 78/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.5587 - acc: 0.9499 - val_loss: 0.5941 - val_acc: 0.9360\n",
      "Epoch 79/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.5579 - acc: 0.9505 - val_loss: 0.6005 - val_acc: 0.9355\n",
      "Epoch 80/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.5547 - acc: 0.9504 - val_loss: 0.5885 - val_acc: 0.9383\n",
      "Epoch 81/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.5562 - acc: 0.9496 - val_loss: 0.5968 - val_acc: 0.9355\n",
      "Epoch 82/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.5562 - acc: 0.9500 - val_loss: 0.5958 - val_acc: 0.9355\n",
      "Epoch 83/500\n",
      "205448/205448 [==============================] - 19s 92us/step - loss: 0.5554 - acc: 0.9504 - val_loss: 0.5955 - val_acc: 0.9357\n",
      "Epoch 84/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.5540 - acc: 0.9510 - val_loss: 0.6052 - val_acc: 0.9340\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 0.0005904900433961303.\n",
      "Epoch 85/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.5112 - acc: 0.9521 - val_loss: 0.5476 - val_acc: 0.9395\n",
      "Epoch 86/500\n",
      "205448/205448 [==============================] - 19s 93us/step - loss: 0.5118 - acc: 0.9518 - val_loss: 0.5696 - val_acc: 0.9326\n",
      "Epoch 87/500\n",
      "205448/205448 [==============================] - 19s 92us/step - loss: 0.5125 - acc: 0.9513 - val_loss: 0.5510 - val_acc: 0.9361\n",
      "Epoch 88/500\n",
      "205448/205448 [==============================] - 19s 92us/step - loss: 0.5113 - acc: 0.9518 - val_loss: 0.5843 - val_acc: 0.9306\n",
      "Epoch 89/500\n",
      "205448/205448 [==============================] - 19s 95us/step - loss: 0.5116 - acc: 0.9522 - val_loss: 0.5517 - val_acc: 0.9371\n",
      "Epoch 90/500\n",
      "205448/205448 [==============================] - 19s 91us/step - loss: 0.5111 - acc: 0.9520 - val_loss: 0.5577 - val_acc: 0.9349\n",
      "Epoch 91/500\n",
      "205448/205448 [==============================] - 19s 91us/step - loss: 0.5099 - acc: 0.9527 - val_loss: 0.5474 - val_acc: 0.9388\n",
      "Epoch 92/500\n",
      "205448/205448 [==============================] - 19s 91us/step - loss: 0.5111 - acc: 0.9520 - val_loss: 0.5762 - val_acc: 0.9320\n",
      "Epoch 93/500\n",
      "205448/205448 [==============================] - 19s 91us/step - loss: 0.5102 - acc: 0.9521 - val_loss: 0.5582 - val_acc: 0.9364\n",
      "Epoch 94/500\n",
      "205448/205448 [==============================] - 19s 92us/step - loss: 0.5095 - acc: 0.9525 - val_loss: 0.5537 - val_acc: 0.9376\n",
      "Epoch 95/500\n",
      "205448/205448 [==============================] - 19s 92us/step - loss: 0.5098 - acc: 0.9528 - val_loss: 0.5458 - val_acc: 0.9394\n",
      "Epoch 96/500\n",
      "205448/205448 [==============================] - 19s 92us/step - loss: 0.5098 - acc: 0.9526 - val_loss: 0.5607 - val_acc: 0.9351\n",
      "Epoch 97/500\n",
      "205448/205448 [==============================] - 19s 91us/step - loss: 0.5084 - acc: 0.9526 - val_loss: 0.5542 - val_acc: 0.9391\n",
      "Epoch 98/500\n",
      "205448/205448 [==============================] - 19s 91us/step - loss: 0.5085 - acc: 0.9526 - val_loss: 0.5598 - val_acc: 0.9331\n",
      "Epoch 99/500\n",
      "205448/205448 [==============================] - 19s 91us/step - loss: 0.5071 - acc: 0.9536 - val_loss: 0.5545 - val_acc: 0.9361\n",
      "Epoch 100/500\n",
      "205448/205448 [==============================] - 19s 91us/step - loss: 0.5077 - acc: 0.9531 - val_loss: 0.5528 - val_acc: 0.9354\n",
      "Epoch 101/500\n",
      "205448/205448 [==============================] - 19s 91us/step - loss: 0.5071 - acc: 0.9531 - val_loss: 0.5471 - val_acc: 0.9387\n",
      "Epoch 102/500\n",
      " 12000/205448 [>.............................] - ETA: 17s - loss: 0.5043 - acc: 0.9546"
     ]
    }
   ],
   "source": [
    "tf.get_default_graph()\n",
    "########## HYPER PARAMETERS\n",
    "\n",
    "epochs = 500\n",
    "optimizer = optimizers.Adam(lr=0.001)\n",
    "#optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.1)\n",
    "#init=tf.global_variables_initializer()\n",
    "\n",
    "########## HYPER PARAMETERS\n",
    "########## MODEL ARCHITECTURE\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(27,15,1)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='same', data_format='channels_last'))\n",
    "model.add(Dropout(0.50))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='same', data_format='channels_last'))\n",
    "model.add(Dropout(0.50))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=keras.regularizers.L1L2(0.02, 0.15), use_bias=True))\n",
    "#model.add(Dropout(0.50))\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=keras.regularizers.L1L2(0.02, 0.15), use_bias=True))\n",
    "#model.add(Dropout(0.50))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "########## MODEL ARCHITECTURE\n",
    "####TENSORBOARD\n",
    "config = \"\"\n",
    "for layer in model.layers:\n",
    "    config += str(layer.output).split('\\\"')[1].split(\"/\")[0] + str(layer.output_shape) + \"\\n\\n\"\n",
    "#### END TENSORBOARD\n",
    "config += \"batchsize: \" + str(batch_size) + \"\\n\\n\" + \"epochs: \" + str(epochs) + \"\\n\\n\"\n",
    "\n",
    "# Print summary\n",
    "model.summary()\n",
    "readable_timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%d_%H%M%S')\n",
    "tensorflowfolder = \"/srv/share/tensorboardfiles/KnuckleFinger_Jan_\" + readable_timestamp\n",
    "print(\"KnuckleFinger_Jan_\" + readable_timestamp)\n",
    "logger = LoggingTensorBoard(settings_str_to_log = config, log_dir=tensorflowfolder, histogram_freq=0,\n",
    "                            write_graph=True, write_images=True, update_freq = 'epoch')\n",
    "storer = ModelCheckpoint(\"./ModelSnapshots/KnuckleFinger_Jan_\" + readable_timestamp + \".h5\", monitor='val_loss', verbose=0,\n",
    "                         save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=10, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.9, \n",
    "                                            min_lr=0.00001)\n",
    "# compile model for training\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train_one_hot,\n",
    "                    batch_size=500,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test_one_hot),\n",
    "                    callbacks=[logger, storer, learning_rate_reduction])\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model():\n",
    "    optimizer = optimizers.Adam()\n",
    "    #optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.1)\n",
    "    #init=tf.global_variables_initializer()\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(120, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(27,15,1)))\n",
    "    model.add(Conv2D(120, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='valid', data_format='channels_last'))\n",
    "    model.add(Dropout(0.15))\n",
    "    model.add(Conv2D(50, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(50, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='valid', data_format='channels_last'))\n",
    "    model.add(Dropout(0.10))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(400, activation=('relu'), use_bias=True))\n",
    "    model.add(Dense(100, activation=('relu'), use_bias=True))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    ####TENSORBOARD\n",
    "    config = \"\"\n",
    "    for layer in model.layers:\n",
    "        config += str(layer.output).split('\\\"')[1].split(\"/\")[0] + str(layer.output_shape) + \"\\n\\n\"\n",
    "    config += \"batchsize: \" + str(batch_size) + \"\\n\\n\" + \"epochs: \" + str(epochs) + \"\\n\\n\"\n",
    "    #### END TENSORBOARD\n",
    "     \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_default_graph()\n",
    "\n",
    "readable_timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%d_%H%M%S')\n",
    "tensorflowfolder = \"/srv/share/tensorboardfiles/KnuckleFinger_Jan_\" + readable_timestamp\n",
    "\n",
    "#set early stopping criteria\n",
    "pat = 5 #this is the number of epochs with no improvment after which the training will stop\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=pat, verbose=1)\n",
    "logger = LoggingTensorBoard(settings_str_to_log = config, log_dir=tensorflowfolder, histogram_freq=0, write_graph=True, write_images=True, update_freq = 'epoch')\n",
    "model_checkpoint = ModelCheckpoint(\"./ModelSnapshots/KnuckleFinger_Jan_\" + readable_timestamp + \".h5\", monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "\n",
    "def fit_and_evaluate(t_x, val_x, t_y, val_y, EPOCHS=500, BATCH_SIZE=200):\n",
    "    model = None\n",
    "    model = cnn_model()\n",
    "    model.summary()\n",
    "    history = model.fit(x_train, y_train_one_hot,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test_one_hot),\n",
    "                    callbacks=[logger, model_checkpoint, tg_callback])\n",
    "    \n",
    "    print(\"Val Score: \", model.evaluate(val_x, val_y))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds=3\n",
    "epochs=500\n",
    "batch_size=200\n",
    "\n",
    "#save the model history in a list after fitting so that we can plot later\n",
    "model_history = [] \n",
    "\n",
    "for i in range(n_folds):\n",
    "    print(\"Training on Fold: \",i+1)\n",
    "    ####TODO\n",
    "    t_x, val_x, t_y, val_y = train_test_split(train_x, train_y, test_size=0.1, random_state = np.random.randint(1,1000, 1)[0])\n",
    "    ####END\n",
    "    model_history.append(fit_and_evaluate(t_x, val_x, t_y, val_y, epochs, batch_size))\n",
    "    print(\"=======\"*12, end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Accuracies vs Epochs')\n",
    "plt.plot(model_history[0].history['acc'], label='Training Fold 1')\n",
    "plt.plot(model_history[1].history['acc'], label='Training Fold 2')\n",
    "plt.plot(model_history[2].history['acc'], label='Training Fold 3')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use model for inference to get test accuracy\n",
    "y_test_pred = model.predict(x_test)\n",
    "y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "\n",
    "print ('\\n Summary of the precision, recall, F1 score for each class:')\n",
    "print (sklearn.metrics.classification_report(y_test, y_test_pred))\n",
    "\n",
    "print ('\\n Confusion matrix: ')\n",
    "print (sklearn.metrics.confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['val_acc'], label=\"Test Accuracy\")\n",
    "plt.plot(history.history['acc'], label=\"Training Accuracy\")\n",
    "plt.legend()\n",
    "plt.ylim(0.5,1)\n",
    "plt.savefig(\"pres.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save(\"2312.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
