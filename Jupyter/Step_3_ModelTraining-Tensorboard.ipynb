{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import *\n",
    "from keras import optimizers\n",
    "from keras import utils\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Importing matplotlib to plot images.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Importing SK-learn to calculate precision and recall\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, LeaveOneGroupOut\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "# Used for graph export\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.framework import graph_io\n",
    "from keras import backend as K\n",
    "\n",
    "import pickle as pkl\n",
    "import h5py\n",
    "\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import telegram\n",
    "from keras.callbacks import Callback\n",
    "from callbacks import TelegramCallback\n",
    "from callbacks.TelegramData import TelegramData\n",
    "\n",
    "\n",
    "# create callback\n",
    "config = {\n",
    "    'token': TelegramData.Token,   # paste your bot token\n",
    "    'telegram_id': TelegramData.ID,                                   # paste your telegram_id\n",
    "}\n",
    "\n",
    "tg_callback = TelegramCallback(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggingTensorBoard(TensorBoard):    \n",
    "\n",
    "    def __init__(self, log_dir, settings_str_to_log, **kwargs):\n",
    "        super(LoggingTensorBoard, self).__init__(log_dir, **kwargs)\n",
    "\n",
    "        self.settings_str = settings_str_to_log\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        TensorBoard.on_train_begin(self, logs=logs)\n",
    "\n",
    "        tensor =  tf.convert_to_tensor(self.settings_str)\n",
    "        summary = tf.summary.text (\"Run_Settings\", tensor)\n",
    "\n",
    "        with  tf.Session() as sess:\n",
    "            s = sess.run(summary)\n",
    "            self.writer.add_summary(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [1,2,3, 7, 8, 9, 10]\n",
    "test_ids = [4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, split between train and test sets\n",
    "dfAll = pd.read_pickle(\"PklData/df_blobs.pkl\")\n",
    "#df_train = dfAll[(dfAll.userID != 1) | (dfAll.userID != 2)]\n",
    "#df_test = dfAll[(dfAll.userID == 1) | (dfAll.userID == 2)]\n",
    "df_train = dfAll[dfAll.userID.isin(train_ids)]\n",
    "df_test = dfAll[dfAll.userID.isin(test_ids)]\n",
    "\n",
    "df_test = df_test.reset_index()\n",
    "df_train = df_train.reset_index()\n",
    "#Create InputMethod Column and fill it with Knuckel / Finger\n",
    "def f(row):\n",
    "    if row['TaskID'] < 17:\n",
    "        #val = \"Knuckle\"\n",
    "        val = 0\n",
    "    elif row['TaskID'] >= 17:\n",
    "        #val = \"Finger\"\n",
    "        val = 1\n",
    "    return val\n",
    "df_train['InputMethod'] = df_train.apply(f, axis=1)\n",
    "df_test['InputMethod'] = df_test.apply(f, axis=1)\n",
    "df_train2 = df_train[['Blobs', 'InputMethod']].copy()\n",
    "df_test2 = df_test[['Blobs', 'InputMethod']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.vstack(df_train2.Blobs)\n",
    "x_test = np.vstack(df_test2.Blobs)\n",
    "y_train = df_train2.InputMethod.values\n",
    "y_test = df_test2.InputMethod.values\n",
    "\n",
    "x_train = x_train.reshape(-1, 27, 15, 1)\n",
    "x_test = x_test.reshape(-1, 27, 15, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices (one-hot notation)\n",
    "num_classes = 2\n",
    "y_train_one_hot = utils.to_categorical(df_train2.InputMethod, num_classes)\n",
    "y_test_one_hot = utils.to_categorical(df_test2.InputMethod, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Label for image 1 is: 0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 1\n",
    "plt.imshow(x_train[i].reshape(27, 15)) #np.sqrt(784) = 28\n",
    "plt.title(\"Label for image %i is: %s\" % (i, y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If GPU is not available: \n",
    "# GPU_USE = '/cpu:0'\n",
    "# config = tf.ConfigProto(device_count = {\"GPU\": 0})\n",
    "\n",
    "\n",
    "# If GPU is available: \n",
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = True\n",
    "config.allow_soft_placement = True\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "\n",
    "# Limit the maximum memory used\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.1\n",
    "\n",
    "# set session config\n",
    "tf.keras.backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxPooling2D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_25 (Conv2D)           (None, 27, 15, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 27, 15, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 27, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 14, 8, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 14, 8, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 14, 8, 32)         18464     \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 14, 8, 32)         9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 14, 8, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 7, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 7, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 256)               229632    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 328,450\n",
      "Trainable params: 328,258\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "KnuckleFinger_Jan_20190108_191725\n",
      "Train on 47696 samples, validate on 30086 samples\n",
      "Epoch 1/500\n",
      "47696/47696 [==============================] - 16s 329us/step - loss: 37.6501 - acc: 0.7766 - val_loss: 0.9765 - val_acc: 0.8738\n",
      "Epoch 2/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.9688 - acc: 0.8574 - val_loss: 0.9031 - val_acc: 0.9093\n",
      "Epoch 3/500\n",
      "47696/47696 [==============================] - 15s 309us/step - loss: 0.9451 - acc: 0.8744 - val_loss: 0.9279 - val_acc: 0.8827\n",
      "Epoch 4/500\n",
      "47696/47696 [==============================] - 15s 324us/step - loss: 0.9202 - acc: 0.8867 - val_loss: 0.9034 - val_acc: 0.8980\n",
      "Epoch 5/500\n",
      "47696/47696 [==============================] - 15s 325us/step - loss: 0.9017 - acc: 0.8969 - val_loss: 0.8441 - val_acc: 0.9266\n",
      "Epoch 6/500\n",
      "47696/47696 [==============================] - 15s 324us/step - loss: 0.8920 - acc: 0.9012 - val_loss: 0.9956 - val_acc: 0.8651\n",
      "Epoch 7/500\n",
      "47696/47696 [==============================] - 15s 324us/step - loss: 0.8789 - acc: 0.9057 - val_loss: 0.8868 - val_acc: 0.9116\n",
      "Epoch 8/500\n",
      "47696/47696 [==============================] - 15s 324us/step - loss: 0.8725 - acc: 0.9106 - val_loss: 0.8494 - val_acc: 0.9241\n",
      "Epoch 9/500\n",
      "47696/47696 [==============================] - 15s 325us/step - loss: 0.8621 - acc: 0.9143 - val_loss: 0.9821 - val_acc: 0.8719\n",
      "Epoch 10/500\n",
      "47696/47696 [==============================] - 16s 327us/step - loss: 0.8567 - acc: 0.9156 - val_loss: 0.8264 - val_acc: 0.9311\n",
      "Epoch 11/500\n",
      "47696/47696 [==============================] - 15s 324us/step - loss: 0.8479 - acc: 0.9212 - val_loss: 0.8452 - val_acc: 0.9227\n",
      "Epoch 12/500\n",
      "47696/47696 [==============================] - 15s 324us/step - loss: 0.8447 - acc: 0.9226 - val_loss: 0.8474 - val_acc: 0.9218\n",
      "Epoch 13/500\n",
      "47696/47696 [==============================] - 15s 325us/step - loss: 0.8413 - acc: 0.9234 - val_loss: 0.8208 - val_acc: 0.9343\n",
      "Epoch 14/500\n",
      "47696/47696 [==============================] - 15s 324us/step - loss: 0.8373 - acc: 0.9263 - val_loss: 0.8433 - val_acc: 0.9280\n",
      "Epoch 15/500\n",
      "47696/47696 [==============================] - 15s 325us/step - loss: 0.8313 - acc: 0.9279 - val_loss: 0.8158 - val_acc: 0.9350\n",
      "Epoch 16/500\n",
      "47696/47696 [==============================] - 15s 324us/step - loss: 0.8301 - acc: 0.9285 - val_loss: 0.8673 - val_acc: 0.9147\n",
      "Epoch 17/500\n",
      "47696/47696 [==============================] - 15s 310us/step - loss: 0.8293 - acc: 0.9299 - val_loss: 0.8288 - val_acc: 0.9286\n",
      "Epoch 18/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.8233 - acc: 0.9319 - val_loss: 0.8075 - val_acc: 0.9401\n",
      "Epoch 19/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.8198 - acc: 0.9332 - val_loss: 0.8468 - val_acc: 0.9217\n",
      "Epoch 20/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.8151 - acc: 0.9354 - val_loss: 0.8269 - val_acc: 0.9333\n",
      "Epoch 21/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.8114 - acc: 0.9370 - val_loss: 0.8840 - val_acc: 0.9095\n",
      "Epoch 22/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.8130 - acc: 0.9372 - val_loss: 0.8576 - val_acc: 0.9223\n",
      "Epoch 23/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.8130 - acc: 0.9369 - val_loss: 0.8593 - val_acc: 0.9204\n",
      "Epoch 24/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.8086 - acc: 0.9381 - val_loss: 0.8380 - val_acc: 0.9267\n",
      "Epoch 25/500\n",
      "47696/47696 [==============================] - 15s 306us/step - loss: 0.8113 - acc: 0.9392 - val_loss: 0.8318 - val_acc: 0.9343\n",
      "Epoch 26/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.8088 - acc: 0.9393 - val_loss: 0.8418 - val_acc: 0.9309\n",
      "Epoch 27/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.8036 - acc: 0.9416 - val_loss: 0.8420 - val_acc: 0.9295\n",
      "Epoch 28/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.8022 - acc: 0.9417 - val_loss: 0.8156 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "Epoch 29/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.7343 - acc: 0.9451 - val_loss: 0.7626 - val_acc: 0.9306\n",
      "Epoch 30/500\n",
      "47696/47696 [==============================] - 15s 309us/step - loss: 0.7333 - acc: 0.9433 - val_loss: 0.8008 - val_acc: 0.9206\n",
      "Epoch 31/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.7300 - acc: 0.9442 - val_loss: 0.7500 - val_acc: 0.9386\n",
      "Epoch 32/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.7301 - acc: 0.9457 - val_loss: 0.7881 - val_acc: 0.9292\n",
      "Epoch 33/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.7301 - acc: 0.9456 - val_loss: 0.7771 - val_acc: 0.9338\n",
      "Epoch 34/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.7282 - acc: 0.9477 - val_loss: 0.7692 - val_acc: 0.9295\n",
      "Epoch 35/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.7268 - acc: 0.9465 - val_loss: 0.7841 - val_acc: 0.9291\n",
      "Epoch 36/500\n",
      "47696/47696 [==============================] - 15s 309us/step - loss: 0.7262 - acc: 0.9483 - val_loss: 0.7854 - val_acc: 0.9284\n",
      "Epoch 37/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.7239 - acc: 0.9485 - val_loss: 0.7921 - val_acc: 0.9230\n",
      "Epoch 38/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.7232 - acc: 0.9491 - val_loss: 0.7607 - val_acc: 0.9354\n",
      "Epoch 39/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.7204 - acc: 0.9495 - val_loss: 0.7492 - val_acc: 0.9422\n",
      "Epoch 40/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.7247 - acc: 0.9493 - val_loss: 0.7420 - val_acc: 0.9418\n",
      "Epoch 41/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.7178 - acc: 0.9501 - val_loss: 0.7962 - val_acc: 0.9283\n",
      "Epoch 42/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.7212 - acc: 0.9508 - val_loss: 0.7900 - val_acc: 0.9284\n",
      "Epoch 43/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.7169 - acc: 0.9515 - val_loss: 0.7631 - val_acc: 0.9395\n",
      "Epoch 44/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.7163 - acc: 0.9523 - val_loss: 0.7773 - val_acc: 0.9367\n",
      "Epoch 45/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.7158 - acc: 0.9519 - val_loss: 0.7833 - val_acc: 0.9309\n",
      "Epoch 46/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.7149 - acc: 0.9532 - val_loss: 0.7932 - val_acc: 0.9291\n",
      "Epoch 47/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.7135 - acc: 0.9544 - val_loss: 0.7647 - val_acc: 0.9369\n",
      "Epoch 48/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.7137 - acc: 0.9537 - val_loss: 0.7555 - val_acc: 0.9364\n",
      "Epoch 49/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.7105 - acc: 0.9543 - val_loss: 0.8095 - val_acc: 0.9218\n",
      "Epoch 50/500\n",
      "47696/47696 [==============================] - 15s 309us/step - loss: 0.7122 - acc: 0.9531 - val_loss: 0.7656 - val_acc: 0.9365\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0008100000384729356.\n",
      "Epoch 51/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.6518 - acc: 0.9544 - val_loss: 0.6997 - val_acc: 0.9369\n",
      "Epoch 52/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.6482 - acc: 0.9559 - val_loss: 0.7387 - val_acc: 0.9280\n",
      "Epoch 53/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.6459 - acc: 0.9569 - val_loss: 0.7871 - val_acc: 0.9168\n",
      "Epoch 54/500\n",
      "47696/47696 [==============================] - 15s 320us/step - loss: 0.6454 - acc: 0.9573 - val_loss: 0.7119 - val_acc: 0.9362\n",
      "Epoch 55/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.6468 - acc: 0.9573 - val_loss: 0.7260 - val_acc: 0.9335\n",
      "Epoch 56/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.6491 - acc: 0.9574 - val_loss: 0.7739 - val_acc: 0.9183\n",
      "Epoch 57/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.6463 - acc: 0.9572 - val_loss: 0.7282 - val_acc: 0.9325\n",
      "Epoch 58/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.6442 - acc: 0.9574 - val_loss: 0.7283 - val_acc: 0.9318\n",
      "Epoch 59/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.6429 - acc: 0.9582 - val_loss: 0.7350 - val_acc: 0.9311\n",
      "Epoch 60/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.6423 - acc: 0.9580 - val_loss: 0.7116 - val_acc: 0.9379\n",
      "Epoch 61/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.6430 - acc: 0.9587 - val_loss: 0.7182 - val_acc: 0.9295\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.0007290000503417104.\n",
      "Epoch 62/500\n",
      "47696/47696 [==============================] - 15s 321us/step - loss: 0.5888 - acc: 0.9600 - val_loss: 0.6692 - val_acc: 0.9339\n",
      "Epoch 63/500\n",
      "47696/47696 [==============================] - 15s 309us/step - loss: 0.5868 - acc: 0.9607 - val_loss: 0.6918 - val_acc: 0.9295\n",
      "Epoch 64/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.5866 - acc: 0.9607 - val_loss: 0.6540 - val_acc: 0.9402\n",
      "Epoch 65/500\n",
      "47696/47696 [==============================] - 15s 306us/step - loss: 0.5888 - acc: 0.9600 - val_loss: 0.7078 - val_acc: 0.9274\n",
      "Epoch 66/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.5873 - acc: 0.9606 - val_loss: 0.6851 - val_acc: 0.9339\n",
      "Epoch 67/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.5857 - acc: 0.9616 - val_loss: 0.7136 - val_acc: 0.9266\n",
      "Epoch 68/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.5852 - acc: 0.9612 - val_loss: 0.6833 - val_acc: 0.9311\n",
      "Epoch 69/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.5817 - acc: 0.9637 - val_loss: 0.7084 - val_acc: 0.9245\n",
      "Epoch 70/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.5873 - acc: 0.9608 - val_loss: 0.6841 - val_acc: 0.9316\n",
      "Epoch 71/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.5855 - acc: 0.9613 - val_loss: 0.6938 - val_acc: 0.9292\n",
      "Epoch 72/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.5835 - acc: 0.9621 - val_loss: 0.6913 - val_acc: 0.9270\n",
      "Epoch 73/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.5826 - acc: 0.9619 - val_loss: 0.6632 - val_acc: 0.9366\n",
      "Epoch 74/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.5805 - acc: 0.9634 - val_loss: 0.6780 - val_acc: 0.9320\n",
      "\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 0.0006561000715009868.\n",
      "Epoch 75/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.5304 - acc: 0.9642 - val_loss: 0.6283 - val_acc: 0.9372\n",
      "Epoch 76/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.5308 - acc: 0.9633 - val_loss: 0.6233 - val_acc: 0.9367\n",
      "Epoch 77/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.5323 - acc: 0.9638 - val_loss: 0.6516 - val_acc: 0.9308\n",
      "Epoch 78/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.5326 - acc: 0.9641 - val_loss: 0.6272 - val_acc: 0.9371\n",
      "Epoch 79/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.5313 - acc: 0.9630 - val_loss: 0.6665 - val_acc: 0.9287\n",
      "Epoch 80/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.5282 - acc: 0.9644 - val_loss: 0.6362 - val_acc: 0.9347\n",
      "Epoch 81/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.5249 - acc: 0.9668 - val_loss: 0.6450 - val_acc: 0.9320\n",
      "Epoch 82/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.5277 - acc: 0.9651 - val_loss: 0.6566 - val_acc: 0.9281\n",
      "Epoch 83/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.5297 - acc: 0.9646 - val_loss: 0.6458 - val_acc: 0.9282\n",
      "Epoch 84/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.5258 - acc: 0.9656 - val_loss: 0.6665 - val_acc: 0.9272\n",
      "Epoch 85/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.5272 - acc: 0.9652 - val_loss: 0.6345 - val_acc: 0.9319\n",
      "Epoch 86/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.5260 - acc: 0.9654 - val_loss: 0.6249 - val_acc: 0.9361\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 0.0005904900433961303.\n",
      "Epoch 87/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.4835 - acc: 0.9670 - val_loss: 0.5847 - val_acc: 0.9361\n",
      "Epoch 88/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.4833 - acc: 0.9667 - val_loss: 0.6186 - val_acc: 0.9346\n",
      "Epoch 89/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.4823 - acc: 0.9676 - val_loss: 0.5980 - val_acc: 0.9375\n",
      "Epoch 90/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.4828 - acc: 0.9675 - val_loss: 0.5818 - val_acc: 0.9384\n",
      "Epoch 91/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.4830 - acc: 0.9665 - val_loss: 0.5652 - val_acc: 0.9413\n",
      "Epoch 92/500\n",
      "47696/47696 [==============================] - 15s 309us/step - loss: 0.4812 - acc: 0.9678 - val_loss: 0.6085 - val_acc: 0.9311\n",
      "Epoch 93/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.4821 - acc: 0.9678 - val_loss: 0.5901 - val_acc: 0.9379\n",
      "Epoch 94/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.4843 - acc: 0.9660 - val_loss: 0.6267 - val_acc: 0.9276\n",
      "Epoch 95/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.4827 - acc: 0.9674 - val_loss: 0.6247 - val_acc: 0.9271\n",
      "Epoch 96/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.4814 - acc: 0.9676 - val_loss: 0.6062 - val_acc: 0.9326\n",
      "Epoch 97/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.4799 - acc: 0.9682 - val_loss: 0.6582 - val_acc: 0.9202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.4779 - acc: 0.9692 - val_loss: 0.5864 - val_acc: 0.9378\n",
      "Epoch 99/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.4781 - acc: 0.9690 - val_loss: 0.6032 - val_acc: 0.9320\n",
      "Epoch 100/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.4792 - acc: 0.9682 - val_loss: 0.6194 - val_acc: 0.9313\n",
      "Epoch 101/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.4769 - acc: 0.9696 - val_loss: 0.5870 - val_acc: 0.9381\n",
      "\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 0.0005314410547725857.\n",
      "Epoch 102/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.4396 - acc: 0.9692 - val_loss: 0.5407 - val_acc: 0.9394\n",
      "Epoch 103/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.4375 - acc: 0.9696 - val_loss: 0.5910 - val_acc: 0.9277\n",
      "Epoch 104/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.4407 - acc: 0.9695 - val_loss: 0.5791 - val_acc: 0.9326\n",
      "Epoch 105/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.4383 - acc: 0.9690 - val_loss: 0.5869 - val_acc: 0.9294\n",
      "Epoch 106/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.4373 - acc: 0.9699 - val_loss: 0.5755 - val_acc: 0.9367\n",
      "Epoch 107/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.4362 - acc: 0.9699 - val_loss: 0.5516 - val_acc: 0.9389\n",
      "Epoch 108/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.4370 - acc: 0.9701 - val_loss: 0.5594 - val_acc: 0.9344\n",
      "Epoch 109/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.4348 - acc: 0.9709 - val_loss: 0.5513 - val_acc: 0.9392\n",
      "Epoch 110/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.4354 - acc: 0.9704 - val_loss: 0.5530 - val_acc: 0.9354\n",
      "Epoch 111/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.4334 - acc: 0.9710 - val_loss: 0.5636 - val_acc: 0.9363\n",
      "Epoch 112/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.4339 - acc: 0.9706 - val_loss: 0.5734 - val_acc: 0.9351\n",
      "\n",
      "Epoch 00112: ReduceLROnPlateau reducing learning rate to 0.00047829695977270604.\n",
      "Epoch 113/500\n",
      "47696/47696 [==============================] - 15s 310us/step - loss: 0.3984 - acc: 0.9717 - val_loss: 0.5258 - val_acc: 0.9350\n",
      "Epoch 114/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.3993 - acc: 0.9712 - val_loss: 0.5239 - val_acc: 0.9345\n",
      "Epoch 115/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.3992 - acc: 0.9719 - val_loss: 0.5034 - val_acc: 0.9395\n",
      "Epoch 116/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.3994 - acc: 0.9713 - val_loss: 0.5301 - val_acc: 0.9349\n",
      "Epoch 117/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.3983 - acc: 0.9709 - val_loss: 0.5084 - val_acc: 0.9400\n",
      "Epoch 118/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.3979 - acc: 0.9721 - val_loss: 0.5336 - val_acc: 0.9357\n",
      "Epoch 119/500\n",
      "47696/47696 [==============================] - 15s 309us/step - loss: 0.3973 - acc: 0.9725 - val_loss: 0.5338 - val_acc: 0.9360\n",
      "Epoch 120/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.3953 - acc: 0.9731 - val_loss: 0.5437 - val_acc: 0.9318\n",
      "Epoch 121/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.3981 - acc: 0.9717 - val_loss: 0.5529 - val_acc: 0.9300\n",
      "Epoch 122/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.3965 - acc: 0.9721 - val_loss: 0.5343 - val_acc: 0.9337\n",
      "Epoch 123/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.3964 - acc: 0.9727 - val_loss: 0.5378 - val_acc: 0.9299\n",
      "Epoch 124/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.3952 - acc: 0.9736 - val_loss: 0.5253 - val_acc: 0.9366\n",
      "Epoch 125/500\n",
      "47696/47696 [==============================] - 15s 317us/step - loss: 0.3939 - acc: 0.9735 - val_loss: 0.5152 - val_acc: 0.9394\n",
      "\n",
      "Epoch 00125: ReduceLROnPlateau reducing learning rate to 0.0004304672533180565.\n",
      "Epoch 126/500\n",
      "47696/47696 [==============================] - 15s 323us/step - loss: 0.3624 - acc: 0.9744 - val_loss: 0.5236 - val_acc: 0.9323\n",
      "Epoch 127/500\n",
      "47696/47696 [==============================] - 15s 324us/step - loss: 0.3654 - acc: 0.9730 - val_loss: 0.5156 - val_acc: 0.9323\n",
      "Epoch 128/500\n",
      "47696/47696 [==============================] - 15s 324us/step - loss: 0.3618 - acc: 0.9737 - val_loss: 0.5140 - val_acc: 0.9347\n",
      "Epoch 129/500\n",
      "47696/47696 [==============================] - 15s 323us/step - loss: 0.3596 - acc: 0.9741 - val_loss: 0.5180 - val_acc: 0.9324\n",
      "Epoch 130/500\n",
      "47696/47696 [==============================] - 15s 323us/step - loss: 0.3628 - acc: 0.9741 - val_loss: 0.5180 - val_acc: 0.9347\n",
      "Epoch 131/500\n",
      "47696/47696 [==============================] - 16s 329us/step - loss: 0.3625 - acc: 0.9740 - val_loss: 0.4966 - val_acc: 0.9364\n",
      "Epoch 132/500\n",
      "47696/47696 [==============================] - 18s 380us/step - loss: 0.3607 - acc: 0.9739 - val_loss: 0.5069 - val_acc: 0.9346\n",
      "Epoch 133/500\n",
      "47696/47696 [==============================] - 16s 342us/step - loss: 0.3609 - acc: 0.9748 - val_loss: 0.5422 - val_acc: 0.9292\n",
      "Epoch 134/500\n",
      "47696/47696 [==============================] - 16s 327us/step - loss: 0.3589 - acc: 0.9754 - val_loss: 0.5141 - val_acc: 0.9298\n",
      "Epoch 135/500\n",
      "47696/47696 [==============================] - 17s 364us/step - loss: 0.3608 - acc: 0.9747 - val_loss: 0.5023 - val_acc: 0.9363\n",
      "Epoch 136/500\n",
      "47696/47696 [==============================] - 16s 337us/step - loss: 0.3590 - acc: 0.9750 - val_loss: 0.5352 - val_acc: 0.9296\n",
      "Epoch 137/500\n",
      "47696/47696 [==============================] - 16s 334us/step - loss: 0.3611 - acc: 0.9743 - val_loss: 0.5055 - val_acc: 0.9356\n",
      "Epoch 138/500\n",
      "47696/47696 [==============================] - 18s 370us/step - loss: 0.3592 - acc: 0.9755 - val_loss: 0.4833 - val_acc: 0.9427\n",
      "Epoch 139/500\n",
      "47696/47696 [==============================] - 16s 327us/step - loss: 0.3609 - acc: 0.9743 - val_loss: 0.5144 - val_acc: 0.9314\n",
      "Epoch 140/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.3587 - acc: 0.9759 - val_loss: 0.5363 - val_acc: 0.9337\n",
      "Epoch 141/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.3605 - acc: 0.9743 - val_loss: 0.5067 - val_acc: 0.9367\n",
      "Epoch 142/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.3610 - acc: 0.9741 - val_loss: 0.4774 - val_acc: 0.9413\n",
      "Epoch 143/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.3585 - acc: 0.9750 - val_loss: 0.5063 - val_acc: 0.9385\n",
      "Epoch 144/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.3569 - acc: 0.9761 - val_loss: 0.4983 - val_acc: 0.9363\n",
      "Epoch 145/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.3572 - acc: 0.9757 - val_loss: 0.5224 - val_acc: 0.9332\n",
      "Epoch 146/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.3565 - acc: 0.9762 - val_loss: 0.4912 - val_acc: 0.9373\n",
      "Epoch 147/500\n",
      "47696/47696 [==============================] - 15s 307us/step - loss: 0.3585 - acc: 0.9753 - val_loss: 0.5207 - val_acc: 0.9347\n",
      "Epoch 148/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.3591 - acc: 0.9752 - val_loss: 0.5301 - val_acc: 0.9302\n",
      "Epoch 149/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.3580 - acc: 0.9747 - val_loss: 0.5099 - val_acc: 0.9362\n",
      "Epoch 150/500\n",
      "47696/47696 [==============================] - 15s 308us/step - loss: 0.3565 - acc: 0.9752 - val_loss: 0.5267 - val_acc: 0.9351\n",
      "Epoch 151/500\n",
      "47400/47696 [============================>.] - ETA: 0s - loss: 0.3570 - acc: 0.9753"
     ]
    }
   ],
   "source": [
    "tf.get_default_graph()\n",
    "########## HYPER PARAMETERS\n",
    "batch_size = 200\n",
    "epochs = 500\n",
    "optimizer = optimizers.Adam(lr=0.001)\n",
    "#optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.1)\n",
    "#init=tf.global_variables_initializer()\n",
    "\n",
    "########## HYPER PARAMETERS\n",
    "########## MODEL ARCHITECTURE\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(27,15,1)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='same', data_format='channels_last'))\n",
    "model.add(Dropout(0.50))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='same', data_format='channels_last'))\n",
    "model.add(Dropout(0.50))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=keras.regularizers.L1L2(0.02, 0.15), use_bias=True))\n",
    "#model.add(Dropout(0.50))\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=keras.regularizers.L1L2(0.02, 0.15), use_bias=True))\n",
    "#model.add(Dropout(0.50))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "########## MODEL ARCHITECTURE\n",
    "####TENSORBOARD\n",
    "config = \"\"\n",
    "for layer in model.layers:\n",
    "    config += str(layer.output).split('\\\"')[1].split(\"/\")[0] + str(layer.output_shape) + \"\\n\\n\"\n",
    "#### END TENSORBOARD\n",
    "config += \"batchsize: \" + str(batch_size) + \"\\n\\n\" + \"epochs: \" + str(epochs) + \"\\n\\n\"\n",
    "\n",
    "# Print summary\n",
    "model.summary()\n",
    "readable_timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%d_%H%M%S')\n",
    "tensorflowfolder = \"/srv/share/tensorboardfiles/KnuckleFinger_Jan_\" + readable_timestamp\n",
    "print(\"KnuckleFinger_Jan_\" + readable_timestamp)\n",
    "logger = LoggingTensorBoard(settings_str_to_log = config, log_dir=tensorflowfolder, histogram_freq=0,\n",
    "                            write_graph=True, write_images=True, update_freq = 'epoch')\n",
    "storer = ModelCheckpoint(\"./ModelSnapshots/KnuckleFinger_Jan_\" + readable_timestamp + \".h5\", monitor='val_loss', verbose=0,\n",
    "                         save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=10, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.9, \n",
    "                                            min_lr=0.00001)\n",
    "# compile model for training\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train_one_hot,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test_one_hot),\n",
    "                    callbacks=[logger, storer, tg_callback, learning_rate_reduction])\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-4d1ad3a896cd>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-4d1ad3a896cd>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    optimizers.Adam{}\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "optimizers.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model():\n",
    "    optimizer = optimizers.Adam()\n",
    "    #optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.1)\n",
    "    #init=tf.global_variables_initializer()\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(120, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(27,15,1)))\n",
    "    model.add(Conv2D(120, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='valid', data_format='channels_last'))\n",
    "    model.add(Dropout(0.15))\n",
    "    model.add(Conv2D(50, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(50, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='valid', data_format='channels_last'))\n",
    "    model.add(Dropout(0.10))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(400, activation=('relu'), use_bias=True))\n",
    "    model.add(Dense(100, activation=('relu'), use_bias=True))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    ####TENSORBOARD\n",
    "    config = \"\"\n",
    "    for layer in model.layers:\n",
    "        config += str(layer.output).split('\\\"')[1].split(\"/\")[0] + str(layer.output_shape) + \"\\n\\n\"\n",
    "    config += \"batchsize: \" + str(batch_size) + \"\\n\\n\" + \"epochs: \" + str(epochs) + \"\\n\\n\"\n",
    "    #### END TENSORBOARD\n",
    "     \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_default_graph()\n",
    "\n",
    "readable_timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%d_%H%M%S')\n",
    "tensorflowfolder = \"/srv/share/tensorboardfiles/KnuckleFinger_Jan_\" + readable_timestamp\n",
    "\n",
    "#set early stopping criteria\n",
    "pat = 5 #this is the number of epochs with no improvment after which the training will stop\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=pat, verbose=1)\n",
    "logger = LoggingTensorBoard(settings_str_to_log = config, log_dir=tensorflowfolder, histogram_freq=0, write_graph=True, write_images=True, update_freq = 'epoch')\n",
    "model_checkpoint = ModelCheckpoint(\"./ModelSnapshots/KnuckleFinger_Jan_\" + readable_timestamp + \".h5\", monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "\n",
    "def fit_and_evaluate(t_x, val_x, t_y, val_y, EPOCHS=500, BATCH_SIZE=200):\n",
    "    model = None\n",
    "    model = cnn_model()\n",
    "    model.summary()\n",
    "    history = model.fit(x_train, y_train_one_hot,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test_one_hot),\n",
    "                    callbacks=[logger, model_checkpoint, tg_callback])\n",
    "    \n",
    "    print(\"Val Score: \", model.evaluate(val_x, val_y))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds=3\n",
    "epochs=500\n",
    "batch_size=200\n",
    "\n",
    "#save the model history in a list after fitting so that we can plot later\n",
    "model_history = [] \n",
    "\n",
    "for i in range(n_folds):\n",
    "    print(\"Training on Fold: \",i+1)\n",
    "    ####TODO\n",
    "    t_x, val_x, t_y, val_y = train_test_split(train_x, train_y, test_size=0.1, random_state = np.random.randint(1,1000, 1)[0])\n",
    "    ####END\n",
    "    model_history.append(fit_and_evaluate(t_x, val_x, t_y, val_y, epochs, batch_size))\n",
    "    print(\"=======\"*12, end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Accuracies vs Epochs')\n",
    "plt.plot(model_history[0].history['acc'], label='Training Fold 1')\n",
    "plt.plot(model_history[1].history['acc'], label='Training Fold 2')\n",
    "plt.plot(model_history[2].history['acc'], label='Training Fold 3')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use model for inference to get test accuracy\n",
    "y_test_pred = model.predict(x_test)\n",
    "y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "\n",
    "print ('\\n Summary of the precision, recall, F1 score for each class:')\n",
    "print (sklearn.metrics.classification_report(y_test, y_test_pred))\n",
    "\n",
    "print ('\\n Confusion matrix: ')\n",
    "print (sklearn.metrics.confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['val_acc'], label=\"Test Accuracy\")\n",
    "plt.plot(history.history['acc'], label=\"Training Accuracy\")\n",
    "plt.legend()\n",
    "plt.ylim(0.5,1)\n",
    "plt.savefig(\"pres.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save(\"2312.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
