{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import *\n",
    "from keras import optimizers\n",
    "from keras import utils\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Importing matplotlib to plot images.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Importing SK-learn to calculate precision and recall\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, LeaveOneGroupOut\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "# Used for graph export\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.framework import graph_io\n",
    "from keras import backend as K\n",
    "\n",
    "import pickle as pkl\n",
    "import h5py\n",
    "\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import telegram\n",
    "from keras.callbacks import Callback\n",
    "from callbacks import TelegramCallback\n",
    "from callbacks.TelegramData import TelegramData\n",
    "\n",
    "\n",
    "# create callback\n",
    "config = {\n",
    "    'token': TelegramData.Token,   # paste your bot token\n",
    "    'telegram_id': TelegramData.ID,                                   # paste your telegram_id\n",
    "    'model_name': 'Jan_LSTM'\n",
    "}\n",
    "\n",
    "tg_callback = TelegramCallback(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggingTensorBoard(TensorBoard):    \n",
    "\n",
    "    def __init__(self, log_dir, settings_str_to_log, **kwargs):\n",
    "        super(LoggingTensorBoard, self).__init__(log_dir, **kwargs)\n",
    "\n",
    "        self.settings_str = settings_str_to_log\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        TensorBoard.on_train_begin(self, logs=logs)\n",
    "\n",
    "        tensor =  tf.convert_to_tensor(self.settings_str)\n",
    "        summary = tf.summary.text (\"Run_Settings\", tensor)\n",
    "\n",
    "        with  tf.Session() as sess:\n",
    "            s = sess.run(summary)\n",
    "            self.writer.add_summary(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  9  6  4 14 17 16 12  3 10 18  5] [13  8 11 15  7]\n"
     ]
    }
   ],
   "source": [
    "dfAll = pd.read_pickle(\"PklData/df_lstm_norm50.pkl\")\n",
    "\n",
    "lst = dfAll.userID.unique()\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(lst)\n",
    "test_ids = lst[-5:]\n",
    "train_ids = lst[:-5]\n",
    "print(train_ids, test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAll.userID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAll.TaskID = dfAll.TaskID % 17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = dfAll[dfAll.userID.isin(train_ids)]\n",
    "df_test = dfAll[dfAll.userID.isin(test_ids)]\n",
    "\n",
    "df_train2 = df_train[['Blobs', 'TaskID']].copy()\n",
    "df_test2 = df_test[['Blobs', 'TaskID']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate(df_train2.Blobs.values).reshape(-1,50,27,15,1)\n",
    "x_test = np.concatenate(df_test2.Blobs.values).reshape(-1,50,27,15,1)\n",
    "\n",
    "y_train = df_train2.TaskID.values\n",
    "y_test = df_test2.TaskID.values\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices (one-hot notation)\n",
    "num_classes = len(dfAll.TaskID.unique())\n",
    "y_train_one_hot = utils.to_categorical(df_train2.TaskID, num_classes)\n",
    "y_test_one_hot = utils.to_categorical(df_test2.TaskID, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If GPU is not available: \n",
    "# GPU_USE = '/cpu:0'\n",
    "#config = tf.ConfigProto(device_count = {\"GPU\": 1})\n",
    "\n",
    "\n",
    "# If GPU is available: \n",
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = True\n",
    "config.allow_soft_placement = True\n",
    "config.gpu_options.allow_growth=True\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "\n",
    "# Limit the maximum memory used\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "\n",
    "# set session config\n",
    "tf.keras.backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_11 (TimeDis (None, 50, 27, 15, 64)    640       \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 50, 27, 15, 32)    18464     \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 50, 14, 8, 32)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 50, 14, 8, 32)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 50, 14, 8, 32)     9248      \n",
      "_________________________________________________________________\n",
      "time_distributed_16 (TimeDis (None, 50, 14, 8, 16)     4624      \n",
      "_________________________________________________________________\n",
      "time_distributed_17 (TimeDis (None, 50, 7, 4, 16)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, 50, 7, 4, 16)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_19 (TimeDis (None, 50, 448)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_20 (TimeDis (None, 50, 64)            28736     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50, 256)           328704    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                82176     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 17)                1105      \n",
      "=================================================================\n",
      "Total params: 473,697\n",
      "Trainable params: 473,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "KnuckleFinger_LSTM_Jan_20190301_105757\n",
      "Train on 6624 samples, validate on 2569 samples\n",
      "Epoch 1/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 53.8826 - acc: 0.0971 - val_loss: 45.8201 - val_acc: 0.1059\n",
      "Epoch 2/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 38.9715 - acc: 0.1236 - val_loss: 32.7405 - val_acc: 0.0969\n",
      "Epoch 3/3000\n",
      "6624/6624 [==============================] - 101s 15ms/step - loss: 27.3187 - acc: 0.1300 - val_loss: 22.4946 - val_acc: 0.1222\n",
      "Epoch 4/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 18.4250 - acc: 0.1384 - val_loss: 14.8570 - val_acc: 0.1397\n",
      "Epoch 5/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 11.9441 - acc: 0.1467 - val_loss: 9.4766 - val_acc: 0.1580\n",
      "Epoch 6/3000\n",
      "6624/6624 [==============================] - 100s 15ms/step - loss: 7.6157 - acc: 0.1611 - val_loss: 6.1744 - val_acc: 0.1608\n",
      "Epoch 7/3000\n",
      "6624/6624 [==============================] - 100s 15ms/step - loss: 5.0906 - acc: 0.1689 - val_loss: 4.4147 - val_acc: 0.1557\n",
      "Epoch 8/3000\n",
      "6624/6624 [==============================] - 101s 15ms/step - loss: 3.6554 - acc: 0.1726 - val_loss: 3.3243 - val_acc: 0.1374\n",
      "Epoch 9/3000\n",
      "6624/6624 [==============================] - 100s 15ms/step - loss: 2.9643 - acc: 0.1800 - val_loss: 2.9555 - val_acc: 0.1896\n",
      "Epoch 10/3000\n",
      "6624/6624 [==============================] - 100s 15ms/step - loss: 2.7803 - acc: 0.1899 - val_loss: 2.9068 - val_acc: 0.1417\n",
      "Epoch 11/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.7083 - acc: 0.1926 - val_loss: 2.7872 - val_acc: 0.1896\n",
      "Epoch 12/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.6459 - acc: 0.2002 - val_loss: 2.7101 - val_acc: 0.2149\n",
      "Epoch 13/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.6225 - acc: 0.2040 - val_loss: 2.6878 - val_acc: 0.2079\n",
      "Epoch 14/3000\n",
      "6624/6624 [==============================] - 101s 15ms/step - loss: 2.6058 - acc: 0.1960 - val_loss: 2.7801 - val_acc: 0.1678\n",
      "Epoch 15/3000\n",
      "6624/6624 [==============================] - 102s 15ms/step - loss: 2.5625 - acc: 0.2100 - val_loss: 2.6812 - val_acc: 0.2160\n",
      "Epoch 16/3000\n",
      "6624/6624 [==============================] - 101s 15ms/step - loss: 2.5386 - acc: 0.2107 - val_loss: 2.6306 - val_acc: 0.2141\n",
      "Epoch 17/3000\n",
      "6624/6624 [==============================] - 100s 15ms/step - loss: 2.5258 - acc: 0.2168 - val_loss: 2.6533 - val_acc: 0.2001\n",
      "Epoch 18/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.5135 - acc: 0.2121 - val_loss: 2.6166 - val_acc: 0.2079\n",
      "Epoch 19/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.5076 - acc: 0.2171 - val_loss: 2.6279 - val_acc: 0.2047\n",
      "Epoch 20/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.4947 - acc: 0.2184 - val_loss: 2.6848 - val_acc: 0.1900\n",
      "Epoch 21/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.4905 - acc: 0.2186 - val_loss: 2.5553 - val_acc: 0.2254\n",
      "Epoch 22/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.4696 - acc: 0.2201 - val_loss: 2.6016 - val_acc: 0.2098\n",
      "Epoch 23/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.4697 - acc: 0.2183 - val_loss: 2.5370 - val_acc: 0.2339\n",
      "Epoch 24/3000\n",
      "6624/6624 [==============================] - 100s 15ms/step - loss: 2.4523 - acc: 0.2228 - val_loss: 2.5854 - val_acc: 0.2133\n",
      "Epoch 25/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.4402 - acc: 0.2237 - val_loss: 2.7016 - val_acc: 0.1806\n",
      "Epoch 26/3000\n",
      "6624/6624 [==============================] - 98s 15ms/step - loss: 2.4476 - acc: 0.2258 - val_loss: 2.6052 - val_acc: 0.2075\n",
      "Epoch 27/3000\n",
      "6624/6624 [==============================] - 98s 15ms/step - loss: 2.4408 - acc: 0.2257 - val_loss: 2.5473 - val_acc: 0.2203\n",
      "Epoch 28/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.4427 - acc: 0.2200 - val_loss: 2.5617 - val_acc: 0.2168\n",
      "Epoch 29/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.4259 - acc: 0.2305 - val_loss: 2.5033 - val_acc: 0.2285\n",
      "Epoch 30/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.4118 - acc: 0.2334 - val_loss: 2.5318 - val_acc: 0.2246\n",
      "Epoch 31/3000\n",
      "6624/6624 [==============================] - 98s 15ms/step - loss: 2.4254 - acc: 0.2311 - val_loss: 2.5483 - val_acc: 0.2250\n",
      "Epoch 32/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.3982 - acc: 0.2357 - val_loss: 2.5089 - val_acc: 0.2316\n",
      "Epoch 33/3000\n",
      "6624/6624 [==============================] - 100s 15ms/step - loss: 2.4200 - acc: 0.2322 - val_loss: 2.4773 - val_acc: 0.2378\n",
      "Epoch 34/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.3948 - acc: 0.2364 - val_loss: 2.4970 - val_acc: 0.2336\n",
      "Epoch 35/3000\n",
      "6624/6624 [==============================] - 98s 15ms/step - loss: 2.3896 - acc: 0.2340 - val_loss: 2.4926 - val_acc: 0.2359\n",
      "Epoch 36/3000\n",
      "6624/6624 [==============================] - 98s 15ms/step - loss: 2.3756 - acc: 0.2431 - val_loss: 2.5343 - val_acc: 0.2102\n",
      "Epoch 37/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.3863 - acc: 0.2421 - val_loss: 2.5409 - val_acc: 0.2304\n",
      "Epoch 38/3000\n",
      "6624/6624 [==============================] - 98s 15ms/step - loss: 2.3770 - acc: 0.2444 - val_loss: 2.4816 - val_acc: 0.2301\n",
      "Epoch 39/3000\n",
      "6624/6624 [==============================] - 97s 15ms/step - loss: 2.3877 - acc: 0.2328 - val_loss: 2.4718 - val_acc: 0.2445\n",
      "Epoch 40/3000\n",
      "6624/6624 [==============================] - 98s 15ms/step - loss: 2.3694 - acc: 0.2488 - val_loss: 2.4572 - val_acc: 0.2491\n",
      "Epoch 41/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.3538 - acc: 0.2438 - val_loss: 2.4515 - val_acc: 0.2515\n",
      "Epoch 42/3000\n",
      "6624/6624 [==============================] - 98s 15ms/step - loss: 2.3511 - acc: 0.2449 - val_loss: 2.4469 - val_acc: 0.2515\n",
      "Epoch 43/3000\n",
      "6624/6624 [==============================] - 98s 15ms/step - loss: 2.3484 - acc: 0.2435 - val_loss: 2.4381 - val_acc: 0.2518\n",
      "Epoch 44/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.3447 - acc: 0.2503 - val_loss: 2.4776 - val_acc: 0.2456\n",
      "Epoch 45/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.3405 - acc: 0.2497 - val_loss: 2.4734 - val_acc: 0.2374\n",
      "Epoch 46/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.3359 - acc: 0.2554 - val_loss: 2.4090 - val_acc: 0.2573\n",
      "Epoch 47/3000\n",
      "6624/6624 [==============================] - 100s 15ms/step - loss: 2.3640 - acc: 0.2506 - val_loss: 2.5337 - val_acc: 0.2184\n",
      "Epoch 48/3000\n",
      "6624/6624 [==============================] - 100s 15ms/step - loss: 2.3373 - acc: 0.2497 - val_loss: 2.4915 - val_acc: 0.2390\n",
      "Epoch 49/3000\n",
      "6624/6624 [==============================] - 98s 15ms/step - loss: 2.3313 - acc: 0.2539 - val_loss: 2.3882 - val_acc: 0.2585\n",
      "Epoch 50/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.3222 - acc: 0.2601 - val_loss: 2.4227 - val_acc: 0.2581\n",
      "Epoch 51/3000\n",
      "6624/6624 [==============================] - 98s 15ms/step - loss: 2.3118 - acc: 0.2601 - val_loss: 2.3956 - val_acc: 0.2557\n",
      "Epoch 52/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.3111 - acc: 0.2613 - val_loss: 2.3887 - val_acc: 0.2554\n",
      "Epoch 53/3000\n",
      "6624/6624 [==============================] - 100s 15ms/step - loss: 2.3204 - acc: 0.2551 - val_loss: 2.4383 - val_acc: 0.2483\n",
      "Epoch 54/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.2998 - acc: 0.2604 - val_loss: 2.3802 - val_acc: 0.2725\n",
      "Epoch 55/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.2991 - acc: 0.2616 - val_loss: 2.3754 - val_acc: 0.2635\n",
      "Epoch 56/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.2933 - acc: 0.2607 - val_loss: 2.4125 - val_acc: 0.2616\n",
      "Epoch 57/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.2922 - acc: 0.2619 - val_loss: 2.4570 - val_acc: 0.2483\n",
      "Epoch 58/3000\n",
      "6624/6624 [==============================] - 98s 15ms/step - loss: 2.2943 - acc: 0.2630 - val_loss: 2.3376 - val_acc: 0.2772\n",
      "Epoch 59/3000\n",
      "6624/6624 [==============================] - 98s 15ms/step - loss: 2.2774 - acc: 0.2692 - val_loss: 2.3603 - val_acc: 0.2674\n",
      "Epoch 60/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.2770 - acc: 0.2643 - val_loss: 2.3954 - val_acc: 0.2635\n",
      "Epoch 61/3000\n",
      "6624/6624 [==============================] - 98s 15ms/step - loss: 2.2708 - acc: 0.2674 - val_loss: 2.3941 - val_acc: 0.2686\n",
      "Epoch 62/3000\n",
      "6624/6624 [==============================] - 98s 15ms/step - loss: 2.2823 - acc: 0.2619 - val_loss: 2.3866 - val_acc: 0.2686\n",
      "Epoch 63/3000\n",
      "6624/6624 [==============================] - 98s 15ms/step - loss: 2.2619 - acc: 0.2689 - val_loss: 2.3455 - val_acc: 0.2795\n",
      "Epoch 64/3000\n",
      "6624/6624 [==============================] - 97s 15ms/step - loss: 2.2570 - acc: 0.2737 - val_loss: 2.3099 - val_acc: 0.2842\n",
      "Epoch 65/3000\n",
      "6624/6624 [==============================] - 97s 15ms/step - loss: 2.2485 - acc: 0.2720 - val_loss: 2.3487 - val_acc: 0.2698\n",
      "Epoch 66/3000\n",
      "6624/6624 [==============================] - 96s 15ms/step - loss: 2.2543 - acc: 0.2731 - val_loss: 2.3229 - val_acc: 0.2919\n",
      "Epoch 67/3000\n",
      "6624/6624 [==============================] - 97s 15ms/step - loss: 2.2566 - acc: 0.2763 - val_loss: 2.3418 - val_acc: 0.2970\n",
      "Epoch 68/3000\n",
      "6624/6624 [==============================] - 97s 15ms/step - loss: 2.2505 - acc: 0.2761 - val_loss: 2.4554 - val_acc: 0.2659\n",
      "Epoch 69/3000\n",
      "6624/6624 [==============================] - 95s 14ms/step - loss: 2.2450 - acc: 0.2729 - val_loss: 2.3735 - val_acc: 0.2604\n",
      "Epoch 70/3000\n",
      "6624/6624 [==============================] - 95s 14ms/step - loss: 2.2415 - acc: 0.2784 - val_loss: 2.3480 - val_acc: 0.2795\n",
      "Epoch 71/3000\n",
      "6624/6624 [==============================] - 97s 15ms/step - loss: 2.2385 - acc: 0.2782 - val_loss: 2.2745 - val_acc: 0.3025\n",
      "Epoch 72/3000\n",
      "6624/6624 [==============================] - 98s 15ms/step - loss: 2.2234 - acc: 0.2799 - val_loss: 2.2990 - val_acc: 0.2888\n",
      "Epoch 73/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.2158 - acc: 0.2817 - val_loss: 2.3036 - val_acc: 0.2896\n",
      "Epoch 74/3000\n",
      "6624/6624 [==============================] - 98s 15ms/step - loss: 2.2210 - acc: 0.2785 - val_loss: 2.2559 - val_acc: 0.3095\n",
      "Epoch 75/3000\n",
      "6624/6624 [==============================] - 97s 15ms/step - loss: 2.2107 - acc: 0.2840 - val_loss: 2.2719 - val_acc: 0.2993\n",
      "Epoch 76/3000\n",
      "6624/6624 [==============================] - 98s 15ms/step - loss: 2.2035 - acc: 0.2847 - val_loss: 2.2947 - val_acc: 0.2916\n",
      "Epoch 77/3000\n",
      "6624/6624 [==============================] - 97s 15ms/step - loss: 2.1885 - acc: 0.2883 - val_loss: 2.2793 - val_acc: 0.2892\n",
      "Epoch 78/3000\n",
      "6624/6624 [==============================] - 96s 15ms/step - loss: 2.1881 - acc: 0.2877 - val_loss: 2.2698 - val_acc: 0.3060\n",
      "Epoch 79/3000\n",
      "6624/6624 [==============================] - 97s 15ms/step - loss: 2.1769 - acc: 0.2903 - val_loss: 2.3508 - val_acc: 0.2787\n",
      "Epoch 80/3000\n",
      "6624/6624 [==============================] - 98s 15ms/step - loss: 2.1820 - acc: 0.2883 - val_loss: 2.2604 - val_acc: 0.2892\n",
      "Epoch 81/3000\n",
      "6624/6624 [==============================] - 99s 15ms/step - loss: 2.1639 - acc: 0.2951 - val_loss: 2.2346 - val_acc: 0.3060\n",
      "Epoch 82/3000\n",
      "6624/6624 [==============================] - 97s 15ms/step - loss: 2.1680 - acc: 0.3001 - val_loss: 2.2260 - val_acc: 0.3149\n",
      "Epoch 83/3000\n",
      "6624/6624 [==============================] - 97s 15ms/step - loss: 2.1661 - acc: 0.2982 - val_loss: 2.3669 - val_acc: 0.2713\n",
      "Epoch 84/3000\n",
      "6624/6624 [==============================] - 97s 15ms/step - loss: 2.1761 - acc: 0.2953 - val_loss: 2.2074 - val_acc: 0.3332\n",
      "Epoch 85/3000\n",
      "6624/6624 [==============================] - 97s 15ms/step - loss: 2.1701 - acc: 0.2930 - val_loss: 2.2107 - val_acc: 0.3196\n",
      "Epoch 86/3000\n",
      "6624/6624 [==============================] - 96s 15ms/step - loss: 2.1557 - acc: 0.2953 - val_loss: 2.2145 - val_acc: 0.3040\n",
      "Epoch 87/3000\n",
      "6624/6624 [==============================] - 98s 15ms/step - loss: 2.1582 - acc: 0.2994 - val_loss: 2.1891 - val_acc: 0.3196\n",
      "Epoch 88/3000\n",
      "6624/6624 [==============================] - 98s 15ms/step - loss: 2.1450 - acc: 0.2936 - val_loss: 2.2275 - val_acc: 0.3052\n",
      "Epoch 89/3000\n",
      "6624/6624 [==============================] - 98s 15ms/step - loss: 2.1420 - acc: 0.2973 - val_loss: 2.2058 - val_acc: 0.3207\n",
      "Epoch 90/3000\n",
      "6624/6624 [==============================] - 97s 15ms/step - loss: 2.1389 - acc: 0.2960 - val_loss: 2.2174 - val_acc: 0.3137\n",
      "Epoch 91/3000\n",
      "6624/6624 [==============================] - 97s 15ms/step - loss: 2.1523 - acc: 0.2985 - val_loss: 2.2330 - val_acc: 0.2982\n",
      "Epoch 92/3000\n",
      "6624/6624 [==============================] - 96s 15ms/step - loss: 2.1435 - acc: 0.3036 - val_loss: 2.2687 - val_acc: 0.2787\n",
      "Epoch 93/3000\n",
      "6624/6624 [==============================] - 96s 15ms/step - loss: 2.1370 - acc: 0.3037 - val_loss: 2.1885 - val_acc: 0.3227\n",
      "Epoch 94/3000\n",
      "6624/6624 [==============================] - 96s 14ms/step - loss: 2.1392 - acc: 0.2968 - val_loss: 2.2547 - val_acc: 0.2799\n",
      "Epoch 95/3000\n",
      "6624/6624 [==============================] - 97s 15ms/step - loss: 2.1185 - acc: 0.3066 - val_loss: 2.1707 - val_acc: 0.3278\n",
      "Epoch 96/3000\n",
      "6624/6624 [==============================] - 96s 15ms/step - loss: 2.1210 - acc: 0.3081 - val_loss: 2.1545 - val_acc: 0.3289\n",
      "Epoch 97/3000\n",
      "6624/6624 [==============================] - 96s 15ms/step - loss: 2.1382 - acc: 0.3095 - val_loss: 2.1754 - val_acc: 0.3200\n",
      "Epoch 98/3000\n",
      "6624/6624 [==============================] - 97s 15ms/step - loss: 2.1225 - acc: 0.3053 - val_loss: 2.1883 - val_acc: 0.3320\n",
      "Epoch 99/3000\n",
      "6624/6624 [==============================] - 97s 15ms/step - loss: 2.1273 - acc: 0.3102 - val_loss: 2.1997 - val_acc: 0.3071\n",
      "Epoch 100/3000\n",
      "6624/6624 [==============================] - 96s 14ms/step - loss: 2.1242 - acc: 0.3163 - val_loss: 2.1626 - val_acc: 0.3192\n",
      "Epoch 101/3000\n",
      "6624/6624 [==============================] - 96s 14ms/step - loss: 2.1367 - acc: 0.3071 - val_loss: 2.1950 - val_acc: 0.3095\n",
      "Epoch 102/3000\n",
      "6624/6624 [==============================] - 97s 15ms/step - loss: 2.1187 - acc: 0.3045 - val_loss: 2.2259 - val_acc: 0.3032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/3000\n",
      "6624/6624 [==============================] - 96s 14ms/step - loss: 2.1081 - acc: 0.3220 - val_loss: 2.1771 - val_acc: 0.3196\n",
      "Epoch 104/3000\n",
      "6624/6624 [==============================] - 97s 15ms/step - loss: 2.1170 - acc: 0.3130 - val_loss: 2.1686 - val_acc: 0.3340\n",
      "Epoch 105/3000\n",
      "6624/6624 [==============================] - 96s 15ms/step - loss: 2.1155 - acc: 0.3169 - val_loss: 2.1824 - val_acc: 0.3215\n",
      "Epoch 106/3000\n",
      "6624/6624 [==============================] - 96s 15ms/step - loss: 2.1042 - acc: 0.3196 - val_loss: 2.2400 - val_acc: 0.2954\n",
      "Epoch 107/3000\n",
      "6624/6624 [==============================] - 96s 14ms/step - loss: 2.1114 - acc: 0.3197 - val_loss: 2.1742 - val_acc: 0.3184\n",
      "Epoch 108/3000\n",
      "6624/6624 [==============================] - 97s 15ms/step - loss: 2.0958 - acc: 0.3149 - val_loss: 2.1793 - val_acc: 0.3196\n",
      "Epoch 109/3000\n",
      "6624/6624 [==============================] - 97s 15ms/step - loss: 2.1025 - acc: 0.3204 - val_loss: 2.2142 - val_acc: 0.3025\n",
      "Epoch 110/3000\n",
      "6624/6624 [==============================] - 97s 15ms/step - loss: 2.1097 - acc: 0.3101 - val_loss: 2.2745 - val_acc: 0.2853\n",
      "Epoch 111/3000\n",
      "6624/6624 [==============================] - 97s 15ms/step - loss: 2.0924 - acc: 0.3197 - val_loss: 2.1457 - val_acc: 0.3262\n",
      "Epoch 112/3000\n",
      "6624/6624 [==============================] - 97s 15ms/step - loss: 2.0813 - acc: 0.3223 - val_loss: 2.1667 - val_acc: 0.3289\n",
      "Epoch 113/3000\n",
      "6624/6624 [==============================] - 97s 15ms/step - loss: 2.0807 - acc: 0.3211 - val_loss: 2.2303 - val_acc: 0.3067\n",
      "Epoch 114/3000\n",
      "6624/6624 [==============================] - 96s 15ms/step - loss: 2.0943 - acc: 0.3222 - val_loss: 2.1483 - val_acc: 0.3285\n",
      "Epoch 115/3000\n",
      "6624/6624 [==============================] - 96s 14ms/step - loss: 2.0908 - acc: 0.3151 - val_loss: 2.1983 - val_acc: 0.2947\n",
      "Epoch 116/3000\n",
      "6624/6624 [==============================] - 96s 15ms/step - loss: 2.0949 - acc: 0.3253 - val_loss: 2.1439 - val_acc: 0.3235\n",
      "Epoch 117/3000\n",
      "4850/6624 [====================>.........] - ETA: 22s - loss: 2.0776 - acc: 0.3214"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "epochs = 3000\n",
    "timesteps = 50\n",
    "data_dim = (27,15)\n",
    "l1v = 0.007\n",
    "l2v = 0.014\n",
    "\n",
    "\n",
    "tf.get_default_graph()\n",
    "model = Sequential()\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(64, kernel_size=(3,3), activation='relu', \n",
    "            padding='same', kernel_regularizer=regularizers.l1_l2(l1v,l2v)), input_shape=(timesteps ,27, 15, 1)))\n",
    "model.add(TimeDistributed(Conv2D(32, kernel_size=(3,3), activation='relu', padding='same', \n",
    "                                 kernel_regularizer=regularizers.l1_l2(l1v,l2v))))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2,2), strides=None, padding='same', data_format='channels_last')))\n",
    "model.add(TimeDistributed(Dropout(0.50)))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, kernel_size=(3, 3), activation='relu', \n",
    "            padding='same', kernel_regularizer=regularizers.l1_l2(l1v,l2v)), input_shape=(timesteps ,27, 15, 1)))\n",
    "model.add(TimeDistributed(Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same', \n",
    "                                 kernel_regularizer=regularizers.l1_l2(l1v,l2v))))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2,2), strides=None, padding='same', data_format='channels_last')))\n",
    "model.add(TimeDistributed(Dropout(0.50)))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "#model.add(TimeDistributed(Dense(128)))\n",
    "model.add(TimeDistributed(Dense(64)))\n",
    "#model.add(TimeDistributed(Dense(32)))\n",
    "model.add(LSTM(256, return_sequences=True, input_shape=(timesteps, data_dim),kernel_regularizer=regularizers.l1_l2(l1v,l2v)))\n",
    "#model.add(Dropout(0.25))\n",
    "#model.add(LSTM(256, return_sequences=True, input_shape=(timesteps, data_dim),kernel_regularizer=regularizers.l1_l2(0.001,0.01)))\n",
    "#model.add(Dense(512))\n",
    "model.add(LSTM(64, return_sequences=False, input_shape=(timesteps, data_dim),kernel_regularizer=regularizers.l1_l2(l1v,l2v)))\n",
    "#model.add(Dropout(0.20))\n",
    "#model.add(Dense(256))\n",
    "#model.add(LSTM(64, kernel_regularizer=regularizers.l2(0.01)))\n",
    "#model.add(Dropout(0.20))\n",
    "#model.add(Dense(64))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#optimizer = optimizers.Adagrad()\n",
    "optimizer = optimizers.Adam(lr = 0.0001, decay=1e-6)\n",
    "#optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.1)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Broadcast progress to the tensorboard.\n",
    "readable_timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%d_%H%M%S')\n",
    "tensorflowfolder = \"/srv/share/tensorboardfiles/KnuckleFinger_LSTM_Jan_\" + readable_timestamp\n",
    "\n",
    "config = \"\"\n",
    "for layer in model.layers:\n",
    "    config += str(layer.output).split('\\\"')[1].split(\"/\")[0] + str(layer.output_shape) + \"\\n\\n\"\n",
    "config += \"batchsize: \" + str(batch_size) + \"\\n\\n\" + \"epochs: \" + str(epochs) + \"\\n\\n\" \n",
    "config += \"l1: \" + str(l1v) + \"\\n\\n\" + \"l2: \" + str(l2v) + \"\\n\\n\"\n",
    "\n",
    "model.summary()\n",
    "\n",
    "readable_timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%d_%H%M%S')\n",
    "tensorflowfolder = \"/srv/share/tensorboardfiles/KnuckleFinger_LSTM_Jan_\" + readable_timestamp\n",
    "print(\"KnuckleFinger_LSTM_Jan_\" + readable_timestamp)\n",
    "logger = LoggingTensorBoard(settings_str_to_log = config, log_dir=tensorflowfolder, histogram_freq=0,\n",
    "                            write_graph=True, write_images=True, update_freq = 'epoch')\n",
    "storer = ModelCheckpoint(\"./ModelSnapshots/KnuckleFinger_LSTM_Jan_\" + readable_timestamp + \".h5\", monitor='val_loss', verbose=0,\n",
    "                         save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=100, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.95, \n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "history = model.fit(x_train, y_train_one_hot,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test_one_hot),\n",
    "                        callbacks=[storer,logger,tg_callback, learning_rate_reduction])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['val_acc'], label=\"Test Accuracy\")\n",
    "plt.plot(history.history['acc'], label=\"Training Accuracy\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('lstm_01_03_19_w50_b40.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test_pred = model.predict(x_test, batch_size=30)\n",
    "y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "#model.predict(x_test)\n",
    "\n",
    "print ('\\n Summary of the precision, recall, F1 score for each class:')\n",
    "print (sklearn.metrics.classification_report(y_test, y_test_pred))\n",
    "\n",
    "print ('\\n Confusion matrix: ')\n",
    "print (sklearn.metrics.confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
