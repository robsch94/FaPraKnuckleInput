\documentclass[sigchi-a, authorversion]{acmart}
\usepackage{booktabs} % For formal tables
\usepackage{ccicons}  % For Creative Commons citation icons

% Copyright
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\acmDOI{}

% ISBN
%\acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[FIS'18]{Fachpraktikum Interaktive Systeme}{July 2018}{Stuttgart, Germany}
\acmYear{2018}
\copyrightyear{2018}

\acmPrice{00.00}

%\acmBadgeL[http://ctuning.org/ae/ppopp2016.html]{ae-logo}
%\acmBadgeR[http://ctuning.org/ae/ppopp2016.html]{ae-logo}

\begin{document}
\title{Knuckle Input}

\author{Robin Schweigert}
\affiliation{%
  \institution{University of Stuttgart}
  \city{Stuttgart}
  \country{Germany} }
\email{author1@anotherco.edu}

\author{Simon Hagenmayer}
\affiliation{%
  \institution{University of Stuttgart}
  \city{Stuttgart}
  \country{Germany} }
\email{st107469@stud.uni-stuttgart.de}

\author{Jan Leusmann}
\affiliation{%
  \institution{University of Stuttgart}
  \city{Stuttgart}
  \country{Germany} }
\email{st112158@stud.uni-stuttgart.de}


% The default list of authors is too long for headers.
\renewcommand{\shortauthors}{F. Author et al.}


%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%


\begin{CCSXML}
<ccs2012>
 <concept>
<concept_id>10003120.10003121.10003122.10003334</concept_id>
<concept_desc>Human-centered computing~User studies</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003120.10003138.10003141</concept_id>
<concept_desc>Human-centered computing~Ubiquitous and mobile devices</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~User studies}
\ccsdesc[500]{Human-centered computing~Ubiquitous and mobile devices}


\begin{abstract}
Abstracts should be about 150 words. Required. See: \url{https://users.ece.cmu.edu/~koopman/essays/abstract.html}
\end{abstract}


\keywords{Touch Input; Knuckle Input; Gestures; HCI}

\maketitle

\section{Introduction}
The structure of the final submission is only a suggestion, feel free to change if it needed. Final a example publication under: \url{http://sven-mayer.com/wp-content/uploads/2017/03/le2016placement.pdf}. How to report machine learning? See: \url{http://sven-mayer.com/wp-content/uploads/2018/01/le2018palmtouch.pdf} or \url{http://sven-mayer.com/wp-content/uploads/2017/08/mayer2017orientation.pdf}.

Motivate your project by reporting about related work and common goals.

Random example citation \cite{Le:2018:PalmTouch}.
\section{Related Work}
%Andere knuckle input ideen(Sound, vibration...)
%Andere NN zu palm touch...
%Andere NN zu knuckle input
%Baseline machine learning knuckle input (evtl nicht in related work) 

\section{Data Collection Study}
In order to be able to train a model, which can detect if users used their smartphone with their knuckle or finger, we did a data collection study, in which the users had to perform 17 different gestures on a smartphone so we could collect the capacative images.
%To gather data, which are as close to real life capacative images as possible we thought about example applications.
%These applications would only be executed if the gesture would be performed with a knuckle, but for our study purpose we shows the same applications for the finger and knuckle gestures.

\subsection{Gesture Survey}
To get some input, which gestures we should use for our data collection study, we conducted a small survey, in which we presented 20 different gestures and suiting real world applications for each.
In this survey we found, that users did not like gestures with two knuckles, but generally all gestures were rated equally.
This is why we took 17 of the 20 gestures for our data collection study.


\subsection{Apparatus}
Our apparatus consisted out of a Nexus 5, with a specific kernel, an android application, and a laptop with a python 3.6 application.

The custom kernel for the Nexus 5 allowed us to gather approximately 25 capacitive images per second, but because of this the touch input of the smart-phone is very unresponsive.
This is why we chose to have no interaction with the UI in our data collection android application, and instead do all interaction with our laptop, which had an UDP connection to the smart-phone.
With the python application on the laptop we could iterate through the study procedure, and see the current capacitive image in real time.

In our android application we first had to input some demographic data of the current participant.
The task application was a screen, which displayed information about the current task, participants had to perform.
This consisted out of the current input method(finger, knuckle), the current gesture, a progress bar, and an image for an example application for the current gesture.

\subsection{Data}
We collected every capacitive image we got from the smartphone, beginning when the task application was started.
The resolution of this capacitive image was 27 by 15 pixels.
We saved our data in an csv file.
Next to the capacitive image for every entry we saved the following data: User ID, Task ID, Time-stamp, Version ID, Repetition ID, isPause boolean.
Every user got his user ID when starting the study, starting by 1.
As we used 17 gestures, the task ID ranged from 1 to 34, as every gesture was performed with each the finger and the knuckle.
The version ID states which iteration of one task was performed by the participant.
The repetition ID was set higher, when we pressed the revert button in our python application when the gesture was not performed correctly by the participants.
We later took only the data with the highest repetition ID, for every user ID, task ID and version ID.

\subsection{Tasks}
The study duration was split into two parts.
The first part was a small tutorial where the participants had to do each of the 17 gestures once with the finger and knuckle each. 
The real task began as soon as they finished this tutorial.
All of the 17 gestures had to be executed 20 times by the participants with their finger and knuckel each, resulting in 340 $/times$ 2 gestures. 
We randomized the order of these 17 gestures but divided the task into two halfs, one part where all the finger gestures and one all the knuckle gestures.
We did this division into two halfs to not overload the participants mentally as this would most likely result in a high error rate, because they always had to check which input method they had to use and which gesture they had to perform with it.

%TODO: image of one gesture task
The task activity can be seen in (FIGURE).
The participant then had to perform the current gesture anywhere on the screen.
If the gesture was performed correctly, we iterated to the next task, if it was performed incorrectly we asked the participants to perform it again.

\subsection{Procedure}
We conducted the study in a lab environment to have as little distraction for the participants as possible.
The participants were first handed out a consent form, which they had to read and sign and then we gave them a quick overview about the study.
Every participant was told he could take a break or quit the study at any point.

The study started with a tutorial, where participants had to perform every gesture with the finger and knuckle once.
Here we also explained each application for each gesture.
After they were done with the tutorial we started the data collection part.
Depending on if the user ID was even or odd, the participant had to do every finger gesture or knuckle gesture first.
After the 340 gestures with the first input method, the participant had to do every gesture with the other input method.

\subsection{Participants}
Participants were either invited within the course or orally.
The XX participants were XX years old on average and we had XX male participants.
No participant was forced to do the study.
No participant had any impairments and every participants' dominant hand was their right hand. 
\section{Results}
Report about your model. No source code!

Report about the validation dataset / validation study. 
\section{Discussion}
Disuses why it is still not awesome and how this could be improved. Why this is still awesome? Think about: Nobody has done this before. 
\section{Conclusion}
Two sentences wrap up what you have done. Than report what you achieved. 


\begin{sidebar}
  \textbf{Good Utilization of the Side Bar}

  \textbf{Preparation:} Do not change the margin
  dimensions and do not flow the margin text to the
  next page.

  \textbf{Materials:} The margin box must not intrude
  or overflow into the header or the footer, or the gutter space
  between the margin paragraph and the main left column.

  \textbf{Images \& Figures:} Practically anything
  can be put in the margin if it fits. Use the
  \texttt{{\textbackslash}marginparwidth} constant to set the
  width of the figure, table, minipage, or whatever you are trying
  to fit in this skinny space.

  \caption{This is the optional caption}
  \label{bar:sidebar}
\end{sidebar}



\begin{marginfigure}
    %\includegraphics[width=\marginparwidth]{cats}
    \caption{In this image, the cats are tessellated within a square
      frame. Images should also have captions and be within the
      boundaries of the sidebar on page~\pageref{bar:sidebar}. Photo:
      \cczero~jofish on Flickr.}
    \label{fig:marginfig}
\end{marginfigure}

\begin{margintable}
    \caption{A simple narrow table in the left margin
      space.}
    \label{tab:table2}
    \begin{tabular}{r r l}
      & {\small \textbf{First}}
      & {\small \textbf{Location}} \\
      \toprule
      Child & 22.5 & Melbourne \\
      Adult & 22.0 & Bogot\'a \\
      \midrule
      Gene & 22.0 & Palo Alto \\
      John & 34.5 & Minneapolis \\
      \bottomrule
    \end{tabular}
\end{margintable}

\bibliography{bibliography}
\bibliographystyle{ACM-Reference-Format}

\end{document}
